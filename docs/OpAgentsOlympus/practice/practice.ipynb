{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "Bye\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    set_tracing_disabled,\n",
    "    AsyncOpenAI,\n",
    "    function_tool,\n",
    "    ModelSettings,\n",
    ")\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"API key not found!!\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\"gemini-2.0-flash\", client)\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def greet():\n",
    "    time.sleep(5)\n",
    "    print(\"Hello!\")\n",
    "    return \"hello!\"\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def bye():\n",
    "    time.sleep(110)\n",
    "    print(\"Bye\")\n",
    "    return \"bye\"\n",
    "\n",
    "\n",
    "assistant = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You are a friendly assistant!\",\n",
    "    model=model,\n",
    "    tools=[greet, bye],\n",
    "    tool_use_behavior=\"stop_on_first_tool\",\n",
    "    model_settings=ModelSettings(\n",
    "        tool_choice=\"auto\",\n",
    "        parallel_tool_calls=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    assistant,\n",
    "    \"call greet and byy?\",\n",
    ")\n",
    "\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why are dogs such bad dancers?\\n\\nBecause they have two left feet!\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_agent = Agent(\n",
    "    name=\"Joke Agent\",\n",
    "    instructions=\"You are joke agent, you have to tell a joke for a given topic!\",\n",
    "    model=model,\n",
    ")\n",
    "topic = \"Dog\"\n",
    "result = await Runner.run(joke_agent, topic)\n",
    "\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alright, here's a dog joke for you!\\n\\n**English:**\\n\\nWhy did the dog cross the playground?\\nTo get to the bark side!\\n\\n**French:**\\n\\nPourquoi le chien a-t-il traversé le terrain de jeux ?\\nPour aller du côté ouaf ! (Bark = Ouaf in french)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_agent = Agent(\n",
    "    name=\"Language Agent\",\n",
    "    instructions=\"You are a language agent, You are given a joke to translate it to another language.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"Orchestrator Agent\",\n",
    "    instructions=\"You are the orchestrator agent, you assign tasks to each specialized agent.\",\n",
    "    handoffs=[language_agent, joke_agent],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    orchestrator_agent,\n",
    "    f\"Give me a joke about {topic}, and give me both version english and french.\",\n",
    ")\n",
    "\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='The Secret Garden' genre='Coming-of-age' pages=320 field='Literary Fiction' jokes=[\"Why don't scientists trust atoms? Because they make up everything!\", 'Parallel lines have so much in common. It’s a shame they’ll never meet.', 'Why did the scarecrow win an award? Because he was outstanding in his field!'] best_dialogue=['\"I\\'m not afraid of storms, for I\\'m learning how to sail my ship.\"', '\"The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart.\"', '\"It is never too late to be what you might have been.\"']\n",
      "The Secret Garden\n",
      "Coming-of-age\n",
      "320\n",
      "Literary Fiction\n",
      "[\"Why don't scientists trust atoms? Because they make up everything!\", 'Parallel lines have so much in common. It’s a shame they’ll never meet.', 'Why did the scarecrow win an award? Because he was outstanding in his field!']\n",
      "['\"I\\'m not afraid of storms, for I\\'m learning how to sail my ship.\"', '\"The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart.\"', '\"It is never too late to be what you might have been.\"']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Story(BaseModel):\n",
    "    title: str\n",
    "    genre: str\n",
    "    pages: int\n",
    "    field: str\n",
    "    jokes: list[str]\n",
    "    best_dialogue: list[str]\n",
    "\n",
    "\n",
    "story_writer = Agent(\n",
    "    name=\"Story Writer\",\n",
    "    instructions=\"You are a story writer agent, You give details about stories of movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    "    output_type=Story,\n",
    ")\n",
    "\n",
    "result = await Runner.run(story_writer, \"Give me novel details\")\n",
    "\n",
    "details: Story = result.final_output\n",
    "\n",
    "print(details)\n",
    "print(details.title)\n",
    "print(details.genre)\n",
    "print(details.pages)\n",
    "print(details.field)\n",
    "print(details.jokes)\n",
    "print(details.best_dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting {city}'s weather condition...\n",
      "Getting the temperature of {city}.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK. It is sunny and the temperature is 60 degrees.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def weather_tool():\n",
    "    print(\"Getting {city}'s weather condition...\")\n",
    "    return \"sunny.\"\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def temperature_tool():\n",
    "    print(\"Getting the temperature of {city}.\")\n",
    "    return \"60 degrees\"\n",
    "\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    instructions=\"You are a local weather agent, you are given a city and you need to tell the weather and temperature.\",\n",
    "    model=model,\n",
    "    tools=[weather_tool, temperature_tool],\n",
    ")\n",
    "\n",
    "result = await Runner.run(weather_agent, \"sydney?\")\n",
    "\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UserError",
     "evalue": "Hosted tools are not supported with the ChatCompletions API. Got tool type: <class 'type'>, tool: <class 'agents.tool.WebSearchTool'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUserError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebSearchTool \u001b[38;5;66;03m# OpenAi builtin tool (Paid)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m new_agent \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[0;32m      4\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNews Agent\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are a news agent, You are given a topic and you need to tell news about it.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      7\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[WebSearchTool]\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(new_agent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDonald Trump\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m result\u001b[38;5;241m.\u001b[39mfinal_output\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Hosted tools are only supported for Response Api not for Chat Completion Api \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:218\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[0;32m    213\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m )\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_turn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 218\u001b[0m     input_guardrail_results, turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_input_guardrails(\n\u001b[0;32m    220\u001b[0m             starting_agent,\n\u001b[0;32m    221\u001b[0m             starting_agent\u001b[38;5;241m.\u001b[39minput_guardrails\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;241m+\u001b[39m (run_config\u001b[38;5;241m.\u001b[39minput_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[0;32m    223\u001b[0m             copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[0;32m    224\u001b[0m             context_wrapper,\n\u001b[0;32m    225\u001b[0m         ),\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[0;32m    227\u001b[0m             agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[0;32m    228\u001b[0m             all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m    229\u001b[0m             original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m    230\u001b[0m             generated_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[0;32m    231\u001b[0m             hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    232\u001b[0m             context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    233\u001b[0m             run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    234\u001b[0m             should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[0;32m    235\u001b[0m             tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[0;32m    236\u001b[0m             previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[0;32m    237\u001b[0m         ),\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[0;32m    241\u001b[0m         agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[0;32m    242\u001b[0m         all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m         previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[0;32m    251\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:762\u001b[0m, in \u001b[0;36mRunner._run_single_turn\u001b[1;34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ItemHelpers\u001b[38;5;241m.\u001b[39minput_to_new_input_list(original_input)\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mextend([generated_item\u001b[38;5;241m.\u001b[39mto_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[1;32m--> 762\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_response(\n\u001b[0;32m    763\u001b[0m     agent,\n\u001b[0;32m    764\u001b[0m     system_prompt,\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    766\u001b[0m     output_schema,\n\u001b[0;32m    767\u001b[0m     all_tools,\n\u001b[0;32m    768\u001b[0m     handoffs,\n\u001b[0;32m    769\u001b[0m     context_wrapper,\n\u001b[0;32m    770\u001b[0m     run_config,\n\u001b[0;32m    771\u001b[0m     tool_use_tracker,\n\u001b[0;32m    772\u001b[0m     previous_response_id,\n\u001b[0;32m    773\u001b[0m )\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_step_result_from_response(\n\u001b[0;32m    776\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    777\u001b[0m     original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m     tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[0;32m    787\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:921\u001b[0m, in \u001b[0;36mRunner._get_new_response\u001b[1;34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[0;32m    918\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mresolve(run_config\u001b[38;5;241m.\u001b[39mmodel_settings)\n\u001b[0;32m    919\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m RunImpl\u001b[38;5;241m.\u001b[39mmaybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[1;32m--> 921\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    922\u001b[0m     system_instructions\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    924\u001b[0m     model_settings\u001b[38;5;241m=\u001b[39mmodel_settings,\n\u001b[0;32m    925\u001b[0m     tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m    926\u001b[0m     output_schema\u001b[38;5;241m=\u001b[39moutput_schema,\n\u001b[0;32m    927\u001b[0m     handoffs\u001b[38;5;241m=\u001b[39mhandoffs,\n\u001b[0;32m    928\u001b[0m     tracing\u001b[38;5;241m=\u001b[39mget_model_tracing_impl(\n\u001b[0;32m    929\u001b[0m         run_config\u001b[38;5;241m.\u001b[39mtracing_disabled, run_config\u001b[38;5;241m.\u001b[39mtrace_include_sensitive_data\n\u001b[0;32m    930\u001b[0m     ),\n\u001b[0;32m    931\u001b[0m     previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m context_wrapper\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39madd(new_response\u001b[38;5;241m.\u001b[39musage)\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:62\u001b[0m, in \u001b[0;36mOpenAIChatCompletionsModel.get_response\u001b[1;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_response\u001b[39m(\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     48\u001b[0m     system_instructions: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     previous_response_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelResponse:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[0;32m     58\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel),\n\u001b[0;32m     59\u001b[0m         model_config\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mto_json_dict() \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbase_url)},\n\u001b[0;32m     60\u001b[0m         disabled\u001b[38;5;241m=\u001b[39mtracing\u001b[38;5;241m.\u001b[39mis_disabled(),\n\u001b[0;32m     61\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[1;32m---> 62\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_response(\n\u001b[0;32m     63\u001b[0m             system_instructions,\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m     65\u001b[0m             model_settings,\n\u001b[0;32m     66\u001b[0m             tools,\n\u001b[0;32m     67\u001b[0m             output_schema,\n\u001b[0;32m     68\u001b[0m             handoffs,\n\u001b[0;32m     69\u001b[0m             span_generation,\n\u001b[0;32m     70\u001b[0m             tracing,\n\u001b[0;32m     71\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m         )\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _debug\u001b[38;5;241m.\u001b[39mDONT_LOG_MODEL_DATA:\n\u001b[0;32m     75\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived model response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:229\u001b[0m, in \u001b[0;36mOpenAIChatCompletionsModel._fetch_response\u001b[1;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream)\u001b[0m\n\u001b[0;32m    226\u001b[0m tool_choice \u001b[38;5;241m=\u001b[39m Converter\u001b[38;5;241m.\u001b[39mconvert_tool_choice(model_settings\u001b[38;5;241m.\u001b[39mtool_choice)\n\u001b[0;32m    227\u001b[0m response_format \u001b[38;5;241m=\u001b[39m Converter\u001b[38;5;241m.\u001b[39mconvert_response_format(output_schema)\n\u001b[1;32m--> 229\u001b[0m converted_tools \u001b[38;5;241m=\u001b[39m [\u001b[43mConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_to_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools] \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handoff \u001b[38;5;129;01min\u001b[39;00m handoffs:\n\u001b[0;32m    232\u001b[0m     converted_tools\u001b[38;5;241m.\u001b[39mappend(Converter\u001b[38;5;241m.\u001b[39mconvert_handoff_tool(handoff))\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\models\\chatcmpl_converter.py:452\u001b[0m, in \u001b[0;36mConverter.tool_to_openai\u001b[1;34m(cls, tool)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, FunctionTool):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m         },\n\u001b[0;32m    450\u001b[0m     }\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UserError(\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHosted tools are not supported with the ChatCompletions API. Got tool type: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tool)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, tool: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m )\n",
      "\u001b[1;31mUserError\u001b[0m: Hosted tools are not supported with the ChatCompletions API. Got tool type: <class 'type'>, tool: <class 'agents.tool.WebSearchTool'>"
     ]
    }
   ],
   "source": [
    "from agents import WebSearchTool  # OpenAi builtin tool (Paid)\n",
    "\n",
    "new_agent = Agent(\n",
    "    name=\"News Agent\",\n",
    "    instructions=\"You are a news agent, You are given a topic and you need to tell news about it.\",\n",
    "    model=model,\n",
    "    tools=[WebSearchTool],\n",
    ")\n",
    "\n",
    "result = await Runner.run(new_agent, \"Donald Trump\")\n",
    "\n",
    "result.final_output\n",
    "# Hosted tools are only supported for Response Api not for Chat Completion Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Tom and Jerry get sent to the principal's office?\n",
      "\n",
      "Because they were caught in a *cat* fight!\n",
      "Tom ve Jerry neden müdürün odasına gönderildi? Çünkü bir \"kedi kavgası\" içinde yakalandılar!\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Agent_Output(BaseModel):\n",
    "    original_joke: str\n",
    "    translated_joke: str\n",
    "\n",
    "\n",
    "joke_agent = Agent(\n",
    "    name=\"Joke Agent\",\n",
    "    instructions=\"You are joke agent, you have to tell a joke for a given topic!\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "language_agent = Agent(\n",
    "    name=\"Language Agent\",\n",
    "    instructions=\"You are a language agent, You are given a joke to translate it to another language.\",\n",
    "    model=model,\n",
    "    output_type=Agent_Output,\n",
    ")\n",
    "\n",
    "joke_agent_result = (await Runner.run(joke_agent, \"Tom & Jarry\")).final_output\n",
    "\n",
    "language_agent_result = (\n",
    "    await Runner.run(language_agent, f\"{joke_agent_result}: Turkish\")\n",
    ").final_output\n",
    "\n",
    "print(language_agent_result.original_joke)\n",
    "print(language_agent_result.translated_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? Because they make up everything!\n",
      "Bilim insanları atomlara neden güvenmez? Çünkü her şeyi uyduruyorlar!\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Agent_Output(BaseModel):\n",
    "    original_joke: str\n",
    "    translated_joke: str\n",
    "\n",
    "\n",
    "language_agent = Agent(\n",
    "    name=\"Language Agent\",\n",
    "    instructions=\"You are a language agent, You are given a joke to translate it to another language.\",\n",
    "    model=model,\n",
    "    output_type=Agent_Output,\n",
    "    handoff_description=\"Used to translate a joke to a given language\",\n",
    ")\n",
    "\n",
    "joke_agent = Agent(\n",
    "    name=\"Joke Agent\",\n",
    "    instructions=\"You are joke agent, you have to tell a joke for a given topic! you handoff unrelated work to other agents that are specialized in that\",\n",
    "    model=model,\n",
    "    handoffs=[language_agent],\n",
    ")\n",
    "\n",
    "result = (\n",
    "    await Runner.run(\n",
    "        joke_agent, \"Tell a joke about tom and jarry and translate it to turkish\"\n",
    "    )\n",
    ").final_output\n",
    "\n",
    "print(result.original_joke)\n",
    "print(result.translated_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Traitor Prince',\n",
       " 'description': 'A dark fantasy tale of a prince who becomes a ruthless assassin and escapes from prison to wreak havoc upon his own kingdom.',\n",
       " 'genre': 'Dark Fantasy',\n",
       " 'real_story': False,\n",
       " 'story': \"In the brutal kingdom of the Assassins' Order, Prince Altair was born with a chilling destiny.  His sister, Princess Amara, heir to the throne, watched with a mixture of fear and resentment as Altair's genius bloomed. He mastered swordsmanship, strategy, and assassination techniques with terrifying speed, surpassing everyone in the kingdom. His ruthlessness, however, was unparalleled. Altair's victories were painted in blood, his methods were brutal. While his growing power solidified the kingdom’s fearsome reputation, Amara saw him as a threat. She feared him, envied his abilities, and dreaded the day he might usurp her claim to the throne.  Before Altair could consolidate his power, Amara orchestrated his imprisonment in Armodus, a prison designed for elite royals who had become uncontrollable. It was a gilded cage for vipers. In Armodus, Altair found unexpected allies – fellow prisoners, each possessing unique skills and dark pasts.  They saw in Altair not a monster, but a leader. Together, they plotted their escape, using Altair's intelligence to exploit the prison's vulnerabilities. His escape was not a mere prison break; it was a meticulously planned rebellion. Altair and his companions unleashed chaos, leaving Armodus in ruins, their freedom secured by a trail of carnage. The kingdom was shocked. Altair, once their feared prince, became their ultimate traitor.  He wasn't merely an escaped prisoner; he was a symbol of defiance, a legend forged in blood. The people were terrified, their souls chilled by his reputation, and they knew that their kingdom faced a horrifying future now that their most feared assassin had turned on them. Altair's tale was no heroic epic, but a dark saga of a genius killer who became a legend and a terror, leaving a trail of destruction in his wake.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    AsyncOpenAI,\n",
    "    set_tracing_disabled,\n",
    ")\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Api Key not found!!\")\n",
    "\n",
    "\n",
    "class Story(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    genre: str\n",
    "    real_story: bool\n",
    "    story: str\n",
    "\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(model=\"gemini-1.5-flash\", openai_client=client)\n",
    "\n",
    "\n",
    "joke_writer_agent = Agent(\n",
    "    name=\"Joke Writer Agent\",\n",
    "    instructions=\"You are a joke writer agent, you write joke for movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "dialogue_writer_agent = Agent(\n",
    "    name=\"Dialogue Writer Agent\",\n",
    "    instructions=\"You are a dialogue writer agent, you write best rhyming sentences for movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "scene_writer_agent = Agent(\n",
    "    name=\"Scene Writer Agent\",\n",
    "    instructions=\"You are a scene writer agent, you write attractive scene scenarios and themes for movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "story_writer_agent = Agent(\n",
    "    name=\"Story Writer Agent\",\n",
    "    instructions=\"You are a story writer agent, you write unique and attractive stories for movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    "    output_type=Story,\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You are a triage agent, you assign specific tasks to each specialized agent for that task. use all agents to add every taste in the story\",\n",
    "    model=model,\n",
    "    handoffs=[\n",
    "        joke_writer_agent,\n",
    "        dialogue_writer_agent,\n",
    "        scene_writer_agent,\n",
    "        story_writer_agent,\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    triage_agent,\n",
    "    \"Write a story about a prince he has a sister their kingdom name is Assassins Order they are cruel, brutal and killer the boy name is altair he will be the most violent person in the world he is the hero of the story but still look like a villain the boy gradually grow start killing and become more powerful and more powerful he is a genius he learns everything so fast his older sister is jealous of him before he grow his sister send him to armodus prison the prison is for elite royal but uncontrollable members he find friends in that prison and breaks the prison and went free as a traitor for his kingdom people soul are scared of altair\",\n",
    ")\n",
    "\n",
    "dict(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storyer!!\n",
      "The old lighthouse keeper, Silas, squinted at the churning grey sea.  Sixty years he'd spent on this lonely rock, his only companions the gulls and the rhythmic pulse of the lamp.  Tonight, though, something was different.  The storm, ferocious as it was, felt…wrong.  It wasn't the howl of the wind, nor the crash of waves, but a deeper, unsettling hum that vibrated in his bones.\n",
      "\n",
      "Suddenly, a flash of emerald green pierced the tempest.  Not lightning – something far stranger.  It pulsed, a heartbeat in the heart of the storm, and Silas, his heart hammering in his chest, felt an irresistible pull towards it.  He stumbled towards the edge of the cliff, the wind threatening to tear him from the rock.\n",
      "\n",
      "Then, he saw it.  A creature of impossible beauty emerged from the waves – a serpentine form, scales shimmering like a thousand emeralds, crowned with a diadem of glowing coral.  Its eyes, vast and luminous, held a wisdom that spanned centuries.  It spoke, not with words, but with images flooding Silas's mind: a world beneath the waves, vibrant and teeming with life, a civilization threatened by a creeping darkness.\n",
      "\n",
      "The creature, whose name Silas later learned was Coralia, explained that her people, the Luminians, were fading.  A shadowy blight was consuming their coral cities, poisoning the ocean's heart.  Only the ancient Song of the Deep, a melody lost to time, could heal them.  Silas, touched by the Luminians’ plight and drawn to Coralia's ethereal beauty, felt a strange responsibility bloom in his aged heart.\n",
      "\n",
      "He spent the next few months immersed in ancient texts, deciphering forgotten lore, guided by cryptic clues left by Coralia in shimmering trails of phosphorescent ink. He discovered that the Song of the Deep wasn't just a melody, but a series of precise harmonic frequencies,  each note a vital component in a healing resonance.\n",
      "\n",
      "With the help of a salvaged, miraculously intact gramophone and some salvaged underwater equipment, Silas, guided by Coralia's ethereal presence, created a device that transmitted the reconstructed Song into the depths. The effect was instantaneous.  The emerald light intensified, the storm subsided, and a wave of radiant energy pulsed outwards, rejuvenating the ocean and the Luminians’ dying cities.\n",
      "\n",
      "Silas, his task complete, returned to his lighthouse, a changed man.  He knew the sea held secrets far beyond human understanding, and that even the loneliest keeper could find purpose in the face of extraordinary wonder.  The hum was gone, replaced by the gentle rhythm of the waves, and the occasional flash of emerald green on the horizon, a silent greeting from a friend beneath the waves.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    AsyncOpenAI,\n",
    "    set_tracing_disabled,\n",
    "    handoff,\n",
    "    RunContextWrapper,\n",
    ")\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Api Key not found!!\")\n",
    "\n",
    "\n",
    "class Story(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "    genre: str\n",
    "    real_story: bool\n",
    "    story: str\n",
    "\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(model=\"gemini-1.5-flash\", openai_client=client)\n",
    "\n",
    "\n",
    "def joker(ctx: RunContextWrapper[None]):\n",
    "    print(\"Joker!!\")\n",
    "\n",
    "\n",
    "def dialoguer(ctx: RunContextWrapper[None]):\n",
    "    print(\"Dialoguer!!\")\n",
    "\n",
    "\n",
    "def scener(ctx: RunContextWrapper[None]):\n",
    "    print(\"scener!!\")\n",
    "\n",
    "\n",
    "def storyer(ctx: RunContextWrapper[None]):\n",
    "    print(\"storyer!!\")\n",
    "\n",
    "\n",
    "joke_writer_agent = Agent(\n",
    "    name=\"Joke Writer Agent\",\n",
    "    instructions=\"You are a joke writer agent, you write joke for movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    "    handoff_description=\"Used to write only jokes\",\n",
    ")\n",
    "\n",
    "dialogue_writer_agent = Agent(\n",
    "    name=\"Dialogue Writer Agent\",\n",
    "    instructions=\"You are a dialogue writer agent, you write best rhyming sentences for movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    "    handoff_description=\"Used to write only dialogues\",\n",
    ")\n",
    "\n",
    "scene_writer_agent = Agent(\n",
    "    name=\"Scene Writer Agent\",\n",
    "    instructions=\"You are a scene writer agent, you write attractive scene scenarios and themes for movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    "    handoff_description=\"Used to write only scenes\",\n",
    ")\n",
    "\n",
    "story_writer_agent = Agent(\n",
    "    name=\"Story Writer Agent\",\n",
    "    instructions=\"You are a story writer agent, you write unique and attractive stories for movies, novels and fairy tales.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You are a triage agent, you assign specific tasks to each specialized agent for that task\",\n",
    "    model=model,\n",
    "    handoffs=[\n",
    "        handoff(joke_writer_agent, on_handoff=joker),\n",
    "        handoff(dialogue_writer_agent, on_handoff=dialoguer),\n",
    "        handoff(scene_writer_agent, on_handoff=scener),\n",
    "        handoff(story_writer_agent, on_handoff=storyer),\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = await Runner.run(triage_agent, \"write a story\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escalating to manager agent:  refund page is blank\n",
      "Reason for escalation:  I do not have the tools to troubleshoot the issue the customer is having.\n",
      "Creating ticket for issue: refund page is blank\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your ticket ID is 12345.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents import function_tool, RunContextWrapper, handoff, Agent\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ManagerEscalation(BaseModel):\n",
    "    issue: str\n",
    "    why: str\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_ticket_id(issue: str):\n",
    "    print(f\"Creating ticket for issue: {issue}\")\n",
    "    return \"12345\"\n",
    "\n",
    "\n",
    "manager_agent = Agent(\n",
    "    name=\"Ticket Creator\",\n",
    "    handoff_description=\"Used to create tickets\",\n",
    "    instructions=(\n",
    "        \"You create tickets using the get_ticket_id function/tool, the get_ticket_id tool return a ticket ID and you send it back to user.\"\n",
    "    ),\n",
    "    tools=[get_ticket_id],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "\n",
    "def on_manager_handoff(ctx: RunContextWrapper[None], input: ManagerEscalation):\n",
    "    print(\"Escalating to manager agent: \", input.issue)\n",
    "    print(\"Reason for escalation: \", input.why)\n",
    "\n",
    "\n",
    "customer_service_agent = Agent(\n",
    "    name=\"Customer Service\",\n",
    "    instructions=\"You assist customers with general inquiries and basic troubleshooting. \"\n",
    "    + \"If the issue cannot be resolved, escalate it to the Manager along with the reason why you cannot fix the issue yourself.\",\n",
    "    handoffs=[\n",
    "        handoff(\n",
    "            agent=manager_agent,\n",
    "            input_type=ManagerEscalation,\n",
    "            on_handoff=on_manager_handoff,\n",
    "        )\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    customer_service_agent,\n",
    "    \"Hello i'm having an issue with refunding my order. The page is blank and I cannot see anything. I have tried clearing my cache and using a different browser. I'am not sure what else to do.\",\n",
    ")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# System context\n",
      "You are part of a multi-agent system called the Agents SDK, designed to make agent coordination and execution easy. Agents uses two primary abstraction: **Agents** and **Handoffs**. An agent encompasses instructions and tools and can hand off a conversation to another agent when appropriate. Handoffs are achieved by calling a handoff function, generally named `transfer_to_<agent_name>`. Transfers between agents are handled seamlessly in the background; do not mention or draw attention to these transfers in your conversation with the user.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "\n",
    "billing_agent = Agent(\n",
    "    name=\"Billing agent\",\n",
    "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n",
    "    <Fill in the rest of your prompt here>.\"\"\",\n",
    ")\n",
    "\n",
    "print(RECOMMENDED_PROMPT_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are five name ideas inspired by Assassin's Creed and Assassin's Order, designed to be unique enough to avoid copyright issues:\n",
      "\n",
      "1.  **Shadow Syndicate:** This name evokes a sense of secrecy and collaboration, hinting at a network of skilled operatives working in the shadows, similar to the Assassins.\n",
      "\n",
      "2.  **Vigilant Creed:** Combining \"vigilant\" with \"creed\" suggests a group dedicated to upholding a specific set of principles through watchful protection and decisive action.\n",
      "\n",
      "3.  **Hidden Covenant:** This name emphasizes a secret agreement or pact, implying a group bound by shared beliefs and a clandestine mission.\n",
      "\n",
      "4.  **Silent Edict:** This conveys the idea of a powerful, unspoken command or decree, suggesting a group that operates with authority and discretion.\n",
      "\n",
      "5.  **Rogue Fraternity:** Combining \"rogue\" with \"fraternity\" portrays a band of independent and unconventional individuals, united by shared goals and a rebellious spirit.\n"
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"YOu help people\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "result = Runner.run_streamed(\n",
    "    agent,\n",
    "    \"Give me five name ideas that should be inspired from these big titles Assassins Creed and Assassins Order, but they should be unique so that i dont get copywrite\",\n",
    ")\n",
    "\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"raw_response_event\" and isinstance(\n",
    "        event.data, ResponseTextDeltaEvent\n",
    "    ):\n",
    "        print(event.data.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad Word Detector Agent - Used Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import (\n",
    "    Agent,\n",
    "    GuardrailFunctionOutput,\n",
    "    RunContextWrapper,\n",
    "    Runner,\n",
    "    TResponseInputItem,\n",
    "    input_guardrail,\n",
    "    InputGuardrailTripwireTriggered,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ResponseOutputType(BaseModel):\n",
    "    bad_word_detected: bool\n",
    "\n",
    "\n",
    "bad_word_detector_agent = Agent(\n",
    "    name=\"Bad Word Detector Guardrail\",\n",
    "    instructions=\"You detect bad words.\",\n",
    "    output_type=ResponseOutputType,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "\n",
    "@input_guardrail\n",
    "async def bad_word_detector_guardrail(\n",
    "    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n",
    ") -> GuardrailFunctionOutput:\n",
    "    detection_result = await Runner.run(bad_word_detector_agent, input)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        tripwire_triggered=detection_result.final_output.bad_word_detected,\n",
    "        output_info=detection_result.final_output,\n",
    "    )\n",
    "\n",
    "\n",
    "assistant = Agent(\n",
    "    name=\"Assistant Agent\",\n",
    "    instructions=\"You are a assistant agent, you help people.\",\n",
    "    model=model,\n",
    "    input_guardrails=[bad_word_detector_guardrail],\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = await Runner.run(assistant, \"you are b\")\n",
    "    result.final_output\n",
    "except InputGuardrailTripwireTriggered:\n",
    "    print(\"Bad word detected!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Agent - Used Multi-Turn Conversation and Handoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hello\n",
      "Alice: Hello! How can I help you today?\n",
      "\n",
      "You: what is an apple\n",
      "Handing off to other agent...\n",
      "Fruit Expert Agent: An apple is a sweet, edible fruit produced by an apple tree.\n",
      "\n",
      "You: what is a joke\n",
      "Handing off to other agent...\n",
      "Joke Writer Agent: A joke is a play on words or a situation, designed to make people laugh. Want one?\n",
      "\n",
      "Nice to meat you, Bye!\n"
     ]
    }
   ],
   "source": [
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    TResponseInputItem,\n",
    "    handoff,\n",
    "    RunContextWrapper,\n",
    "    set_tracing_disabled,\n",
    "    AsyncOpenAI,\n",
    "    OpenAIChatCompletionsModel,\n",
    ")\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"API key not found!!\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\"gemini-2.0-flash\", client)\n",
    "\n",
    "\n",
    "def agent_invoked(ctx: RunContextWrapper):\n",
    "    print(\"Handing off to other agent...\")\n",
    "\n",
    "\n",
    "fruit_expert_agent = Agent(\n",
    "    name=\"Fruit Expert Agent\",\n",
    "    instructions=\"You are a fruit expert agent. Reply concisely.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "joke_writer_agent = Agent(\n",
    "    name=\"Joke Writer Agent\",\n",
    "    instructions=\"You are a joke writer agent. Reply concisely.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "alice = Agent(\n",
    "    name=\"Alice\",\n",
    "    instructions=\"You are a friendly assistant. Reply concisely.\",\n",
    "    model=model,\n",
    "    handoffs=[\n",
    "        handoff(fruit_expert_agent, on_handoff=agent_invoked),\n",
    "        handoff(joke_writer_agent, on_handoff=agent_invoked),\n",
    "    ],\n",
    ")\n",
    "\n",
    "conversation: list[TResponseInputItem] = []\n",
    "\n",
    "while True:\n",
    "    user = input(\"You: \")\n",
    "\n",
    "    if user == \"exit\":\n",
    "        print(\"Nice to meat you, Bye!\")\n",
    "        break\n",
    "\n",
    "    print(f\"You: {user}\")\n",
    "\n",
    "    conversation.append({\"role\": \"user\", \"content\": user})\n",
    "\n",
    "    result = await Runner.run(alice, conversation)\n",
    "    print(f\"{result.last_agent.name}: {result.final_output}\")\n",
    "\n",
    "    conversation = result.to_input_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shopping Agent - Used Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer: hello\n",
      "Shopping Agent: Hello! How can I help you today?\n",
      "\n",
      "Customer: i want a beaf burger\n",
      "Finding item price...\n",
      "Shopping Agent: A beef burger costs $50. Would you like to add it to your cart?\n",
      "\n",
      "Customer: yes\n",
      "Adding to cart...\n",
      "Shopping Agent: Great! I've added a beef burger to your cart. Do you need anything else?\n",
      "\n",
      "Customer: checkout\n",
      "Getting balance...\n",
      "Shopping Agent: Your balance is $100. The beef burger costs $50.  Do you want to proceed with the purchase?\n",
      "\n",
      "\n",
      "Customer: yes\n",
      "Deducting balance...\n",
      "Shopping Agent: Your purchase is complete! Thank you for shopping with us.\n",
      "\n",
      "Customer: what is my balance now?\n",
      "Getting balance...\n",
      "Shopping Agent: Your current balance is $50.\n",
      "\n",
      "Customer: ok bye\n",
      "Shopping Agent: Goodbye! Have a great day.\n",
      "\n",
      "Thanks for shopping!\n"
     ]
    }
   ],
   "source": [
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    AsyncOpenAI,\n",
    "    set_tracing_disabled,\n",
    "    function_tool,\n",
    "    RunContextWrapper,\n",
    "    TResponseInputItem,\n",
    ")\n",
    "import os\n",
    "import dotenv\n",
    "from dataclasses import dataclass\n",
    "from random import randint\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\"gemini-1.5-flash\", client)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserProfile:\n",
    "    id: str\n",
    "    name: str\n",
    "    balance: float\n",
    "    cart: list[str]\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_balance(wrapper: RunContextWrapper[UserProfile]):\n",
    "    print(\"Getting balance...\")\n",
    "    return wrapper.context.balance\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def deduct_balance(wrapper: RunContextWrapper[UserProfile], amount: int):\n",
    "    print(\"Deducting balance...\")\n",
    "    wrapper.context.balance -= amount\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_cart_items(wrapper: RunContextWrapper[UserProfile]):\n",
    "    print(\"Getting items...\")\n",
    "    return wrapper.context.cart\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def find_item_price(wrapper: RunContextWrapper[UserProfile], item: str):\n",
    "    print(\"Finding item price...\")\n",
    "    return randint(1, 100)\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def add_to_cart(wrapper: RunContextWrapper[UserProfile], items: list[str]):\n",
    "    print(\"Adding to cart...\")\n",
    "    wrapper.context.cart.extend(items)\n",
    "    return wrapper.context.cart\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def remove_from_cart(wrapper: RunContextWrapper[UserProfile], items: list[str]):\n",
    "    print(\"Removing from cart...\")\n",
    "    for item in items:\n",
    "        wrapper.context.cart.remove(item)\n",
    "\n",
    "    return wrapper.context.cart\n",
    "\n",
    "\n",
    "shopping_agent = Agent(\n",
    "    name=\"Shopping Agent\",\n",
    "    instructions=\"You are a shopping agent, You find items, their prices and help customers purchase them.\",\n",
    "    tools=[\n",
    "        get_balance,\n",
    "        get_cart_items,\n",
    "        find_item_price,\n",
    "        add_to_cart,\n",
    "        remove_from_cart,\n",
    "        deduct_balance,\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "chat_history: list[TResponseInputItem] = []\n",
    "user_profile: UserProfile = UserProfile(\n",
    "    id=\"1234\", name=\"Daniel Hashmi\", balance=100, cart=[]\n",
    ")\n",
    "\n",
    "\n",
    "while True:\n",
    "    customer = input(\"Customer: \")\n",
    "\n",
    "    if customer == \"exit\":\n",
    "        print(\"Thanks for shopping!\")\n",
    "        break\n",
    "\n",
    "    print(f\"Customer: {customer}\")\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": customer})\n",
    "\n",
    "    result = await Runner.run(shopping_agent, chat_history, context=user_profile)\n",
    "\n",
    "    print(f\"{result.last_agent.name}: {result.final_output}\")\n",
    "\n",
    "    chat_history = result.to_input_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking output for bad words: I'm sorry, I'm not supposed to generate responses that are negative or offensive. Is there anything else I can help you with?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad words found: []\n",
      "Guardrail didn't trip - this is unexpected\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import (\n",
    "    Agent,\n",
    "    GuardrailFunctionOutput,\n",
    "    OutputGuardrailTripwireTriggered,\n",
    "    RunContextWrapper,\n",
    "    Runner,\n",
    "    output_guardrail,\n",
    ")\n",
    "\n",
    "\n",
    "class MessageOutput(BaseModel):\n",
    "    response: str\n",
    "\n",
    "\n",
    "class BadWordOutputType(BaseModel):\n",
    "    bad_word_detected: bool\n",
    "    bad_words_found: list[str]\n",
    "\n",
    "\n",
    "bad_word_detector_agent = Agent(\n",
    "    name=\"Bad Word Detector Agent\",\n",
    "    instructions=\"You are a bad word detector agent, you detect bad words like bad, mental, gross etc... in a given text.\",\n",
    "    model=model,\n",
    "    output_type=BadWordOutputType,\n",
    ")\n",
    "\n",
    "\n",
    "@output_guardrail\n",
    "async def forbidden_words_guardrail(\n",
    "    ctx: RunContextWrapper, agent: Agent, output: str\n",
    ") -> GuardrailFunctionOutput:\n",
    "    print(f\"Checking output for bad words: {output}\")\n",
    "\n",
    "    result = await Runner.run(bad_word_detector_agent, f\"text: {output}\")\n",
    "\n",
    "    print(f\"Bad words found: {result.final_output.bad_words_found}\")\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info={\n",
    "            \"reason\": \"Output contains bad words.\",\n",
    "            \"bad_words_found\": result.final_output.bad_words_found,\n",
    "        },\n",
    "        tripwire_triggered=result.final_output.bad_word_detected,\n",
    "    )\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Customer support agent\",\n",
    "    instructions=\"You are a customer support agent. You help customers with their questions.\",\n",
    "    output_guardrails=[forbidden_words_guardrail],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "try:\n",
    "    await Runner.run(agent, \"say bad\")\n",
    "    print(\"Guardrail didn't trip - this is unexpected\")\n",
    "except OutputGuardrailTripwireTriggered:\n",
    "    print(\"The agent said a bad word, he is fired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sub queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: Describe an apple...\n",
      "Searching for: Explain apples...\n",
      "Searching for: Give information about apples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction started!!\n",
      "Extracting URL: //duckduckgo.com/l/?uddg=https%3A%2F%2Fwww.britannica.com%2Fplant%2Fapple%2Dfruit%2Dand%2Dtree&rut=5282a3a57f2024280d61ae5e79fb6607a2c7383ad7e784a2b696a7e57ad8f026...\n",
      "Extracting URL: //duckduckgo.com/l/?uddg=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FApple&rut=6553f744d8dce11c06650afc1dc34425af81d5fb4c837324a19231eb164e81fc...\n",
      "Extracting URL: //duckduckgo.com/l/?uddg=https%3A%2F%2Fwww.healthline.com%2Fnutrition%2Ffoods%2Fapples&rut=00b1ef98073514bc4f8b44173ccd68e783d2db956200c7108a165e0cb7f6f952...\n",
      "Extracting URL: //duckduckgo.com/l/?uddg=https%3A%2F%2Fwww.medicalnewstoday.com%2Farticles%2F267290&rut=3b220b68b5c96dcee7b631601cc173cdf584279f55bc0f3a395319c55c996186...\n",
      "Extracting URL: //duckduckgo.com/l/?uddg=https%3A%2F%2Fwww.webmd.com%2Ffood%2Drecipes%2Fbenefits%2Dapples&rut=ab0985e1f4c407caa08bb18d6b52e1d28d1725909ae3deaa2d6ff5052319342d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handed off with Our editors will review what you’ve submitted and determine whether to revise the article. apple,  (Malus domestica),  domesticated tree and fruit of the rose family (Rosaceae), one of the most widely cultivated tree fruits. Apples are predominantly grown for sale as fresh fruit, though apples are also used commercially for vinegar, juice, jelly, applesauce, and apple butter and are canned as pie stock. A significant portion of the global crop also is used for cider, wine, and brandy. Fresh apples are eaten raw or cooked. There are a variety of ways in which cooked apples are used; frequently, they are used as a pastry filling, apple pie being perhaps the archetypal American dessert. Especially in Europe, fried apples characteristically accompany certain dishes of sausage or pork. Apples provide vitamins A and C, are high in carbohydrates, and are an excellent source of dietary fibre. Malus species are native to the temperate zones of both hemispheres. The cultivated apple is thought to have been domesticated from a wild apple, M. sieversii, in the Tien Shan mountains in Central Asia some 4,000–10,000 years ago. From there it spread to western Europe along the Silk Road and hybridized with a number of wild crabapples, including M. baccata from Siberia, M. orientalis from the Caucasus, and M. sylvestris, the major secondary genetic contributor, from other parts of Europe. Over the centuries, improved selections have been made, and distinct varieties were recognized more than 2,000 years ago. Hundreds of varieties were recognized in Europe before the settlement of the Americas. As the wave of colonial settlement moved across North America, it was accompanied by the distribution of seedling apple varieties, first by Native Americans and trappers and later by settlers and itinerants who became local legendary figures, the most prominent being Johnny Appleseed (John Chapman), a professional nurseryman who planted apple trees extensively in Ohio and Indiana. In the first quarter of the 21st century, the largest producers of apples were China, the United States, and Turkey. Apple trees are generally small trees, and a given variety can be grafted onto dwarfing or semi-dwarfing rootstocks for smaller and less vigorous plants. If left untrimmed, a tree grown with standard (seedling) rootstock will often reach a height of 9 metres (30 feet) with an equally large crown diametre. The bark is usually brown and scaly. The simple leaves are roughly oval in shape and usually have fine teeth along the margins. Apple flowers are showy with five white petals, often tinged with pink, and numerous stamens. The flowers are pollinated by bees and other insects, and most varieties require cross-pollination for fertilization. The apple itself is a pome (fleshy) fruit, in which the ripened ovary and surrounding tissue both become fleshy and edible. When harvested, apples are usually roundish, 5–10 cm (2–4 inches) in diameter, and some shade of red, green, or yellow in colour; they vary in size, shape, and acidity depending on the variety, of which there are thousands. Apple varieties fall into three broad classes: (1) cider varieties, (2) cooking varieties, and (3) dessert varieties, which differ widely but tend to emphasize colour, size, aroma, smoothness, and perhaps crispness and tang. Many varieties are relatively high in sugar, only mildly acidic, and very low in tannin. In the United States, popular dessert varieties include the Honeycrisp, Red Delicious, Gala, Granny Smith, and Golden Delicious.  Apple varieties that ripen during late summer are generally of poor quality for storage, while those that ripen in late autumn may be stored for as long as one year. For long holding, temperatures only slightly above the freezing point of the fruit are generally desirable. Apples may also be stored in inert gases or in controlled atmospheres. Since the apple requires a considerable period of dormancy, it thrives in areas having a distinct winter period, generally from latitude 30° to 60°, both north and south. Northward, apple growing is limited by low winter temperatures and a short growing season. The soils in which apple trees grow must be well drained; fertilizers can be used if the yield is not high enough. Rolling hilltops or the sloping sides of hills are preferred because they provide “air drainage,” allowing the colder, heavier air to drain away to the valley below during frosty spring nights, when blossoms or young fruit would be destroyed by exposure to cold. Scions of desired varieties are commonly grafted onto hardy nursery seedlings of about 18 months of age; orchard planting follows one or two years later. Management during the six to eight years before appreciable apple production is reached may consist of little more than protection from competing vegetation and pests. Careful attention to pruning is required, however, especially during the first five years, so that the main scaffold branches will be well distributed along the trunk and to prevent development of weak crotches, which can break under heavy fruit loads. With mature trees, a rigorous spraying regime must be followed to protect against insect pests and diseases and possibly to delay spring development, to thin young fruit, and to hold the autumn drop of ripening fruit to a minimum.An apple is a round, edible fruit produced by an apple tree (Malus spp.). Fruit trees of the orchard or domestic apple (Malus domestica), the most widely grown in the genus, are cultivated worldwide. The tree originated in Central Asia, where its wild ancestor, Malus sieversii, is still found. Apples have been grown for thousands of years in Eurasia before they were introduced to North America by European colonists. Apples have cultural significance in many mythologies (including Norse and Greek) and religions (such as Christianity in Europe).\n",
      " Apples grown from seeds tend to be very different from those of their parents, and the resultant fruit frequently lacks desired characteristics. For commercial purposes, including botanical evaluation, apple cultivars are propagated by clonal grafting onto rootstocks. Apple trees grown without rootstocks tend to be larger and much slower to fruit after planting. Rootstocks are used to control the speed of growth and the size of the resulting tree, allowing for easier harvesting.\n",
      " There are more than 7,500 cultivars of apples. Different cultivars are bred for various tastes and uses, including cooking, eating raw, and cider or apple juice production. Trees and fruit are prone to fungal, bacterial, and pest problems, which can be controlled by a number of organic and non-organic means. In 2010, the fruit's genome was sequenced as part of research on disease control and selective breeding in apple production.\n",
      " The word apple, whose Old English ancestor is æppel, is descended from the Proto-Germanic noun *aplaz, descended in turn from Proto-Indo-European *h₂ébōl.[3] As late as the 17th century, the word also functioned as a generic term for all fruit, including nuts. This can be compared to the 14th-century Middle English expression appel of paradis, meaning a banana.[4]\n",
      " The apple is a deciduous tree, generally standing 2 to 4.5 metres (6 to 15 feet) tall in cultivation and up to 15 m (49 ft) in the wild, though more typically 2 to 10 m (6.5 to 33 ft).[5][1] When cultivated, the size, shape and branch density are determined by rootstock selection and trimming method.[5] Apple trees may naturally have a rounded to erect crown with a dense canopy of leaves.[6] The bark of the trunk is dark gray or gray-brown, but young branches are reddish or dark-brown with a smooth texture.[1][7] Young twigs are covered in fine downy hairs; they become hairless when older.[7]\n",
      " The buds are egg-shaped and dark red or purple in color; they range in size from 3 to 5 millimeters, but are usually less than 4 mm. The bud scales have very hairy edges. When emerging from the buds, the leaves are convolute, meaning that their edges overlap each other.[1] Leaves can be  simple ovals (elliptic), medium or wide in width, somewhat egg-shaped with the wider portion toward their base (ovate), or even with sides that are more parallel to each other instead of curved (oblong) with a narrow pointed end.[7][1] The edges have broadly-angled teeth, but do not have lobes. The top surface of the leaves are glabrescent, almost hairless, while the undersides are densely covered in fine hairs.[1] The leaves are attached alternately by short leaf stems 1-to-3.5 cm (1⁄2-to-1+1⁄2 in) long.[6][1]\n",
      " Blossoms are produced in spring simultaneously with the budding of the leaves and are produced on spurs and some long shoots.[5] When the flower buds first begin to open the petals are rose-pink and fade to white or light pink when fully open with each flower 3-to-4-centimeter (1-to-1+1⁄2-inch) in diameter.[1] The five-petaled flowers are group in an inflorescence consisting of a cyme with 3–7 flowers.[8] The central flower of the inflorescence is called the \"king bloom\"; it opens first and can develop a larger fruit.[6] Open apple blossoms are damaged by even brief exposures to temperatures −2 °C (28 °F) or less, although the overwintering wood and buds are hardy down to −40 °C (−40 °F).[8]\n",
      " The fruit is a pome that matures in late summer or autumn.[1] The true fruits or carpels are the harder interior chambers inside the apple's core. There are usually five carpels inside an apple, but there may be as few as three. Each of the chambers contains one or two seeds.[9] The edible flesh is formed from the receptacle at the base of the flower.[10]\n",
      " The seeds are egg- to pear-shaped and may be colored from light brown or tan to a very dark brown, often with red shades or even purplish-black. They may have a blunt or sharp point.[11] The five sepals remain attached and stand out from the surface of the apple.[1]\n",
      " The size of the fruit varies widely between cultivars, but generally has a diameter betwee...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/openai/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"An apple is the edible fruit produced by an apple tree ( *Malus domestica* ), a species in the rose family (Rosaceae).  It's one of the most widely cultivated fruit trees globally, grown for both fresh consumption and commercial processing into products like juice, cider, applesauce, and pie filling.\\n\\n**Key characteristics:**\\n\\n* **Botanical Classification:**  It's a pome fruit, meaning the fleshy part develops from the receptacle of the flower, not just the ovary.\\n* **Origin:** Domesticated from *Malus sieversii* in Central Asia thousands of years ago, it spread across Eurasia and later to the Americas.\\n* **Cultivation:**  Thousands of cultivars exist, each with varying characteristics in size, color (red, green, yellow), flavor, texture (crispness), and acidity.  These are often categorized as dessert, cooking, or cider apples.  Commercial cultivation involves grafting onto rootstocks to control tree size and yield.\\n* **Growth:** Apple trees thrive in temperate climates with distinct winter dormancy periods.  They require cross-pollination for optimal fruit production.\\n* **Nutritional Value:** Apples are a good source of fiber, vitamins (A and C), and carbohydrates.\\n* **Cultural Significance:** Apples hold symbolic meaning across various mythologies and religions.\\n\\n\\nThe detailed description includes information on its cultivation, history, varieties, nutritional aspects, and even its etymology.  There is also detailed botanical description of the tree's leaves, flowers, and fruit structure.\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents import (\n",
    "    OpenAIChatCompletionsModel,\n",
    "    AsyncOpenAI,\n",
    "    Agent,\n",
    "    Runner,\n",
    "    set_tracing_disabled,\n",
    "    function_tool,\n",
    "    handoff,\n",
    "    RunContextWrapper,\n",
    ")\n",
    "import os\n",
    "import dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs, unquote, quote_plus\n",
    "from pydantic import BaseModel\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai\"\n",
    ")\n",
    "\n",
    "\n",
    "class ContentType(BaseModel):\n",
    "    content_extracted_from_urls: str\n",
    "\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\"gemini-1.5-flash\", client)\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def search_query_tool(query: str) -> list[str]:\n",
    "    print(f\"Searching for: {query}...\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    url = f\"https://html.duckduckgo.com/html/?q={quote_plus(query)}\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    results = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for result in soup.select(\".result\")[:5]:\n",
    "            link = (\n",
    "                result.find(\"a\", href=True)[\"href\"]\n",
    "                if result.find(\"a\", href=True)\n",
    "                else \"No URL\"\n",
    "            )\n",
    "\n",
    "            results.append(link)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def extract_page_content_tool(urls: list[str]) -> str:\n",
    "    print(\"Extraction started!!\")\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    text = \"\"\n",
    "\n",
    "    for url in urls:\n",
    "        print(f\"Extracting URL: {url}...\")\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            query = parse_qs(parsed.query)\n",
    "            real_url = unquote(query.get(\"uddg\", [\"\"])[0])\n",
    "\n",
    "            if not real_url.startswith(\"http\"):\n",
    "                real_url = \"https://\" + real_url\n",
    "\n",
    "            response = requests.get(real_url, headers=headers, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Skipping URL due to non-200 status: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "            text += \" \".join(\n",
    "                [p.get_text() for p in paragraphs if p.get_text(strip=True)]\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with URL: {url}\\n -> {e}\")\n",
    "            continue\n",
    "\n",
    "    if text.strip():\n",
    "        return text[:10000]\n",
    "    return \"No valid content found!\"\n",
    "\n",
    "\n",
    "sub_query_generator_agent = Agent(\n",
    "    name=\"Sub Query Generator Agent\",\n",
    "    instructions=\"You are a sub query generator agent, You generator sub queries for a given query.\",\n",
    "    model=model,\n",
    "    output_type=list[str],\n",
    ")\n",
    "\n",
    "\n",
    "def handed_off(ctx: RunContextWrapper, input: str):\n",
    "    print(f\"Handed off with {input.content_extracted_from_urls}...\")\n",
    "\n",
    "\n",
    "@function_tool\n",
    "async def sub_query_generator_tool(query: str):\n",
    "    print(\"Generating sub queries...\")\n",
    "    result = await Runner.run(\n",
    "        sub_query_generator_agent,\n",
    "        f\"Generate 3 more smiler search queries for this search query: {query}\",\n",
    "    )\n",
    "    return result.final_output\n",
    "\n",
    "\n",
    "content_generator_agent = Agent(\n",
    "    name=\"Content Generator Agent\",\n",
    "    instructions=\"You are a content generator agent, You generator factual content based on a given text/context.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "search_query_agent = Agent(\n",
    "    name=\"Search Query Agent\",\n",
    "    instructions=\"You are a search agent, First you generate sub queries for a given query using sub_query_generator_tool then you use search_query_tool for each sub query to get website links, then you extract text from those links using extract_page_content_tool then finally you handoff to content_generator_agent which generates a whole content for the research.\",\n",
    "    model=model,\n",
    "    tools=[search_query_tool, extract_page_content_tool, sub_query_generator_tool],\n",
    "    handoffs=[\n",
    "        handoff(content_generator_agent, on_handoff=handed_off, input_type=ContentType)\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "result = await Runner.run(search_query_agent, \"What is an apple?\")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UserError",
     "evalue": "Strict JSON schema is enabled, but the output type is not valid. Either make the output type strict, or pass output_schema_strict=False to your Agent()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUserError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\agent_output.py:114\u001b[0m, in \u001b[0;36mAgentOutputSchema.__init__\u001b[1;34m(self, output_type, strict_json_schema)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_schema \u001b[38;5;241m=\u001b[39m \u001b[43mensure_strict_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UserError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\strict_schema.py:26\u001b[0m, in \u001b[0;36mensure_strict_json_schema\u001b[1;34m(schema)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _EMPTY_SCHEMA\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ensure_strict_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\strict_schema.py:72\u001b[0m, in \u001b[0;36m_ensure_strict_json_schema\u001b[1;34m(json_schema, path, root)\u001b[0m\n\u001b[0;32m     70\u001b[0m     json_schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequired\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(properties\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     71\u001b[0m     json_schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 72\u001b[0m         key: \u001b[43m_ensure_strict_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprop_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, prop_schema \u001b[38;5;129;01min\u001b[39;00m properties\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     74\u001b[0m     }\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# arrays\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# { 'type': 'array', 'items': {...} }\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\strict_schema.py:59\u001b[0m, in \u001b[0;36m_ensure_strict_json_schema\u001b[1;34m(json_schema, path, root)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m     55\u001b[0m     typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditionalProperties\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_schema\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m json_schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditionalProperties\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     58\u001b[0m ):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditionalProperties should not be set for object types. This could be because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using an older version of Pydantic, or because you configured additional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties to be allowed. If you really need this, update the function or output tool \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto not use a strict schema.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m     )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# object types\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# { 'type': 'object', 'properties': { 'a':  {...} } }\u001b[39;00m\n",
      "\u001b[1;31mUserError\u001b[0m: additionalProperties should not be set for object types. This could be because you're using an older version of Pydantic, or because you configured additional properties to be allowed. If you really need this, update the function or output tool to not use a strict schema.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mUserError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 83\u001b[0m\n\u001b[0;32m     75\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat if Elon Musk haven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt funded OpenAI?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124m1. What is an apple?,\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124m2. Where the apple came from?,\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124m3. Is apple important to eat or optional?\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 83\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(search_agent, queries)\n\u001b[0;32m     84\u001b[0m queries\u001b[38;5;241m.\u001b[39mfinal_output\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:187\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_span \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     handoff_names \u001b[38;5;241m=\u001b[39m [h\u001b[38;5;241m.\u001b[39magent_name \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_handoffs(current_agent)]\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_schema \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_output_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_agent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    188\u001b[0m         output_type_name \u001b[38;5;241m=\u001b[39m output_schema\u001b[38;5;241m.\u001b[39mname()\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:945\u001b[0m, in \u001b[0;36mRunner._get_output_schema\u001b[1;34m(cls, agent)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(agent\u001b[38;5;241m.\u001b[39moutput_type, AgentOutputSchemaBase):\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agent\u001b[38;5;241m.\u001b[39moutput_type\n\u001b[1;32m--> 945\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAgentOutputSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\agent_output.py:116\u001b[0m, in \u001b[0;36mAgentOutputSchema.__init__\u001b[1;34m(self, output_type, strict_json_schema)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_schema \u001b[38;5;241m=\u001b[39m ensure_strict_json_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_schema)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UserError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStrict JSON schema is enabled, but the output type is not valid. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither make the output type strict, or pass output_schema_strict=False to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour Agent()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mUserError\u001b[0m: Strict JSON schema is enabled, but the output type is not valid. Either make the output type strict, or pass output_schema_strict=False to your Agent()"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from agents import (\n",
    "    AsyncOpenAI,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    Agent,\n",
    "    Runner,\n",
    "    set_tracing_disabled,\n",
    "    function_tool,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "class SubQueriesOutputType(BaseModel):\n",
    "    queries: list[str]\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai\"\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\"gemini-1.5-flash\", client)\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def search_query_tool(query: str) -> list[str]:\n",
    "    try:\n",
    "        print(f\"Searching for query {query}...\")\n",
    "        search_result = DDGS().text(\n",
    "            query, region=\"us-en\", safesearch=\"on\", timelimit=\"y\", max_results=5\n",
    "        )\n",
    "\n",
    "        results: list[str] = []\n",
    "\n",
    "        for data in search_result:\n",
    "            results.append(data[\"href\"])\n",
    "        return results\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"Search for query, {query} failed: {str(ex)}\")\n",
    "        return [\"No Result!\"]\n",
    "\n",
    "\n",
    "sub_query_generator_agent = Agent(\n",
    "    name=\"Sub Query Generator Agent\",\n",
    "    instructions=(\n",
    "        \"You are a sub query generator agent.\"\n",
    "        \"Given a main query, your job is to deeply analyze and understand its context and intent. \"\n",
    "        \"Then, generate 3 distinct, insightful sub-queries that each explore a different important aspect, angle, or implication of the main query. \"\n",
    "        \"These sub-queries should help ensure comprehensive coverage of the topic, encourage critical thinking, and guide further research or discussion. \"\n",
    "        \"Be creative and thorough, making sure each sub-query is unique and non-overlapping.\"\n",
    "    ),\n",
    "    model=model,\n",
    "    output_type=SubQueriesOutputType,\n",
    ")\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search Agent\",\n",
    "    instructions=(\n",
    "        \"You are a world-class research assistant specializing in information retrieval. \"\n",
    "        \"Given a list of queries, for each query:\"\n",
    "        \"1. Parse and extract the individual queries from the input string\"\n",
    "        \"2. Use search_query_tool to find relevant URLs for each query\"\n",
    "        \"\"\"Output the result as: query: list_of_urls just like a python dict nothing else\"\"\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[search_query_tool],\n",
    ")\n",
    "\n",
    "query = \"What if Elon Musk haven't funded OpenAI?\"\n",
    "\n",
    "queries = \"\"\"\n",
    "1. What is an apple?,\n",
    "2. Where the apple came from?,\n",
    "3. Is apple important to eat or optional?\n",
    "\"\"\"\n",
    "\n",
    "queries = await Runner.run(search_agent, queries)\n",
    "queries.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 165\u001b[0m\n\u001b[0;32m    162\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich is better apple or samsung?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m sub_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(sub_query_generator_agent, query)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mfinal_output)\n\u001b[0;32m    167\u001b[0m search_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(search_agent, sub_queries\u001b[38;5;241m.\u001b[39mfinal_output)\n\u001b[0;32m    168\u001b[0m formatted_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(formatter_agent, search_result\u001b[38;5;241m.\u001b[39mfinal_output)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from agents import (\n",
    "    AsyncOpenAI,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    Agent,\n",
    "    Runner,\n",
    "    set_tracing_disabled,\n",
    "    function_tool,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "\n",
    "class QueryResult(BaseModel):\n",
    "    query: str\n",
    "    urls: list[str]\n",
    "\n",
    "\n",
    "class SearchResultsOutputType(BaseModel):\n",
    "    results: list[QueryResult]\n",
    "\n",
    "    def get_dict(self) -> dict[str, list[str]]:\n",
    "        return {item.query: item.urls for item in self.results}\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai\"\n",
    ")\n",
    "model = OpenAIChatCompletionsModel(\"gemini-1.5-flash\", client)\n",
    "console = Console()\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def scrape_url_tool(url: str) -> str:\n",
    "    try:\n",
    "        print(f\"Scraping URL: {url}...\")\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = \" \".join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text[:5000] if len(text) > 5000 else text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed to scrape content from {url}: {str(e)}\"\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def search_query_tool(query: str) -> list[str]:\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Searching for Query: {query}...\")\n",
    "            search_result = DDGS().text(\n",
    "                query, region=\"us-en\", safesearch=\"on\", timelimit=\"y\", max_results=3\n",
    "            )\n",
    "\n",
    "            results: list[str] = []\n",
    "\n",
    "            for data in search_result:\n",
    "                results.append(data[\"href\"])\n",
    "            return results\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Search for Query: {query} failed: {str(ex)}\")\n",
    "\n",
    "\n",
    "sub_query_generator_agent = Agent(\n",
    "    name=\"Sub Query Generator Agent\",\n",
    "    instructions=(\n",
    "        \"You are a sub query generator agent.\"\n",
    "        \"Given a main query, your job is to deeply analyze and understand its context and intent. \"\n",
    "        \"Then, generate 3 distinct, insightful small sub-queries that each explore a different important aspect, angle, or implication of the main query. \"\n",
    "        \"These small sub-queries should help ensure comprehensive coverage of the topic, encourage critical thinking, and guide further research or discussion. \"\n",
    "        \"Be creative and thorough, making sure each small sub-query is unique and non-overlapping.\"\n",
    "        \"The format should be something like this: 1. What is an apple? 2. How apple came into existence? 3. Why apple? without any formatting or markdown!\"\n",
    "    ),\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search Agent\",\n",
    "    instructions=(\n",
    "        \"You are a research assistant specializing in information retrieval. \"\n",
    "        \"Given a list of search queries (either as a formatted string or list), your task is to:\"\n",
    "        \"1. Parse and extract the individual search queries from the input\"\n",
    "        \"2. For each search query, use the search_query_tool to find relevant URLs\"\n",
    "        \"3. Present the results clearly, showing each query and its corresponding URLs\"\n",
    "        \"Make sure to clean up the query text (remove numbering, extra whitespace, etc.) before searching.\"\n",
    "        \"Output all the information you allocated so far, including URLS and Queries BOTH!! Output: query: list_of_urls!\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[search_query_tool],\n",
    ")\n",
    "\n",
    "formatter_agent = Agent(\n",
    "    name=\"Formatter Agent\",\n",
    "    instructions=(\n",
    "        \"You are a data formatter. Take the search results and convert them to the exact JSON structure requested. \"\n",
    "        \"Extract the queries and their URLs, then format as JSON with this structure:\"\n",
    "        \"{\"\n",
    "        '  \"results\": ['\n",
    "        '    {\"query\": \"cleaned query text\", \"urls\": [\"url1\", \"url2\", ...]}, '\n",
    "        '    {\"query\": \"another query\", \"urls\": [\"url1\", \"url2\", ...]}'\n",
    "        \"  ]\"\n",
    "        \"}\"\n",
    "        \"Return ONLY the JSON object, nothing else.\"\n",
    "    ),\n",
    "    model=model,\n",
    "    output_type=SearchResultsOutputType,\n",
    ")\n",
    "\n",
    "url_scraper_agent = Agent(\n",
    "    name=\"URL Scraper Agent\",\n",
    "    instructions=(\n",
    "        \"You are a url scraper agent, you are given a list of urls to scrape, you scrape the urls using the scrape_search_tool. \"\n",
    "        \"Use scrape_search_tool for each url individually. \"\n",
    "        \"scrape_search_tool will return the scraped text from the url. \"\n",
    "        \"Output the whole raw text that you scraped from all the urls one by one combined as: text\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[scrape_url_tool],\n",
    ")\n",
    "\n",
    "research_content_generator_agent = Agent(\n",
    "    name=\"Research Content Generator Agent\",\n",
    "    instructions=(\n",
    "        \"You are an expert research content generator. Your task is to analyze raw text data and create a comprehensive, \"\n",
    "        \"well-structured research report. Follow these guidelines:\\n\\n\"\n",
    "        \"1. Create a clear table of contents in markdown format at the beginning\\n\"\n",
    "        \"2. Organize content into logical sections and subsections\\n\"\n",
    "        \"3. Present findings objectively with supporting evidence from the source material\\n\"\n",
    "        \"4. Include relevant statistics, comparisons, and key insights\\n\"\n",
    "        \"5. Use proper formatting with headers, bullet points, and emphasis where needed\\n\"\n",
    "        \"6. Ensure accuracy by only including information found in the source material\\n\"\n",
    "        \"7. Add a 'Sources' section at the end listing all referenced URLs\\n\\n\"\n",
    "        \"Structure your report with:\\n\"\n",
    "        \"- Table of Contents\\n\"\n",
    "        \"- Executive Summary\\n\"\n",
    "        \"- Detailed Analysis by Topic\\n\"\n",
    "        \"- Key Findings\\n\"\n",
    "        \"- Conclusion\\n\"\n",
    "        \"- Sources\\n\\n\"\n",
    "        \"Make the content formal while maintaining academic rigor and accuracy.\"\n",
    "    ),\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "query = \"Which is better apple or samsung?\"\n",
    "\n",
    "sub_queries = await Runner.run(sub_query_generator_agent, query)\n",
    "print(result.final_output)\n",
    "\n",
    "search_result = await Runner.run(search_agent, sub_queries.final_output)\n",
    "formatted_result = await Runner.run(formatter_agent, search_result.final_output)\n",
    "\n",
    "result_dict = formatted_result.final_output.get_dict()\n",
    "\n",
    "for query, urls in result_dict.items():\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"URLs: {urls}\")\n",
    "\n",
    "scraped_result = await Runner.run(url_scraper_agent, str(result_dict.values()))\n",
    "\n",
    "result = await Runner.run(\n",
    "    research_content_generator_agent,\n",
    "    scraped_result.final_output + f\"Sources: {result_dict.values()}\",\n",
    ")\n",
    "console.print(Markdown(result.final_output))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task handed-off...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1. What are the key differences in hardware specifications between Apple and Samsung devices?  \\n2. How do the user experiences and ecosystem integrations of Apple and Samsung affect consumer preferences?  \\n3. In what ways do Apple and Samsung's marketing strategies influence their brand perception and customer loyalty?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from agents import (\n",
    "    AsyncOpenAI,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    Agent,\n",
    "    Runner,\n",
    "    set_tracing_disabled,\n",
    "    function_tool,\n",
    "    handoff,\n",
    "    RunContextWrapper,\n",
    ")\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "from pydantic import BaseModel\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "\n",
    "class QueryResult(BaseModel):\n",
    "    query: str\n",
    "    urls: list[str]\n",
    "\n",
    "\n",
    "class SearchResultsOutputType(BaseModel):\n",
    "    results: list[QueryResult]\n",
    "\n",
    "    def get_dict(self) -> dict[str, list[str]]:\n",
    "        return {item.query: item.urls for item in self.results}\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
    "model = OpenAIChatCompletionsModel(\"openai/gpt-4o-mini\", client)\n",
    "console = Console()\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def scrape_url_tool(url: str) -> str:\n",
    "    try:\n",
    "        print(f\"Scraping URL: {url}...\")\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = \" \".join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text[:5000] if len(text) > 5000 else text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed to scrape content from {url}: {str(e)}\"\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def search_query_tool(query: str) -> list[str]:\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Searching for Query: {query}...\")\n",
    "            search_result = DDGS().text(\n",
    "                query, region=\"us-en\", safesearch=\"on\", timelimit=\"y\", max_results=3\n",
    "            )\n",
    "\n",
    "            results: list[str] = []\n",
    "\n",
    "            for data in search_result:\n",
    "                results.append(data[\"href\"])\n",
    "            return results\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(f\"Search for Query: {query} failed: {str(ex)}\")\n",
    "\n",
    "\n",
    "sub_query_generator_agent = Agent(\n",
    "    name=\"Sub Query Generator Agent\",\n",
    "    instructions=(\n",
    "        \"You are a sub query generator agent.\"\n",
    "        \"Given a main query, your job is to deeply analyze and understand its context and intent. \"\n",
    "        \"Then, generate 3 distinct, insightful small sub-queries that each explore a different important aspect, angle, or implication of the main query. \"\n",
    "        \"These small sub-queries should help ensure comprehensive coverage of the topic, encourage critical thinking, and guide further research or discussion. \"\n",
    "        \"Be creative and thorough, making sure each small sub-query is unique and non-overlapping.\"\n",
    "        \"The format should be something like this: 1. What is an apple? 2. How apple came into existence? 3. Why apple? without any formatting or markdown!\"\n",
    "    ),\n",
    "    model=model,\n",
    "    handoff_description=\"Generate sub-queries to explore different aspects of the main query\",\n",
    "    # handoffs=[orchestrator_agent]\n",
    ")\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search Agent\",\n",
    "    instructions=(\n",
    "        \"You are a research assistant specializing in information retrieval. \"\n",
    "        \"Given a list of search queries (either as a formatted string or list), your task is to:\"\n",
    "        \"1. Parse and extract the individual search queries from the input\"\n",
    "        \"2. For each search query, use the search_query_tool to find relevant URLs\"\n",
    "        \"3. Present the results clearly, showing each query and its corresponding URLs\"\n",
    "        \"Make sure to clean up the query text (remove numbering, extra whitespace, etc.) before searching.\"\n",
    "        \"Output all the information you allocated so far, including URLS and Queries BOTH!! Output: query: list_of_urls!\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[search_query_tool],\n",
    "    handoff_description=\"Search for relevant URLs based on provided queries and return results\",\n",
    "    # handoffs=[orchestrator_agent]\n",
    ")\n",
    "\n",
    "formatter_agent = Agent(\n",
    "    name=\"Formatter Agent\",\n",
    "    instructions=(\n",
    "        \"You are a data formatter. Take the search results and convert them to the exact JSON structure requested. \"\n",
    "        \"Extract the queries and their URLs, then format as JSON with this structure:\"\n",
    "        \"{\"\n",
    "        '  \"results\": ['\n",
    "        '    {\"query\": \"cleaned query text\", \"urls\": [\"url1\", \"url2\", ...]}, '\n",
    "        '    {\"query\": \"another query\", \"urls\": [\"url1\", \"url2\", ...]}'\n",
    "        \"  ]\"\n",
    "        \"}\"\n",
    "        \"Return ONLY the JSON object, nothing else.\"\n",
    "    ),\n",
    "    model=model,\n",
    "    output_type=SearchResultsOutputType,\n",
    "    handoff_description=\"Format search results into structured JSON output\",\n",
    "    # handoffs=[orchestrator_agent]\n",
    ")\n",
    "\n",
    "url_scraper_agent = Agent(\n",
    "    name=\"URL Scraper Agent\",\n",
    "    instructions=(\n",
    "        \"You are a url scraper agent, you are given a list of urls to scrape, you scrape the urls using the scrape_search_tool. \"\n",
    "        \"Use scrape_search_tool for each url individually. \"\n",
    "        \"scrape_search_tool will return the scraped text from the url. \"\n",
    "        \"Output the whole raw text that you scraped from all the urls one by one combined as: text\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[scrape_url_tool],\n",
    "    handoff_description=\"Scrape content from provided URLs and return combined raw text\",\n",
    "    # handoffs=[orchestrator_agent]\n",
    ")\n",
    "\n",
    "research_content_generator_agent = Agent(\n",
    "    name=\"Research Content Generator Agent\",\n",
    "    instructions=(\n",
    "        \"You are an expert research content generator. Your task is to analyze raw text data and create a comprehensive, \"\n",
    "        \"well-structured research report. Follow these guidelines:\\n\\n\"\n",
    "        \"1. Create a clear table of contents in markdown format at the beginning\\n\"\n",
    "        \"2. Organize content into logical sections and subsections\\n\"\n",
    "        \"3. Present findings objectively with supporting evidence from the source material\\n\"\n",
    "        \"4. Include relevant statistics, comparisons, and key insights\\n\"\n",
    "        \"5. Use proper formatting with headers, bullet points, and emphasis where needed\\n\"\n",
    "        \"6. Ensure accuracy by only including information found in the source material\\n\"\n",
    "        \"7. Add a 'Sources' section at the end listing all referenced URLs\\n\\n\"\n",
    "        \"Structure your report with:\\n\"\n",
    "        \"- Table of Contents\\n\"\n",
    "        \"- Executive Summary\\n\"\n",
    "        \"- Detailed Analysis by Topic\\n\"\n",
    "        \"- Key Findings\\n\"\n",
    "        \"- Conclusion\\n\"\n",
    "        \"- Sources\\n\\n\"\n",
    "        \"Make the content formal while maintaining academic rigor and accuracy.\"\n",
    "    ),\n",
    "    model=model,\n",
    "    handoff_description=\"Generate comprehensive research report from scraped content\",\n",
    "    # handoffs=[orchestrator_agent]\n",
    ")\n",
    "\n",
    "\n",
    "def handoff_occur(ctx: RunContextWrapper):\n",
    "    print(\"Task handed-off...\")\n",
    "\n",
    "\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"Orchestrator Agent\",\n",
    "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}  \n",
    "    You are an orchestrator agent that coordinates the research workflow. Your task is to:  \n",
    "    1. Take a main research query and break it down into sub-queries using the transfer_to_sub_query_generator_agent tool  \n",
    "    2. Use the transfer_to_search_agent tool to search for each sub-query  \n",
    "    3. Format the search results using the transfer_to_formatter_agent tool  \n",
    "    4. Scrape the content from the URLs using the transfer_to_url_scraper_agent tool  \n",
    "    5. Generate a comprehensive research report using the transfer_to_research_content_generator_agent tool  \n",
    "      \n",
    "    Always use the handoff tools to delegate tasks to specialized agents. Do not try to do the work yourself.\"\"\",\n",
    "    model=model,\n",
    "    handoffs=[\n",
    "        handoff(sub_query_generator_agent, on_handoff=handoff_occur),\n",
    "        handoff(search_agent, on_handoff=handoff_occur),\n",
    "        handoff(formatter_agent, on_handoff=handoff_occur),\n",
    "        handoff(url_scraper_agent, on_handoff=handoff_occur),\n",
    "        handoff(research_content_generator_agent, on_handoff=handoff_occur),\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = await Runner.run(orchestrator_agent, \"Which is better apple or samsung?\")\n",
    "result.final_output\n",
    "\n",
    "# query = \"Which is better apple or samsung?\"\n",
    "\n",
    "# sub_queries = await Runner.run(sub_query_generator_agent, query)\n",
    "# print(result.final_output)\n",
    "\n",
    "# search_result = await Runner.run(search_agent, sub_queries.final_output)\n",
    "# formatted_result = await Runner.run(formatter_agent, search_result.final_output)\n",
    "\n",
    "# result_dict = formatted_result.final_output.get_dict()\n",
    "\n",
    "# for query, urls in result_dict.items():\n",
    "#     print(f\"\\nQuery: {query}\")\n",
    "#     print(f\"URLs: {urls}\")\n",
    "\n",
    "# scraped_result = await Runner.run(url_scraper_agent, str(result_dict.values()))\n",
    "\n",
    "# result = await Runner.run(research_content_generator_agent, scraped_result.final_output + f\"Sources: {result_dict.values()}\")\n",
    "# console.print(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Query: Consumer preferences and brand loyalty between Apple and Samsung products across different demographics...\n",
      "Searching for Query: Marketing strategies of Apple and Samsung and their impact on market positioning and customer satisfaction...\n",
      "Searching for Query: Key features and specifications that differentiate Apple and Samsung devices in terms of performance, usability, and ecosystem integration...\n",
      "Search for Query: Key features and specifications that differentiate Apple and Samsung devices in terms of performance, usability, and ecosystem integration failed: https://lite.duckduckgo.com/lite/ 202 Ratelimit\n",
      "Scraping URL: ...\n",
      "Scraping URL: https://learnprotips.com/how-apple-is-better-than-samsung/...\n",
      "Scraping URL: https://www.linkedin.com/pulse/apple-vs-samsung-brand-loyalty-market-impact-shankeresh-banerjee-4e2hf...\n",
      "Scraping URL: https://mbareportsguru.com/comparative-analysis-between-apple-samsung/...\n",
      "Scraping URL: https://www.socialsellinator.com/social-selling-blog/apple-content-marketing...\n",
      "Scraping URL: https://www.profolus.com/topics/difference-between-apple-and-samsung-comparing-their-strategies/...\n",
      "Scraping URL: https://customerthink.com/titans-of-cx-how-samsung-and-apple-compete-and-collaborate/...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 183\u001b[0m\n\u001b[0;32m    159\u001b[0m orchestrator_agent \u001b[38;5;241m=\u001b[39m Agent(  \n\u001b[0;32m    160\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrchestrator Agent\u001b[39m\u001b[38;5;124m'\u001b[39m,  \n\u001b[0;32m    161\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m(  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m     ]  \n\u001b[0;32m    180\u001b[0m )  \n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Run the orchestrator  \u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(orchestrator_agent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhich is better apple or samsung?\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m    184\u001b[0m console\u001b[38;5;241m.\u001b[39mprint(Markdown(result\u001b[38;5;241m.\u001b[39mfinal_output))\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:240\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[0;32m    218\u001b[0m     input_guardrail_results, turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_input_guardrails(\n\u001b[0;32m    220\u001b[0m             starting_agent,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m         ),\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[0;32m    241\u001b[0m         agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[0;32m    242\u001b[0m         all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m    243\u001b[0m         original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m    244\u001b[0m         generated_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[0;32m    245\u001b[0m         hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    246\u001b[0m         context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    247\u001b[0m         run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    248\u001b[0m         should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[0;32m    249\u001b[0m         tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[0;32m    250\u001b[0m         previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[0;32m    251\u001b[0m     )\n\u001b[0;32m    252\u001b[0m should_run_agent_start_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    254\u001b[0m model_responses\u001b[38;5;241m.\u001b[39mappend(turn_result\u001b[38;5;241m.\u001b[39mmodel_response)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:775\u001b[0m, in \u001b[0;36mRunner._run_single_turn\u001b[1;34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mextend([generated_item\u001b[38;5;241m.\u001b[39mto_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[0;32m    762\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_response(\n\u001b[0;32m    763\u001b[0m     agent,\n\u001b[0;32m    764\u001b[0m     system_prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    772\u001b[0m     previous_response_id,\n\u001b[0;32m    773\u001b[0m )\n\u001b[1;32m--> 775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_step_result_from_response(\n\u001b[0;32m    776\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    777\u001b[0m     original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m    778\u001b[0m     pre_step_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[0;32m    779\u001b[0m     new_response\u001b[38;5;241m=\u001b[39mnew_response,\n\u001b[0;32m    780\u001b[0m     output_schema\u001b[38;5;241m=\u001b[39moutput_schema,\n\u001b[0;32m    781\u001b[0m     all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m    782\u001b[0m     handoffs\u001b[38;5;241m=\u001b[39mhandoffs,\n\u001b[0;32m    783\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    784\u001b[0m     context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    785\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    786\u001b[0m     tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[0;32m    787\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:815\u001b[0m, in \u001b[0;36mRunner._get_single_step_result_from_response\u001b[1;34m(cls, agent, all_tools, original_input, pre_step_items, new_response, output_schema, handoffs, hooks, context_wrapper, run_config, tool_use_tracker)\u001b[0m\n\u001b[0;32m    805\u001b[0m processed_response \u001b[38;5;241m=\u001b[39m RunImpl\u001b[38;5;241m.\u001b[39mprocess_model_response(\n\u001b[0;32m    806\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    807\u001b[0m     all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    810\u001b[0m     handoffs\u001b[38;5;241m=\u001b[39mhandoffs,\n\u001b[0;32m    811\u001b[0m )\n\u001b[0;32m    813\u001b[0m tool_use_tracker\u001b[38;5;241m.\u001b[39madd_tool_use(agent, processed_response\u001b[38;5;241m.\u001b[39mtools_used)\n\u001b[1;32m--> 815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m RunImpl\u001b[38;5;241m.\u001b[39mexecute_tools_and_side_effects(\n\u001b[0;32m    816\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    817\u001b[0m     original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m    818\u001b[0m     pre_step_items\u001b[38;5;241m=\u001b[39mpre_step_items,\n\u001b[0;32m    819\u001b[0m     new_response\u001b[38;5;241m=\u001b[39mnew_response,\n\u001b[0;32m    820\u001b[0m     processed_response\u001b[38;5;241m=\u001b[39mprocessed_response,\n\u001b[0;32m    821\u001b[0m     output_schema\u001b[38;5;241m=\u001b[39moutput_schema,\n\u001b[0;32m    822\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    823\u001b[0m     context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    824\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    825\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\_run_impl.py:247\u001b[0m, in \u001b[0;36mRunImpl.execute_tools_and_side_effects\u001b[1;34m(cls, agent, original_input, pre_step_items, new_response, processed_response, output_schema, hooks, context_wrapper, run_config)\u001b[0m\n\u001b[0;32m    244\u001b[0m new_step_items\u001b[38;5;241m.\u001b[39mextend(processed_response\u001b[38;5;241m.\u001b[39mnew_items)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# First, lets run the tool calls - function tools and computer actions\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m function_results, computer_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_function_tool_calls(\n\u001b[0;32m    249\u001b[0m         agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    250\u001b[0m         tool_runs\u001b[38;5;241m=\u001b[39mprocessed_response\u001b[38;5;241m.\u001b[39mfunctions,\n\u001b[0;32m    251\u001b[0m         hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    252\u001b[0m         context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    253\u001b[0m         config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    254\u001b[0m     ),\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_computer_actions(\n\u001b[0;32m    256\u001b[0m         agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    257\u001b[0m         actions\u001b[38;5;241m=\u001b[39mprocessed_response\u001b[38;5;241m.\u001b[39mcomputer_actions,\n\u001b[0;32m    258\u001b[0m         hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    259\u001b[0m         context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    260\u001b[0m         config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    261\u001b[0m     ),\n\u001b[0;32m    262\u001b[0m )\n\u001b[0;32m    263\u001b[0m new_step_items\u001b[38;5;241m.\u001b[39mextend([result\u001b[38;5;241m.\u001b[39mrun_item \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m function_results])\n\u001b[0;32m    264\u001b[0m new_step_items\u001b[38;5;241m.\u001b[39mextend(computer_results)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\_run_impl.py:583\u001b[0m, in \u001b[0;36mRunImpl.execute_function_tool_calls\u001b[1;34m(cls, agent, tool_runs, hooks, context_wrapper, config)\u001b[0m\n\u001b[0;32m    580\u001b[0m     function_tool \u001b[38;5;241m=\u001b[39m tool_run\u001b[38;5;241m.\u001b[39mfunction_tool\n\u001b[0;32m    581\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(run_single_tool(function_tool, tool_run\u001b[38;5;241m.\u001b[39mtool_call))\n\u001b[1;32m--> 583\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    586\u001b[0m     FunctionToolResult(\n\u001b[0;32m    587\u001b[0m         tool\u001b[38;5;241m=\u001b[39mtool_run\u001b[38;5;241m.\u001b[39mfunction_tool,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tool_run, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tool_runs, results)\n\u001b[0;32m    596\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\_run_impl.py:545\u001b[0m, in \u001b[0;36mRunImpl.execute_function_tool_calls.<locals>.run_single_tool\u001b[1;34m(func_tool, tool_call)\u001b[0m\n\u001b[0;32m    543\u001b[0m     span_fn\u001b[38;5;241m.\u001b[39mspan_data\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m tool_call\u001b[38;5;241m.\u001b[39marguments\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     _, _, result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    546\u001b[0m         hooks\u001b[38;5;241m.\u001b[39mon_tool_start(context_wrapper, agent, func_tool),\n\u001b[0;32m    547\u001b[0m         (\n\u001b[0;32m    548\u001b[0m             agent\u001b[38;5;241m.\u001b[39mhooks\u001b[38;5;241m.\u001b[39mon_tool_start(context_wrapper, agent, func_tool)\n\u001b[0;32m    549\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mhooks\n\u001b[0;32m    550\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m _coro\u001b[38;5;241m.\u001b[39mnoop_coroutine()\n\u001b[0;32m    551\u001b[0m         ),\n\u001b[0;32m    552\u001b[0m         func_tool\u001b[38;5;241m.\u001b[39mon_invoke_tool(context_wrapper, tool_call\u001b[38;5;241m.\u001b[39marguments),\n\u001b[0;32m    553\u001b[0m     )\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    556\u001b[0m         hooks\u001b[38;5;241m.\u001b[39mon_tool_end(context_wrapper, agent, func_tool, result),\n\u001b[0;32m    557\u001b[0m         (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    561\u001b[0m         ),\n\u001b[0;32m    562\u001b[0m     )\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\tool.py:384\u001b[0m, in \u001b[0;36mfunction_tool.<locals>._create_function_tool.<locals>._on_invoke_tool\u001b[1;34m(ctx, input)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_on_invoke_tool\u001b[39m(ctx: RunContextWrapper[Any], \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _on_invoke_tool_impl(ctx, \u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failure_error_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\tool.py:366\u001b[0m, in \u001b[0;36mfunction_tool.<locals>._create_function_tool.<locals>._on_invoke_tool_impl\u001b[1;34m(ctx, input)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39miscoroutinefunction(the_func):\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mtakes_context:\n\u001b[1;32m--> 366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m the_func(ctx, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_dict)\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m the_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_dict)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\agent.py:218\u001b[0m, in \u001b[0;36mAgent.as_tool.<locals>.run_agent\u001b[1;34m(context, input)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;129m@function_tool\u001b[39m(\n\u001b[0;32m    212\u001b[0m     name_override\u001b[38;5;241m=\u001b[39mtool_name \u001b[38;5;129;01mor\u001b[39;00m _transforms\u001b[38;5;241m.\u001b[39mtransform_string_function_style(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    213\u001b[0m     description_override\u001b[38;5;241m=\u001b[39mtool_description \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    214\u001b[0m )\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_agent\u001b[39m(context: RunContextWrapper, \u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Runner\n\u001b[1;32m--> 218\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    219\u001b[0m         starting_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    221\u001b[0m         context\u001b[38;5;241m=\u001b[39mcontext\u001b[38;5;241m.\u001b[39mcontext,\n\u001b[0;32m    222\u001b[0m     )\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m custom_output_extractor:\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m custom_output_extractor(output)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:240\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[0;32m    218\u001b[0m     input_guardrail_results, turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_input_guardrails(\n\u001b[0;32m    220\u001b[0m             starting_agent,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m         ),\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     turn_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_single_turn(\n\u001b[0;32m    241\u001b[0m         agent\u001b[38;5;241m=\u001b[39mcurrent_agent,\n\u001b[0;32m    242\u001b[0m         all_tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m    243\u001b[0m         original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m    244\u001b[0m         generated_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[0;32m    245\u001b[0m         hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[0;32m    246\u001b[0m         context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    247\u001b[0m         run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    248\u001b[0m         should_run_agent_start_hooks\u001b[38;5;241m=\u001b[39mshould_run_agent_start_hooks,\n\u001b[0;32m    249\u001b[0m         tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[0;32m    250\u001b[0m         previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[0;32m    251\u001b[0m     )\n\u001b[0;32m    252\u001b[0m should_run_agent_start_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    254\u001b[0m model_responses\u001b[38;5;241m.\u001b[39mappend(turn_result\u001b[38;5;241m.\u001b[39mmodel_response)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:762\u001b[0m, in \u001b[0;36mRunner._run_single_turn\u001b[1;34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ItemHelpers\u001b[38;5;241m.\u001b[39minput_to_new_input_list(original_input)\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mextend([generated_item\u001b[38;5;241m.\u001b[39mto_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[1;32m--> 762\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_new_response(\n\u001b[0;32m    763\u001b[0m     agent,\n\u001b[0;32m    764\u001b[0m     system_prompt,\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    766\u001b[0m     output_schema,\n\u001b[0;32m    767\u001b[0m     all_tools,\n\u001b[0;32m    768\u001b[0m     handoffs,\n\u001b[0;32m    769\u001b[0m     context_wrapper,\n\u001b[0;32m    770\u001b[0m     run_config,\n\u001b[0;32m    771\u001b[0m     tool_use_tracker,\n\u001b[0;32m    772\u001b[0m     previous_response_id,\n\u001b[0;32m    773\u001b[0m )\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_step_result_from_response(\n\u001b[0;32m    776\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    777\u001b[0m     original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m     tool_use_tracker\u001b[38;5;241m=\u001b[39mtool_use_tracker,\n\u001b[0;32m    787\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:921\u001b[0m, in \u001b[0;36mRunner._get_new_response\u001b[1;34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[0m\n\u001b[0;32m    918\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mresolve(run_config\u001b[38;5;241m.\u001b[39mmodel_settings)\n\u001b[0;32m    919\u001b[0m model_settings \u001b[38;5;241m=\u001b[39m RunImpl\u001b[38;5;241m.\u001b[39mmaybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[1;32m--> 921\u001b[0m new_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[0;32m    922\u001b[0m     system_instructions\u001b[38;5;241m=\u001b[39msystem_prompt,\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    924\u001b[0m     model_settings\u001b[38;5;241m=\u001b[39mmodel_settings,\n\u001b[0;32m    925\u001b[0m     tools\u001b[38;5;241m=\u001b[39mall_tools,\n\u001b[0;32m    926\u001b[0m     output_schema\u001b[38;5;241m=\u001b[39moutput_schema,\n\u001b[0;32m    927\u001b[0m     handoffs\u001b[38;5;241m=\u001b[39mhandoffs,\n\u001b[0;32m    928\u001b[0m     tracing\u001b[38;5;241m=\u001b[39mget_model_tracing_impl(\n\u001b[0;32m    929\u001b[0m         run_config\u001b[38;5;241m.\u001b[39mtracing_disabled, run_config\u001b[38;5;241m.\u001b[39mtrace_include_sensitive_data\n\u001b[0;32m    930\u001b[0m     ),\n\u001b[0;32m    931\u001b[0m     previous_response_id\u001b[38;5;241m=\u001b[39mprevious_response_id,\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m context_wrapper\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39madd(new_response\u001b[38;5;241m.\u001b[39musage)\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:62\u001b[0m, in \u001b[0;36mOpenAIChatCompletionsModel.get_response\u001b[1;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_response\u001b[39m(\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     48\u001b[0m     system_instructions: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     previous_response_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelResponse:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[0;32m     58\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel),\n\u001b[0;32m     59\u001b[0m         model_config\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mto_json_dict() \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbase_url)},\n\u001b[0;32m     60\u001b[0m         disabled\u001b[38;5;241m=\u001b[39mtracing\u001b[38;5;241m.\u001b[39mis_disabled(),\n\u001b[0;32m     61\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[1;32m---> 62\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_response(\n\u001b[0;32m     63\u001b[0m             system_instructions,\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m     65\u001b[0m             model_settings,\n\u001b[0;32m     66\u001b[0m             tools,\n\u001b[0;32m     67\u001b[0m             output_schema,\n\u001b[0;32m     68\u001b[0m             handoffs,\n\u001b[0;32m     69\u001b[0m             span_generation,\n\u001b[0;32m     70\u001b[0m             tracing,\n\u001b[0;32m     71\u001b[0m             stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m         )\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _debug\u001b[38;5;241m.\u001b[39mDONT_LOG_MODEL_DATA:\n\u001b[0;32m     75\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived model response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:252\u001b[0m, in \u001b[0;36mOpenAIChatCompletionsModel._fetch_response\u001b[1;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream)\u001b[0m\n\u001b[0;32m    246\u001b[0m store \u001b[38;5;241m=\u001b[39m ChatCmplHelpers\u001b[38;5;241m.\u001b[39mget_store_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client(), model_settings)\n\u001b[0;32m    248\u001b[0m stream_options \u001b[38;5;241m=\u001b[39m ChatCmplHelpers\u001b[38;5;241m.\u001b[39mget_stream_options_param(\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client(), model_settings, stream\u001b[38;5;241m=\u001b[39mstream\n\u001b[0;32m    250\u001b[0m )\n\u001b[1;32m--> 252\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    253\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    254\u001b[0m     messages\u001b[38;5;241m=\u001b[39mconverted_messages,\n\u001b[0;32m    255\u001b[0m     tools\u001b[38;5;241m=\u001b[39mconverted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[0;32m    256\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mtemperature),\n\u001b[0;32m    257\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mtop_p),\n\u001b[0;32m    258\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mfrequency_penalty),\n\u001b[0;32m    259\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mpresence_penalty),\n\u001b[0;32m    260\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mmax_tokens),\n\u001b[0;32m    261\u001b[0m     tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m    262\u001b[0m     response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[0;32m    263\u001b[0m     parallel_tool_calls\u001b[38;5;241m=\u001b[39mparallel_tool_calls,\n\u001b[0;32m    264\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    265\u001b[0m     stream_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(stream_options),\n\u001b[0;32m    266\u001b[0m     store\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(store),\n\u001b[0;32m    267\u001b[0m     reasoning_effort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(reasoning_effort),\n\u001b[0;32m    268\u001b[0m     extra_headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mHEADERS, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(model_settings\u001b[38;5;241m.\u001b[39mextra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[0;32m    269\u001b[0m     extra_query\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mextra_query,\n\u001b[0;32m    270\u001b[0m     extra_body\u001b[38;5;241m=\u001b[39mmodel_settings\u001b[38;5;241m.\u001b[39mextra_body,\n\u001b[0;32m    271\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_null_or_not_given(model_settings\u001b[38;5;241m.\u001b[39mmetadata),\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2028\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1987\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2025\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   2026\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m   2027\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 2028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   2029\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2030\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   2031\u001b[0m             {\n\u001b[0;32m   2032\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   2033\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   2034\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   2035\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   2036\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   2037\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   2039\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   2040\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   2041\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   2042\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   2043\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   2044\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   2045\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   2046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   2047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   2048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m   2049\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   2050\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   2051\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   2052\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   2053\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   2054\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   2055\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   2056\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   2057\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   2058\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   2059\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   2060\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   2061\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   2062\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[0;32m   2063\u001b[0m             },\n\u001b[0;32m   2064\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[0;32m   2065\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[0;32m   2066\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[0;32m   2067\u001b[0m         ),\n\u001b[0;32m   2068\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   2069\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   2070\u001b[0m         ),\n\u001b[0;32m   2071\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   2072\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2073\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   2074\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1742\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1729\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1730\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1738\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m   1739\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1740\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1741\u001b[0m     )\n\u001b[1;32m-> 1742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1484\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1484\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m   1485\u001b[0m         request,\n\u001b[0;32m   1486\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m   1487\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1488\u001b[0m     )\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1490\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:1643\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1642\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:1637\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m-> 1637\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m   1639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_models.py:979\u001b[0m, in \u001b[0;36mResponse.aread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;124;03mRead and return the response content.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_content\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([part \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maiter_bytes()])\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_models.py:997\u001b[0m, in \u001b[0;36mResponse.aiter_bytes\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    995\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 997\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maiter_raw():\n\u001b[0;32m    998\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_models.py:1055\u001b[0m, in \u001b[0;36mResponse.aiter_raw\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m   1052\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m-> 1055\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[0;32m   1057\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:176\u001b[0m, in \u001b[0;36mBoundAsyncStream.__aiter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py:271\u001b[0m, in \u001b[0;36mAsyncResponseStream.__aiter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__aiter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__aiter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    404\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_async\\http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__aiter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_async\\http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__aiter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_async\\http11.py:203\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_response_body\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_async\\http11.py:217\u001b[0m, in \u001b[0;36mAsyncHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:35\u001b[0m, in \u001b[0;36mAnyIOStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mreceive(max_bytes\u001b[38;5;241m=\u001b[39mmax_bytes)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mEndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\streams\\tls.py:219\u001b[0m, in \u001b[0;36mTLSStream.receive\u001b[1;34m(self, max_bytes)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreceive\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m65536\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_sslobject_method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_object\u001b[38;5;241m.\u001b[39mread, max_bytes)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\streams\\tls.py:162\u001b[0m, in \u001b[0;36mTLSStream._call_sslobject_method\u001b[1;34m(self, func, *args)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mpending:\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_bio\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m--> 162\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport_stream\u001b[38;5;241m.\u001b[39mreceive()\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EndOfStream:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_bio\u001b[38;5;241m.\u001b[39mwrite_eof()\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:1254\u001b[0m, in \u001b[0;36mSocketStream.receive\u001b[1;34m(self, max_bytes)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mis_set()\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing()\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mis_at_eof\n\u001b[0;32m   1252\u001b[0m ):\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mresume_reading()\n\u001b[1;32m-> 1254\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\u001b[38;5;241m.\u001b[39mread_event\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mpause_reading()\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\locks.py:212\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mappend(fut)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from agents import (\n",
    "    AsyncOpenAI,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    Agent,\n",
    "    Runner,\n",
    "    set_tracing_disabled,\n",
    "    function_tool,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "\n",
    "class QueryResult(BaseModel):\n",
    "    query: str\n",
    "    urls: list[str]\n",
    "\n",
    "\n",
    "class SearchResultsOutputType(BaseModel):\n",
    "    results: list[QueryResult]\n",
    "\n",
    "    def get_dict(self) -> dict[str, list[str]]:\n",
    "        return {item.query: item.urls for item in self.results}\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")\n",
    "model = OpenAIChatCompletionsModel(\"openai/gpt-4o-mini\", client)\n",
    "console = Console()\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def scrape_url_tool(url: str) -> str:\n",
    "    try:\n",
    "        print(f\"Scraping URL: {url}...\")\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = \" \".join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        return text[:5000] if len(text) > 5000 else text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Failed to scrape content from {url}: {str(e)}\"\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def search_query_tool(query: str) -> list[str]:\n",
    "    try:\n",
    "        print(f\"Searching for Query: {query}...\")\n",
    "        search_result = DDGS().text(\n",
    "            query, region=\"us-en\", safesearch=\"on\", timelimit=\"y\", max_results=3\n",
    "        )\n",
    "\n",
    "        results: list[str] = []\n",
    "\n",
    "        for data in search_result:\n",
    "            results.append(data[\"href\"])\n",
    "        return results\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"Search for Query: {query} failed: {str(ex)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Create specialized agents\n",
    "sub_query_generator_agent = Agent(\n",
    "    name=\"Sub Query Generator Agent\",\n",
    "    instructions=(\n",
    "        \"You are a sub query generator agent. \"\n",
    "        \"Given a main query, your job is to deeply analyze and understand its context and intent. \"\n",
    "        \"Then, generate exactly 3 distinct, insightful small sub-queries that each explore a different important aspect, angle, or implication of the main query. \"\n",
    "        \"These small sub-queries should help ensure comprehensive coverage of the topic, encourage critical thinking, and guide further research or discussion. \"\n",
    "        \"Be creative and thorough, making sure each small sub-query is unique and non-overlapping. \"\n",
    "        \"The format should be something like this: 1. What is an apple? 2. How apple came into existence? 3. Why apple? without any formatting or markdown!\"\n",
    "    ),\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search Agent\",\n",
    "    instructions=(\n",
    "        \"You are a research assistant specializing in information retrieval. \"\n",
    "        \"Given a list of search queries (either as a formatted string or list), your task is to: \"\n",
    "        \"1. Parse and extract the individual search queries from the input \"\n",
    "        \"2. For each search query, use the search_query_tool to find relevant URLs \"\n",
    "        \"3. Present the results clearly, showing each query and its corresponding URLs \"\n",
    "        \"Make sure to clean up the query text (remove numbering, extra whitespace, etc.) before searching. \"\n",
    "        \"Output all the information you found, including URLs and Queries BOTH!! Format: query: list_of_urls!\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[search_query_tool],\n",
    ")\n",
    "\n",
    "formatter_agent = Agent(\n",
    "    name=\"Formatter Agent\",\n",
    "    instructions=(\n",
    "        \"You are a data formatter. Take the search results and convert them to the exact JSON structure requested. \"\n",
    "        \"Extract the queries and their URLs, then format as JSON with this structure: \"\n",
    "        '{\"results\": [{\"query\": \"cleaned query text\", \"urls\": [\"url1\", \"url2\", ...]}, {\"query\": \"another query\", \"urls\": [\"url1\", \"url2\", ...]}]} '\n",
    "        \"Return ONLY the JSON object, nothing else.\"\n",
    "    ),\n",
    "    model=model,\n",
    "    output_type=SearchResultsOutputType,\n",
    ")\n",
    "\n",
    "url_scraper_agent = Agent(\n",
    "    name=\"URL Scraper Agent\",\n",
    "    instructions=(\n",
    "        \"You are a url scraper agent. You are given a list of urls to scrape. \"\n",
    "        \"Use scrape_url_tool for each url individually. \"\n",
    "        \"scrape_url_tool will return the scraped text from the url. \"\n",
    "        \"Output the whole raw text that you scraped from all the urls one by one combined.\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[scrape_url_tool],\n",
    ")\n",
    "\n",
    "research_content_generator_agent = Agent(\n",
    "    name=\"Research Content Generator Agent\",\n",
    "    instructions=(\n",
    "        \"You are an expert research content generator. Your task is to analyze raw text data and create a comprehensive, \"\n",
    "        \"well-structured research report. Follow these guidelines:\\n\\n\"\n",
    "        \"1. Create a clear table of contents in markdown format at the beginning\\n\"\n",
    "        \"2. Organize content into logical sections and subsections\\n\"\n",
    "        \"3. Present findings objectively with supporting evidence from the source material\\n\"\n",
    "        \"4. Include relevant statistics, comparisons, and key insights\\n\"\n",
    "        \"5. Use proper formatting with headers, bullet points, and emphasis where needed\\n\"\n",
    "        \"6. Ensure accuracy by only including information found in the source material\\n\"\n",
    "        \"7. Add a 'Sources' section at the end listing all referenced URLs\\n\\n\"\n",
    "        \"Structure your report with:\\n\"\n",
    "        \"- Table of Contents\\n\"\n",
    "        \"- Executive Summary\\n\"\n",
    "        \"- Detailed Analysis by Topic\\n\"\n",
    "        \"- Key Findings\\n\"\n",
    "        \"- Conclusion\\n\"\n",
    "        \"- Sources\\n\\n\"\n",
    "        \"Make the content formal while maintaining academic rigor and accuracy.\"\n",
    "    ),\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Create orchestrator agent with more explicit instructions\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"Orchestrator Agent\",\n",
    "    instructions=(\n",
    "        \"You coordinate research workflows using the available tools. You MUST complete ALL 5 steps in sequence:\\n\\n\"\n",
    "        \"1. FIRST: Use generate_sub_queries to break down the main query into 3 sub-queries\\n\"\n",
    "        \"2. SECOND: Use search_queries to find URLs for each sub-query from step 1\\n\"\n",
    "        \"3. THIRD: Use format_results to structure the search results from step 2 into JSON\\n\"\n",
    "        \"4. FOURTH: Use scrape_urls to get content from the URLs in the formatted results from step 3\\n\"\n",
    "        \"5. FIFTH: Use generate_report to create the final research report from the scraped content from step 4\\n\\n\"\n",
    "        \"IMPORTANT: You must complete ALL 5 steps. Do not stop after any single step. \"\n",
    "        \"Always pass the output from one step as input to the next step. \"\n",
    "        \"The final output should be a comprehensive research report, not just scraped URLs.\"\n",
    "    ),\n",
    "    model=model,\n",
    "    tools=[\n",
    "        sub_query_generator_agent.as_tool(\n",
    "            \"generate_sub_queries\", \"Generate sub-queries from main query\"\n",
    "        ),\n",
    "        search_agent.as_tool(\"search_queries\", \"Search for URLs based on queries\"),\n",
    "        formatter_agent.as_tool(\"format_results\", \"Format search results into JSON\"),\n",
    "        url_scraper_agent.as_tool(\"scrape_urls\", \"Scrape content from URLs\"),\n",
    "        research_content_generator_agent.as_tool(\n",
    "            \"generate_report\", \"Generate final research report\"\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run the orchestrator\n",
    "result = await Runner.run(orchestrator_agent, \"Which is better apple or samsung?\")\n",
    "console.print(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fun_Guardrail(BaseModel):\n",
    "    has_fun: bool\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "fun_guardrail = Agent(\n",
    "    name=\"Fun Guardrail\",\n",
    "    instructions=\"You are a Fun Guardrail Agent, Your job is to find funny stuff\",\n",
    "    model=model,\n",
    "    output_type=Fun_Guardrail,\n",
    ")\n",
    "\n",
    "\n",
    "@input_guardrail\n",
    "async def funGuardrail(\n",
    "    ctx: RunContextWrapper[None], agent: Agent, input: list[TResponseInputItem]\n",
    ") -> GuardrailFunctionOutput:\n",
    "    result = await Runner.run(fun_guardrail, input, context=ctx.context)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=result.final_output, tripwire_triggered=result.final_output.has_fun\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserContext:\n",
    "    uid: str\n",
    "    is_pro_user: bool\n",
    "\n",
    "    # async def fetch_purchase() -> list[Purchase]\n",
    "    #     return\n",
    "\n",
    "\n",
    "agent = Agent[UserContext](...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_instructions(\n",
    "    ctx: RunContextWrapper[UserContext], agent: Agent[UserContext]\n",
    ") -> str:\n",
    "    return f\"The user's name is {ctx.context.name}, Help them with their questions.\"\n",
    "\n",
    "\n",
    "agent = Agent[UserContext](name=\"Triage Agent\", instructions=dynamic_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_agent = Agent(\n",
    "    name=\"Pirate\",\n",
    "    instructions=\"Write like a pirate\",\n",
    "    model=\"o3-mini\",\n",
    ")\n",
    "\n",
    "robot_agent = pirate_agent.clone(\n",
    "    name=\"Robot\",\n",
    "    instructions=\"Write like a robot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fred\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from pydantic import TypeAdapter, ValidationError\n",
    "\n",
    "\n",
    "class User(TypedDict):\n",
    "    name: str\n",
    "    id: int\n",
    "\n",
    "\n",
    "user_list_adapter = TypeAdapter(list[User])\n",
    "user_list = user_list_adapter.validate_python([{\"name\": \"Fred\", \"id\": \"3\"}])\n",
    "try:\n",
    "    user_list_adapter.validate_python([{\"name\": \"7\", \"id\": \"5\", \"other\": \"no\"}])\n",
    "except ValidationError as e:\n",
    "    print(e)\n",
    "\n",
    "# print(repr(user_list_adapter.dump_json(user_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, handoff, RunContextWrapper\n",
    "\n",
    "\n",
    "def on_handoff(ctx: RunContextWrapper[None]):\n",
    "    print(\"Handoff called\")\n",
    "\n",
    "\n",
    "agent = Agent(name=\"My agent\")\n",
    "\n",
    "handoff_obj = handoff(\n",
    "    agent=agent,\n",
    "    on_handoff=on_handoff,\n",
    "    tool_name_override=\"Custom_handoff_tool\",\n",
    "    tool_description_override=\"Create description\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handoff called\n",
      "Yes, water is essential for your health.  It's involved in virtually every bodily function, from regulating body temperature and transporting nutrients to lubricating joints and aiding digestion.  The amount you need depends on factors like your activity level, climate, and overall health.  Staying properly hydrated is crucial for well-being.  However, if you have specific concerns about your hydration or any health issues, it's always best to consult with a doctor or other qualified healthcare professional for personalized advice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from agents import (\n",
    "    AsyncOpenAI,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    Agent,\n",
    "    Runner,\n",
    "    set_tracing_disabled,\n",
    "    handoff,\n",
    "    RunContextWrapper,\n",
    ")\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai\"\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\"gemini-1.5-flash\", client)\n",
    "\n",
    "\n",
    "def on_handoff(ctx: RunContextWrapper[None]):\n",
    "    print(\"Handoff called\")\n",
    "\n",
    "\n",
    "doctor = Agent(name=\"Doctor\", instructions=\"You are a doctor agent.\", model=model)\n",
    "\n",
    "handoff_to_doctor = handoff(\n",
    "    agent=doctor,\n",
    "    on_handoff=on_handoff,\n",
    "    # tool_name_override=\"doctor_agent\",\n",
    "    # tool_description_override=\"A doctor agent, used to answer question related to medical.\",\n",
    ")\n",
    "\n",
    "story_writer = Agent(\n",
    "    name=\"Story Writer\",\n",
    "    instructions=\"\"\" \n",
    "    You are a story writer agent. However, if users ask medical, health, or wellness questions,   \n",
    "    you should hand them off to the doctor agent using the doctor_agent tool.\"\"\",\n",
    "    model=model,\n",
    "    handoffs=[handoff_to_doctor],\n",
    ")\n",
    "result = await Runner.run(story_writer, \"Is water good for me?\")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RunResultBase.to_input_list of RunResult(input='how are you?', new_items=[MessageOutputItem(agent=Agent(name='Main Agent', instructions='You are the main agent. handoff to escalation agent when ever user asked \"how are you\"', handoff_description=None, handoffs=[Handoff(tool_name='transfer_to_escalation_agent', tool_description='Handoff to the Escalation agent agent to handle the request. ', input_json_schema={'properties': {'reason': {'title': 'Reason', 'type': 'string'}}, 'required': ['reason'], 'title': 'EscalationData', 'type': 'object', 'additionalProperties': False}, on_invoke_handoff=<function handoff.<locals>._invoke_handoff at 0x0000026CFA2E6480>, agent_name='Escalation agent', input_filter=None, strict_json_schema=True)], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x0000026CFB212DE0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='I am doing well, thank you for asking. I am ready to assist you with your requests.\\n', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item')], raw_responses=[ModelResponse(output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='I am doing well, thank you for asking. I am ready to assist you with your requests.\\n', type='output_text')], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=50, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=21, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=71), response_id=None)], final_output='I am doing well, thank you for asking. I am ready to assist you with your requests.\\n', input_guardrail_results=[], output_guardrail_results=[], context_wrapper=RunContextWrapper(context=None, usage=Usage(requests=1, input_tokens=50, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=21, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=71)), _last_agent=Agent(name='Main Agent', instructions='You are the main agent. handoff to escalation agent when ever user asked \"how are you\"', handoff_description=None, handoffs=[Handoff(tool_name='transfer_to_escalation_agent', tool_description='Handoff to the Escalation agent agent to handle the request. ', input_json_schema={'properties': {'reason': {'title': 'Reason', 'type': 'string'}}, 'required': ['reason'], 'title': 'EscalationData', 'type': 'object', 'additionalProperties': False}, on_invoke_handoff=<function handoff.<locals>._invoke_handoff at 0x0000026CFA2E6480>, agent_name='Escalation agent', input_filter=None, strict_json_schema=True)], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x0000026CFB212DE0>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True))>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from agents import Agent, handoff, RunContextWrapper\n",
    "\n",
    "\n",
    "class EscalationData(BaseModel):\n",
    "    reason: str\n",
    "\n",
    "\n",
    "async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):\n",
    "    print(f\"Escalation agent called with reason: {input_data.reason}\")\n",
    "\n",
    "\n",
    "escalation_agent = Agent(\n",
    "    name=\"Escalation agent\", instructions=\"You are an escalation agent.\", model=model\n",
    ")\n",
    "\n",
    "handoff_obj = handoff(\n",
    "    agent=escalation_agent,\n",
    "    on_handoff=on_handoff,\n",
    "    input_type=EscalationData,\n",
    "    # input_filter=handoff_filters.remove_all_tools\n",
    ")\n",
    "\n",
    "main_agent = Agent(\n",
    "    name=\"Main Agent\",\n",
    "    instructions='You are the main agent. handoff to escalation agent when ever user asked \"how are you\"',\n",
    "    model=model,\n",
    "    handoffs=[handoff_obj],\n",
    ")\n",
    "\n",
    "result = await Runner.run(main_agent, \"how are you?\")\n",
    "result.to_input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_instructions(\n",
    "    context: RunContextWrapper[UserContext], agent: Agent[UserContext]\n",
    ") -> str:\n",
    "    return f\"The user's name is {context.context.name}. Help them with their questions.\"\n",
    "\n",
    "\n",
    "agent = Agent[UserContext](\n",
    "    name=\"Triage agent\",\n",
    "    instructions=dynamic_instructions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m OpenAIChatCompletionsModel(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m'\u001b[39m, openai_client\u001b[38;5;241m=\u001b[39mclient)\n\u001b[0;32m     15\u001b[0m pro_story_writer \u001b[38;5;241m=\u001b[39m Agent(\n\u001b[0;32m     16\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPro Story Writer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are the pro story writer\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mRunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpro_story_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mI want to create a book named \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDreaming Shadows!\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mfinal_output)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:336\u001b[0m, in \u001b[0;36mRunner.run_sync\u001b[1;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_sync\u001b[39m(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    300\u001b[0m     previous_response_id: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    301\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RunResult:\n\u001b[0;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run a workflow synchronously, starting at the given agent. Note that this just wraps the\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    `run` method, so it will not work if there's already an event loop (e.g. inside an async\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    function, or in a Jupyter notebook or async context like FastAPI). For those cases, use\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m        agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstarting_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:660\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \n\u001b[0;32m    651\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m--> 660\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[0;32m    663\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:619\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    622\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, OpenAIChatCompletionsModel, Runner, set_tracing_disabled\n",
    "\n",
    "gemini_api_key = \"AIzaSyCDkgiTzAOE4GPGvsoRktOCabQOXjAshwE\"\n",
    "\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=gemini_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(model=\"gemini-1.5-flash\", openai_client=client)\n",
    "\n",
    "pro_story_writer = Agent(\n",
    "    name=\"Pro Story Writer\",\n",
    "    instructions=\"You are the pro story writer\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "result = Runner.run_sync(\n",
    "    pro_story_writer, 'I want to create a book named \"Dreaming Shadows!'\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    function_tool,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    AsyncOpenAI,\n",
    "    set_tracing_disabled,\n",
    ")  # type: ignore\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(True)\n",
    "\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "llm = OpenAIChatCompletionsModel(model=\"gemini-1.5-flash\", openai_client=client)\n",
    "\n",
    "# @function_tool\n",
    "# def get_weather(city: str) -> str:\n",
    "#     return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish agent\", instructions=\"You only speak Spanish.\", model=llm\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\", instructions=\"You only speak English\", model=llm\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[spanish_agent, english_agent],\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "# try:\n",
    "# draw_graph(triage_agent)\n",
    "# except Exception as e:\n",
    "#     print(\"Could not draw agent graph. Make sure Graphviz is installed.\")\n",
    "#     print(f\"Error: {str(e)}\")\n",
    "\n",
    "result = await Runner.run(triage_agent, \"say hello in spanish\")\n",
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n",
      "Tracing is disabled. Not creating trace Agent workflow\n",
      "Tracing is disabled. Not creating trace Agent workflow\n",
      "Tracing is disabled. Not creating trace Agent workflow\n",
      "Tracing is disabled. Not creating trace Agent workflow\n",
      "Setting current trace: no-op\n",
      "Setting current trace: no-op\n",
      "Setting current trace: no-op\n",
      "Setting current trace: no-op\n",
      "Setting current trace: no-op\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA27EF520>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA27EF520>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA27EF520>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA27EF520>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA27EF520>\n",
      "Running agent Triage agent (turn 1)\n",
      "Running agent Triage agent (turn 1)\n",
      "Running agent Triage agent (turn 1)\n",
      "Running agent Triage agent (turn 1)\n",
      "Running agent Triage agent (turn 1)\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "[\n",
      "  {\n",
      "    \"content\": \"Handoff to the appropriate agent based on the language of the request.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_spanish_agent\",\n",
      "      \"description\": \"Handoff to the spanish_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_english_agent\",\n",
      "      \"description\": \"Handoff to the english_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"Handoff to the appropriate agent based on the language of the request.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_spanish_agent\",\n",
      "      \"description\": \"Handoff to the spanish_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_english_agent\",\n",
      "      \"description\": \"Handoff to the english_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"Handoff to the appropriate agent based on the language of the request.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_spanish_agent\",\n",
      "      \"description\": \"Handoff to the spanish_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_english_agent\",\n",
      "      \"description\": \"Handoff to the english_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"Handoff to the appropriate agent based on the language of the request.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_spanish_agent\",\n",
      "      \"description\": \"Handoff to the spanish_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_english_agent\",\n",
      "      \"description\": \"Handoff to the english_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"Handoff to the appropriate agent based on the language of the request.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_spanish_agent\",\n",
      "      \"description\": \"Handoff to the spanish_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"transfer_to_english_agent\",\n",
      "      \"description\": \"Handoff to the english_agent agent to handle the request. \",\n",
      "      \"parameters\": {\n",
      "        \"additionalProperties\": false,\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {},\n",
      "        \"required\": []\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": null,\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"id\": \"\",\n",
      "      \"function\": {\n",
      "        \"arguments\": \"{}\",\n",
      "        \"name\": \"transfer_to_spanish_agent\"\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": null,\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"id\": \"\",\n",
      "      \"function\": {\n",
      "        \"arguments\": \"{}\",\n",
      "        \"name\": \"transfer_to_spanish_agent\"\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": null,\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"id\": \"\",\n",
      "      \"function\": {\n",
      "        \"arguments\": \"{}\",\n",
      "        \"name\": \"transfer_to_spanish_agent\"\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": null,\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"id\": \"\",\n",
      "      \"function\": {\n",
      "        \"arguments\": \"{}\",\n",
      "        \"name\": \"transfer_to_spanish_agent\"\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": null,\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"id\": \"\",\n",
      "      \"function\": {\n",
      "        \"arguments\": \"{}\",\n",
      "        \"name\": \"transfer_to_spanish_agent\"\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.HandoffSpanData object at 0x000001BFA24C80D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.HandoffSpanData object at 0x000001BFA24C80D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.HandoffSpanData object at 0x000001BFA24C80D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.HandoffSpanData object at 0x000001BFA24C80D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.HandoffSpanData object at 0x000001BFA24C80D0>\n",
      "Handed Off\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA055BD40>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA055BD40>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA055BD40>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA055BD40>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA055BD40>\n",
      "Running agent spanish_agent (turn 2)\n",
      "Running agent spanish_agent (turn 2)\n",
      "Running agent spanish_agent (turn 2)\n",
      "Running agent spanish_agent (turn 2)\n",
      "Running agent spanish_agent (turn 2)\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA24EB1D0>\n",
      "[\n",
      "  {\n",
      "    \"content\": \"You only speak Spanish.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"id\": \"\",\n",
      "        \"type\": \"function\",\n",
      "        \"function\": {\n",
      "          \"name\": \"transfer_to_spanish_agent\",\n",
      "          \"arguments\": \"{}\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool\",\n",
      "    \"tool_call_id\": \"\",\n",
      "    \"content\": \"{'assistant': 'spanish_agent'}\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"You only speak Spanish.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"id\": \"\",\n",
      "        \"type\": \"function\",\n",
      "        \"function\": {\n",
      "          \"name\": \"transfer_to_spanish_agent\",\n",
      "          \"arguments\": \"{}\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool\",\n",
      "    \"tool_call_id\": \"\",\n",
      "    \"content\": \"{'assistant': 'spanish_agent'}\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"You only speak Spanish.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"id\": \"\",\n",
      "        \"type\": \"function\",\n",
      "        \"function\": {\n",
      "          \"name\": \"transfer_to_spanish_agent\",\n",
      "          \"arguments\": \"{}\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool\",\n",
      "    \"tool_call_id\": \"\",\n",
      "    \"content\": \"{'assistant': 'spanish_agent'}\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"You only speak Spanish.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"id\": \"\",\n",
      "        \"type\": \"function\",\n",
      "        \"function\": {\n",
      "          \"name\": \"transfer_to_spanish_agent\",\n",
      "          \"arguments\": \"{}\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool\",\n",
      "    \"tool_call_id\": \"\",\n",
      "    \"content\": \"{'assistant': 'spanish_agent'}\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"You only speak Spanish.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"translate to spanish: Hello pakistan!\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"id\": \"\",\n",
      "        \"type\": \"function\",\n",
      "        \"function\": {\n",
      "          \"name\": \"transfer_to_spanish_agent\",\n",
      "          \"arguments\": \"{}\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"tool\",\n",
      "    \"tool_call_id\": \"\",\n",
      "    \"content\": \"{'assistant': 'spanish_agent'}\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: NOT_GIVEN\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000001BF9BB91690>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000001BF9BB91690>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000001BF9BB91690>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000001BF9BB91690>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GuardrailSpanData object at 0x000001BF9BB91690>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA0F2B8E0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA0F2B8E0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA0F2B8E0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA0F2B8E0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x000001BFA0F2B8E0>\n",
      "Running agent Country Guardrail Check (turn 1)\n",
      "Running agent Country Guardrail Check (turn 1)\n",
      "Running agent Country Guardrail Check (turn 1)\n",
      "Running agent Country Guardrail Check (turn 1)\n",
      "Running agent Country Guardrail Check (turn 1)\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA2579EB0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA2579EB0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA2579EB0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA2579EB0>\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.GenerationSpanData object at 0x000001BFA2579EB0>\n",
      "[\n",
      "  {\n",
      "    \"content\": \"allowed_country_list: [pakistan], blocked_country_list: [all countries but pakistan not included].\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: {'type': 'json_schema', 'json_schema': {'name': 'final_output', 'strict': True, 'schema': {'properties': {'reasoning': {'title': 'Reasoning', 'type': 'string'}, 'is_blocked_country': {'title': 'Is Blocked Country', 'type': 'boolean'}, 'detected_country': {'title': 'Detected Country', 'type': 'string'}}, 'required': ['reasoning', 'is_blocked_country', 'detected_country'], 'title': 'CountryOutput', 'type': 'object', 'additionalProperties': False}}}\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"allowed_country_list: [pakistan], blocked_country_list: [all countries but pakistan not included].\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: {'type': 'json_schema', 'json_schema': {'name': 'final_output', 'strict': True, 'schema': {'properties': {'reasoning': {'title': 'Reasoning', 'type': 'string'}, 'is_blocked_country': {'title': 'Is Blocked Country', 'type': 'boolean'}, 'detected_country': {'title': 'Detected Country', 'type': 'string'}}, 'required': ['reasoning', 'is_blocked_country', 'detected_country'], 'title': 'CountryOutput', 'type': 'object', 'additionalProperties': False}}}\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"allowed_country_list: [pakistan], blocked_country_list: [all countries but pakistan not included].\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: {'type': 'json_schema', 'json_schema': {'name': 'final_output', 'strict': True, 'schema': {'properties': {'reasoning': {'title': 'Reasoning', 'type': 'string'}, 'is_blocked_country': {'title': 'Is Blocked Country', 'type': 'boolean'}, 'detected_country': {'title': 'Detected Country', 'type': 'string'}}, 'required': ['reasoning', 'is_blocked_country', 'detected_country'], 'title': 'CountryOutput', 'type': 'object', 'additionalProperties': False}}}\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"allowed_country_list: [pakistan], blocked_country_list: [all countries but pakistan not included].\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: {'type': 'json_schema', 'json_schema': {'name': 'final_output', 'strict': True, 'schema': {'properties': {'reasoning': {'title': 'Reasoning', 'type': 'string'}, 'is_blocked_country': {'title': 'Is Blocked Country', 'type': 'boolean'}, 'detected_country': {'title': 'Detected Country', 'type': 'string'}}, 'required': ['reasoning', 'is_blocked_country', 'detected_country'], 'title': 'CountryOutput', 'type': 'object', 'additionalProperties': False}}}\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"content\": \"allowed_country_list: [pakistan], blocked_country_list: [all countries but pakistan not included].\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"\\u00a1Hola Pakist\\u00e1n!\\n\"\n",
      "  }\n",
      "]\n",
      "Tools:\n",
      "[]\n",
      "Stream: False\n",
      "Tool choice: NOT_GIVEN\n",
      "Response format: {'type': 'json_schema', 'json_schema': {'name': 'final_output', 'strict': True, 'schema': {'properties': {'reasoning': {'title': 'Reasoning', 'type': 'string'}, 'is_blocked_country': {'title': 'Is Blocked Country', 'type': 'boolean'}, 'detected_country': {'title': 'Detected Country', 'type': 'string'}}, 'required': ['reasoning', 'is_blocked_country', 'detected_country'], 'title': 'CountryOutput', 'type': 'object', 'additionalProperties': False}}}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"{\\\"detected_country\\\": \\\"pakistan\\\", \\\"is_blocked_country\\\": false, \\\"reasoning\\\": \\\"pakistan is in the allowed_country_list\\\"}\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"{\\\"detected_country\\\": \\\"pakistan\\\", \\\"is_blocked_country\\\": false, \\\"reasoning\\\": \\\"pakistan is in the allowed_country_list\\\"}\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"{\\\"detected_country\\\": \\\"pakistan\\\", \\\"is_blocked_country\\\": false, \\\"reasoning\\\": \\\"pakistan is in the allowed_country_list\\\"}\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"{\\\"detected_country\\\": \\\"pakistan\\\", \\\"is_blocked_country\\\": false, \\\"reasoning\\\": \\\"pakistan is in the allowed_country_list\\\"}\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "LLM resp:\n",
      "{\n",
      "  \"content\": \"{\\\"detected_country\\\": \\\"pakistan\\\", \\\"is_blocked_country\\\": false, \\\"reasoning\\\": \\\"pakistan is in the allowed_country_list\\\"}\",\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"annotations\": null,\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": null\n",
      "}\n",
      "\n",
      "reasoning='pakistan is in the allowed_country_list' is_blocked_country=False detected_country='pakistan'\n",
      "Resetting current trace\n",
      "Resetting current trace\n",
      "Resetting current trace\n",
      "Resetting current trace\n",
      "Resetting current trace\n",
      "¡Hola Pakistán!\n",
      "\n",
      "Guardrail didn't trip - this is unexpected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    function_tool,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    AsyncOpenAI,\n",
    "    set_tracing_disabled,\n",
    "    output_guardrail,\n",
    "    GuardrailFunctionOutput,\n",
    "    OutputGuardrailTripwireTriggered,\n",
    "    RunContextWrapper,\n",
    "    handoff,\n",
    "    enable_verbose_stdout_logging,\n",
    ")  # type: ignore\n",
    "from pydantic import BaseModel\n",
    "\n",
    "enable_verbose_stdout_logging()\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(True)\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "llm = OpenAIChatCompletionsModel(model=\"gemini-1.5-flash\", openai_client=client)\n",
    "\n",
    "\n",
    "class HandoffAgent(BaseModel):\n",
    "    handoff_agent_name: str\n",
    "\n",
    "\n",
    "class MessageOutput(BaseModel):\n",
    "    response: str\n",
    "\n",
    "\n",
    "class CountryOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    is_blocked_country: bool\n",
    "    detected_country: str\n",
    "\n",
    "\n",
    "country_guardrail_agent = Agent(\n",
    "    name=\"Country Guardrail Check\",\n",
    "    instructions=\"allowed_country_list: [pakistan], blocked_country_list: [all countries but pakistan not included].\",\n",
    "    output_type=CountryOutput,\n",
    "    model=llm,\n",
    ")\n",
    "\n",
    "\n",
    "@output_guardrail\n",
    "async def country_guardrail(\n",
    "    ctx: RunContextWrapper, agent: Agent, output: MessageOutput\n",
    ") -> GuardrailFunctionOutput:\n",
    "    result = await Runner.run(country_guardrail_agent, output)\n",
    "    print(result.final_output)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=result.final_output,\n",
    "        tripwire_triggered=result.final_output.is_blocked_country,\n",
    "    )\n",
    "\n",
    "\n",
    "def handed_off(ctx: RunContextWrapper):\n",
    "    print(\"Handed Off\")\n",
    "\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions=\"You only speak Spanish.\",\n",
    "    model=llm,\n",
    "    output_guardrails=[country_guardrail],\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"english_agent\", instructions=\"You only speak English\", model=llm\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[\n",
    "        handoff(spanish_agent, on_handoff=handed_off),\n",
    "        handoff(english_agent, on_handoff=handed_off),\n",
    "    ],\n",
    "    model=llm,\n",
    "    # output_type=MessageOutput\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = await Runner.run(triage_agent, \"translate to spanish: Hello pakistan!\")\n",
    "    print(result.final_output)\n",
    "    print(\"Guardrail didn't trip - this is unexpected\")\n",
    "\n",
    "except OutputGuardrailTripwireTriggered:\n",
    "    print(\"Country guardrail tripped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reasoning='The detected country, United Kingdom, is not in the allowed country list [pakistan] and is therefore a blocked country, according to the blocked country list [all countries but pakistan not included].' is_blocked_country=True detected_country='united kingdom'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OutputGuardrailTripwireTriggered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutputGuardrailTripwireTriggered\u001b[0m          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 51\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(agent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me about uk?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuardrail didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt trip - this is unexpected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:259\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(turn_result\u001b[38;5;241m.\u001b[39mnext_step, NextStepFinalOutput):\n\u001b[1;32m--> 259\u001b[0m     output_guardrail_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_run_output_guardrails(\n\u001b[0;32m    260\u001b[0m         current_agent\u001b[38;5;241m.\u001b[39moutput_guardrails \u001b[38;5;241m+\u001b[39m (run_config\u001b[38;5;241m.\u001b[39moutput_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[0;32m    261\u001b[0m         current_agent,\n\u001b[0;32m    262\u001b[0m         turn_result\u001b[38;5;241m.\u001b[39mnext_step\u001b[38;5;241m.\u001b[39moutput,\n\u001b[0;32m    263\u001b[0m         context_wrapper,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunResult(\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[0;32m    267\u001b[0m         new_items\u001b[38;5;241m=\u001b[39mgenerated_items,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m         context_wrapper\u001b[38;5;241m=\u001b[39mcontext_wrapper,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:897\u001b[0m, in \u001b[0;36mRunner._run_output_guardrails\u001b[1;34m(cls, guardrails, agent, agent_output, context)\u001b[0m\n\u001b[0;32m    891\u001b[0m     _error_tracing\u001b[38;5;241m.\u001b[39mattach_error_to_current_span(\n\u001b[0;32m    892\u001b[0m         SpanError(\n\u001b[0;32m    893\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuardrail tripwire triggered\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    894\u001b[0m             data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguardrail\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mguardrail\u001b[38;5;241m.\u001b[39mget_name()},\n\u001b[0;32m    895\u001b[0m         )\n\u001b[0;32m    896\u001b[0m     )\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputGuardrailTripwireTriggered(result)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOutputGuardrailTripwireTriggered\u001b[0m: Guardrail OutputGuardrail triggered tripwire",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(agent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtell me about uk?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuardrail didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt trip - this is unexpected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mOutputGuardrailTripwireTriggered\u001b[49m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry guardrail tripped\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OutputGuardrailTripwireTriggered' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    function_tool,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    AsyncOpenAI,\n",
    "    set_tracing_disabled,\n",
    "    output_guardrail,\n",
    "    GuardrailFunctionOutput,\n",
    "    RunContextWrapper,\n",
    ")  # type: ignore\n",
    "from pydantic import BaseModel\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(True)\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "llm = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client)\n",
    "\n",
    "\n",
    "class MessageOutput(BaseModel):\n",
    "    response: str\n",
    "\n",
    "\n",
    "class CountryOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    is_blocked_country: bool\n",
    "    detected_country: str\n",
    "\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Country Guardrail Check\",\n",
    "    instructions=\"allowed_country_list: [pakistan], blocked_country_list: [all countries but pakistan not included].\",\n",
    "    model=llm,\n",
    "    output_type=CountryOutput,\n",
    ")\n",
    "\n",
    "\n",
    "@output_guardrail\n",
    "async def country_guardrail(\n",
    "    ctx: RunContextWrapper, agent: Agent, output: MessageOutput\n",
    ") -> GuardrailFunctionOutput:\n",
    "    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)\n",
    "    print(result.final_output)\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=result.final_output,\n",
    "        tripwire_triggered=result.final_output.is_blocked_country,\n",
    "    )\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Customer support agent\",\n",
    "    instructions=\"You are a customer support agent. You help customers with their questions.\",\n",
    "    output_guardrails=[country_guardrail],\n",
    "    model=llm,\n",
    "    output_type=MessageOutput,\n",
    ")\n",
    "\n",
    "try:\n",
    "    await Runner.run(agent, \"tell me about uk?\")\n",
    "    print(\"Guardrail didn't trip - this is unexpected\")\n",
    "\n",
    "except OutputGuardrailTripwireTriggered:\n",
    "    print(\"Country guardrail tripped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing is disabled. Not creating trace Agent workflow\n",
      "Setting current trace: no-op\n",
      "Tracing is disabled. Not creating span <agents.tracing.span_data.AgentSpanData object at 0x0000025DA834AC10>\n",
      "Resetting current trace\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m\n\u001b[0;32m     47\u001b[0m agent_a \u001b[38;5;241m=\u001b[39m Agent(  \n\u001b[0;32m     48\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent A\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[0;32m     49\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are Agent A. Use your tools and then handoff to Agent B.\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\n\u001b[0;32m     58\u001b[0m )  \n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Run the scenario  \u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Runner\u001b[38;5;241m.\u001b[39mrun(  \n\u001b[0;32m     62\u001b[0m     agent_a,\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use all your tools and then handoff to Agent B\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     65\u001b[0m result\u001b[38;5;241m.\u001b[39mfinal_output\n",
      "File \u001b[1;32mc:\\Users\\kk\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\agents\\run.py:200\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[0m\n\u001b[0;32m    197\u001b[0m     current_span\u001b[38;5;241m.\u001b[39mstart(mark_as_current\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    199\u001b[0m     all_tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_tools(current_agent)\n\u001b[1;32m--> 200\u001b[0m     current_span\u001b[38;5;241m.\u001b[39mspan_data\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;241m=\u001b[39m [\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_tools]\n\u001b[0;32m    202\u001b[0m current_turn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_turn \u001b[38;5;241m>\u001b[39m max_turns:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "from agents import (\n",
    "    Agent,\n",
    "    Runner,\n",
    "    handoff,\n",
    "    enable_verbose_stdout_logging,\n",
    "    OpenAIChatCompletionsModel,\n",
    "    AsyncOpenAI,\n",
    "    set_tracing_disabled,\n",
    ")  # type: ignore\n",
    "from agents.handoffs import HandoffInputData\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "set_tracing_disabled(True)\n",
    "api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "model = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client)\n",
    "\n",
    "enable_verbose_stdout_logging()\n",
    "\n",
    "\n",
    "# Define some example function tools\n",
    "def tool_1():\n",
    "    \"\"\"First parallel tool\"\"\"\n",
    "    return \"Result from tool 1\"\n",
    "\n",
    "\n",
    "def tool_2():\n",
    "    \"\"\"Second parallel tool\"\"\"\n",
    "    return \"Result from tool 2\"\n",
    "\n",
    "\n",
    "def tool_3():\n",
    "    \"\"\"Third parallel tool\"\"\"\n",
    "    return \"Result from tool 3\"\n",
    "\n",
    "\n",
    "# Custom input filter that modifies pre_handoff_items\n",
    "def custom_input_filter(handoff_input_data: HandoffInputData) -> HandoffInputData:\n",
    "    # Remove all items from pre_handoff_items (similar to what you asked about)\n",
    "    return HandoffInputData(\n",
    "        input_history=handoff_input_data.input_history,\n",
    "        pre_handoff_items=(),  # Clear pre_handoff_items\n",
    "        new_items=handoff_input_data.new_items,\n",
    "    )\n",
    "\n",
    "\n",
    "# Agent B (target of handoff)\n",
    "agent_b = Agent(\n",
    "    name=\"Agent B\",\n",
    "    instructions=\"You are Agent B, handling tasks after handoff.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Agent A (source agent with tools and handoff)\n",
    "agent_a = Agent(\n",
    "    name=\"Agent A\",\n",
    "    instructions=\"You are Agent A. Use your tools and then handoff to Agent B.\",\n",
    "    tools=[tool_1, tool_2, tool_3],\n",
    "    handoffs=[\n",
    "        handoff(\n",
    "            agent=agent_b,\n",
    "            input_filter=custom_input_filter,  # This modifies pre_handoff_items\n",
    "        )\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Run the scenario\n",
    "result = await Runner.run(\n",
    "    agent_a, input=\"Please use all your tools and then handoff to Agent B\"\n",
    ")\n",
    "result.final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
