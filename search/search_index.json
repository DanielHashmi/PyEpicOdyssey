{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udde0 PyEpicOdyssey: Python Internals &amp; Agentic AI \u2013 Advanced Insights","text":"<p>Dive deep into Python\u2019s internals, advanced mechanics, and the nuts &amp; bolts of modern agent frameworks.  </p>"},{"location":"#what-this-repo-is","title":"\ud83d\ude80 What This Repo Is","text":"<p>Go beyond surface \u2014 see exactly how things tick, break, and can be re-engineered. If you want to truly understand Python (and the new OpenAI Agents SDK), you\u2019re in the right place.</p>"},{"location":"#what-youll-find-inside","title":"\ud83e\udde9 What You\u2019ll Find Inside","text":""},{"location":"#python-deep-dives","title":"\ud83c\udfdb\ufe0f Python Deep Dives","text":"<ul> <li>CPython Internals: Bytecode, VM, memory management, reference counting, garbage collection.</li> <li>Data Model Magic: Dunder methods, MRO, metaclasses, descriptors, and why they matter.</li> <li>Async, Threads &amp; GIL: Threading, multiprocessing, asyncio, and what\u2019s really possible in Python concurrency.</li> <li>Iterators &amp; Generators: How <code>for</code> works, lazy evaluation, <code>yield</code> mechanics.</li> <li>Memory &amp; Performance: Using <code>__slots__</code>, memory optimizations, profiling, and weird edge cases.</li> </ul>"},{"location":"#agentic-ai-openai-agents-sdk","title":"\ud83e\udd16 Agentic AI &amp; OpenAI Agents SDK","text":"<ul> <li>Agent Loop Internals:   Step-by-step breakdowns of what happens in an agent run (See: OpAgentLoop.md)</li> <li>Hooks &amp; Execution Order:   Understand how AgentHooks and RunHooks interact, with MCQs and real trace outputs (See: OpAgentsComplexMCQ.md)</li> <li>Tool Call Logic &amp; Inefficiencies:   Discover silent SDK inefficiencies: parallel tool calls, wasted resources, and how to optimize (See: OpAgentsEfficiencyBug.md)</li> <li>Input Filtering &amp; Handoffs:   The real precedence of input filters during agent handoffs (See: OpAgentsFilteringPrecedence.md)</li> <li>Local LLM Streaming:   How to hook up and stream from a local Jan LLM with the OpenAI-compatible API (See: OpAgentsLocalLLMStreamed.py)</li> <li>Sync vs Async Agent Runs:   What blocks, what doesn\u2019t, and how to work around limitations (See: OpAgentsDiff_run_sync_and_run.py)</li> <li>Tool Use Behaviors &amp; Custom Logic:   How custom tool use behaviors work and when your decision function is called (See: OpAgentsOrderOfTools&amp;CustomToolUseBehavior.md)</li> <li>There is a lot more you can dig &amp; explore...</li> </ul>"},{"location":"#live-code-experiments","title":"\ud83e\uddea Live Code Experiments","text":"<ul> <li>Not just theory.</li> <li>MCQs and \u201cweird cases\u201d to challenge your intuition and solidify real-world understanding.</li> </ul>"},{"location":"#who-this-repo-is-for","title":"\ud83c\udfaf Who This Repo Is For","text":"<ul> <li>You know Python Bro very well \u2014 and want to understand what really happens under the hood.</li> <li>You\u2019re digging the OpenAI Agents SDK or building agentic workflows.</li> <li>You love going deep and want clear explanations, real code, and edge cases that make you say \u201cwait, what?\u201d</li> <li>You want to master advanced stuffs.</li> </ul>"},{"location":"#ready-to-dig-deep","title":"\ud83d\ude80 Ready to dig deep?","text":"<ol> <li>Dive into Python Internals to understand the language better</li> <li>Start with the OpenAI Agents SDK Guide for a comprehensive overview</li> <li>Check out the Practice Examples for hands-on learning</li> <li>Explore Advance Concepts to be confidant when building</li> <li>Gaze OpenAI Agents SDK Advance MindMap to get quick clear concepts</li> <li>Attempt Complex Quizzes to chellange yourself at the deepest levels</li> </ol>"},{"location":"#about-me","title":"\ud83d\udc68\u200d\ud83d\udcbb About Me","text":"<p>I\u2019m Daniel Hashmi a Learner!</p> <ul> <li>I love creative ideas, theories &amp; going deep.</li> <li>Building stuff for devs who want to truly understand.</li> </ul>"},{"location":"#contributions","title":"\ud83e\udd1d Contributions","text":"<ul> <li> <p>Found a Mistake!</p> </li> <li> <p>Want to add a weird Python or agent SDK edge case?</p> </li> <li> <p>Want to add your own experiment or breakdown?  </p> </li> <li> <p>PRs are welcome!  </p> </li> </ul>"},{"location":"#support-share","title":"\u2b50 Support &amp; Share","text":"<p>If these resources helps you: - Give it a \u2b50 on Github. - Share it with other advanced Python or agentic AI learners.</p>"},{"location":"#navigation","title":"\ud83d\udcd6 Navigation","text":"<p>Use the navigation menu on the left to browse through topics, or use the search function at the top to find specific content.</p> <p>Keep digging and diving, keep learning! \ud83d\ude80</p>"},{"location":"OpAgentsOlympus/","title":"OpenAI Agents SDK: Advanced Concepts","text":"<p>Delve into advanced concepts and topics of OpenAI's Agents SDK \ud83e\udd16</p> <p>This folder explores the intricate details and sophisticated mechanisms behind the Agents SDK, including internal architecture, execution flows, feature timing, advanced prompting strategies, and more. Discover how complex features work under the hood to help you master the art of building with OpenAI Agents.</p> <p>Mastering OpenAI Agents SDK \ud83d\udd25</p>"},{"location":"OpAgentsOlympus/OpAgentLoop/","title":"Agent Loop","text":"<p>\u2705 The Exact Steps the Agent Loop Takes When You Send a Request</p> <p>When you call <code>Runner.run()</code> in OpenAI\u2019s Agents SDK, here\u2019s what happens\ud83d\udc47</p>"},{"location":"OpAgentsOlympus/OpAgentLoop/#1-setup-start-tracing","title":"1\ufe0f\u20e3 Setup &amp; Start Tracing \ud83d\udcca","text":"<ul> <li> <p>The runner sets up objects to manage the agent run:</p> </li> <li> <p>A RunContextWrapper keeps track of context, token usage, and state across turns.</p> </li> <li>A tool tracker watches which tools are used on each turn.</li> <li>It sets up tracing so you can follow what the agent does. If tracing is disabled, it skips this.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#2-turn-counter-max-turns-check","title":"2\ufe0f\u20e3 Turn Counter &amp; Max Turns Check \ud83d\udd22","text":"<ul> <li>The loop starts with turn number <code>0</code> and adds <code>1</code> each time.</li> <li>If the turn goes over your max setting, it raises an error and stops execution.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#3-agent-span-tracing-block","title":"3\ufe0f\u20e3 Agent Span (Tracing Block) \ud83c\udfaf","text":"<ul> <li>If no active span yet, it starts one for the current agent.</li> <li>The span stays active until the agent changes (handoff) or the loop ends.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#4-get-tools-handoff-options","title":"4\ufe0f\u20e3 Get Tools &amp; Handoff Options \ud83d\udee0\ufe0f","text":"<ul> <li>Each turn, it collects the tools that the agent can use.</li> <li>It also checks if there are handoff options (if control should move to another agent).</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#5-run-input-guardrails-1st-turn-only","title":"5\ufe0f\u20e3 Run Input Guardrails (1st Turn Only) \ud83d\udee1\ufe0f","text":"<ul> <li>On the first turn, it runs input guardrails (safety checks).</li> <li>If any guardrail triggers a tripwire, it stops and reports an error.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#6-run-agent-start-hooks","title":"6\ufe0f\u20e3 Run Agent Start Hooks \ud83e\ude9d","text":"<ul> <li>The first time an agent runs (or after handoff), it triggers start hooks \u2014 custom code that runs at the start.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#7-get-system-instructions","title":"7\ufe0f\u20e3 Get System Instructions \ud83d\udccb","text":"<ul> <li>The agent provides system instructions (like a system prompt) that help the model understand the task.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#8-send-inputs-to-the-model","title":"8\ufe0f\u20e3 Send Inputs to the Model \ud83e\udde0","text":"<ul> <li>It prepares all needed info: system instructions, input, tool list, settings, etc.</li> <li> <p>It sends this to the model:</p> </li> <li> <p>Non-streaming: waits for the model\u2019s full response.</p> </li> <li>Streaming: handles the model\u2019s response as it comes in, piece-by-piece.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#9-process-the-models-response","title":"9\ufe0f\u20e3 Process the Model\u2019s Response \ud83d\udd0d","text":"<ul> <li> <p>The response is processed to figure out:</p> </li> <li> <p>Did the agent produce a final output?</p> </li> <li>Are tools needed?</li> <li>Is a handoff needed?</li> <li>It updates tool tracking based on what the agent used.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#run-tools-handle-side-effects","title":"\ud83d\udd1f Run Tools &amp; Handle Side Effects \ud83d\udd27","text":"<ul> <li>If tools were called, it runs them and stores their results for the next turn.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#11-decide-what-happens-next","title":"1\ufe0f\u20e31\ufe0f\u20e3 Decide What Happens Next \ud83d\ude80","text":"<ul> <li>If there\u2019s a final output, it runs output guardrails (final checks) and returns the result.</li> <li>If it\u2019s a handoff, it switches to the new agent and resets tracing for the new agent.</li> <li>If it needs to run again, it simply loops for another turn.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#12-handle-errors-clean-up","title":"1\ufe0f\u20e32\ufe0f\u20e3 Handle Errors &amp; Clean Up \ud83d\udea8","text":"<ul> <li>If something goes wrong (like a tripwire or max turns hit), it records the error in the trace.</li> <li>It makes sure spans and traces are finished properly.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentLoop/#when-does-the-loop-stop","title":"\ud83d\udd04 When Does the Loop Stop?","text":"<ul> <li>When the agent gives final output.</li> <li>When an error happens.</li> <li>When max turns are hit.</li> </ul> <p>\ud83d\udca1 Streaming vs Non-Streaming</p> What\u2019s different? Non-Streaming Streaming How the model responds Whole response at once Piece-by-piece How events are handled Directly Put into a queue Error handling Raises error Signals error via queue <p>Hopefully this was simple to understand! \ud83d\udc96</p>"},{"location":"OpAgentsOlympus/OpAgentsComplexMCQ/","title":"Complex MCQ","text":""},{"location":"OpAgentsOlympus/OpAgentsComplexMCQ/#complex-mcq-agent-hooks-and-tool-execution","title":"Complex MCQ: Agent Hooks and Tool Execution","text":"<p>Scenario: You have a parent agent with both <code>RunHooks</code> and <code>AgentHooks</code> configured. This parent agent uses an agent-as-tool (created via <code>agent.as_tool()</code>) that also has its own <code>AgentHooks</code>. The child agent-as-tool calls another regular function tool during its execution.</p> <p>Question: What is the correct sequence of hook executions when the parent agent invokes the agent-as-tool?</p> <p>A)  1. Parent's <code>RunHooks.on_tool_start()</code> 2. Child's <code>AgentHooks.on_start()</code> 3. Child's <code>AgentHooks.on_tool_start()</code> (for the function tool) 4. Child's <code>AgentHooks.on_tool_end()</code> (for the function tool) 5. Child's <code>AgentHooks.on_end()</code> 6. Parent's <code>RunHooks.on_tool_end()</code></p> <p>B) 1. Parent's <code>RunHooks.on_agent_start()</code> 2. Parent's <code>RunHooks.on_tool_start()</code> 3. Child agent execution (no hooks because it's a tool) 4. Parent's <code>RunHooks.on_tool_end()</code> 5. Parent's <code>RunHooks.on_agent_end()</code></p> <p>C) 1. Parent's <code>AgentHooks.on_tool_start()</code> 2. Child's <code>Runner.run()</code> starts with no hooks 3. Child's function tool executes with no hooks 4. Parent's <code>AgentHooks.on_tool_end()</code></p> <p>D) 1. Parent's <code>RunHooks.on_tool_start()</code> and <code>AgentHooks.on_tool_start()</code> (parallel) 2. Child's <code>on_agent_start()</code> (from child's execution context) 3. Child's <code>AgentHooks.on_tool_start()</code> and child's context <code>RunHooks.on_tool_start()</code> (parallel, for function tool) 4. Child's <code>AgentHooks.on_tool_end()</code> and child's context <code>RunHooks.on_tool_end()</code> (parallel, for function tool) 5. Child's <code>on_agent_end()</code> (from child's execution context) 6. Parent's <code>RunHooks.on_tool_end()</code> and <code>AgentHooks.on_tool_end()</code> (parallel)</p> <p>HopeFully You Found The Answer \ud83c\udf89</p> <p>Answer: None of The Above, Instead below is the actual Answer!!</p>"},{"location":"OpAgentsOlympus/OpAgentsComplexMCQ/#these-two-red-ones-should-not-be-considered-since-the-question-directly-starts-from-when-the-parent-agent-invokes-the-agent-as-tool","title":"These two red ones should not be considered since the question directly starts from: \ud83d\udfe9 when the parent agent invokes the agent-as-tool?","text":"<p>\u2b55 Parent Runner: on_agent_start called...</p> <p>\u2b55 Parent Agent: on_start called...</p> <p>\ud83d\udca0 Parent Runner: on_tool_start called...</p> <p>\ud83d\udca0 Parent Agent: on_tool_start called...</p> <p>\ud83d\udca0 Default Runner: on_agent_start called...</p> <p>\ud83d\udca0 Child Agent: on_start called...</p> <p>\ud83d\udca0 Default Runner: on_tool_start called...</p> <p>\ud83d\udca0 Child Agent: on_tool_start called...</p> <p>\ud83d\udca0 Default Runner: on_tool_end called...</p> <p>\ud83d\udca0 Child Agent: on_tool_end called...</p> <p>\ud83d\udca0 Default Runner: on_agent_end called...</p> <p>\ud83d\udca0 Child Agent: on_end called...</p> <p>\ud83d\udca0 Parent Runner: on_tool_end called...</p> <p>\ud83d\udca0 Parent Agent: on_tool_end called...</p> <p>\ud83d\udca0 Parent Runner: on_agent_end called...</p> <p>\ud83d\udca0 Parent Agent: on_end called...</p> <p>\ud83d\udd30 Hello_Tool! I was called by the agent_as_tool() \ud83d\udc48 Tool Answer</p>"},{"location":"OpAgentsOlympus/OpAgentsComplexMCQ/#why","title":"WHY?","text":"<p>\ud83d\udfe3 RunHooks will always run regardless of being configured or not, because if RunHooks are None then by default an instance of RunHooks will be passed to it, even though the async methods inside that class do nothing \"pass\"</p> <p>\ud83d\udfe3 AgentsHooks will only run if they are configured else a function will be executed which do nothing \"pass\"</p> <p>\ud83d\udfe3 both RunHooks and AgentHooks execute concurrently But! you might see RunHooks first, because its the first argument in asyncio.gather and asyncio.gather doesn't guarantee order so this is purely an implementation details!</p>"},{"location":"OpAgentsOlympus/OpAgentsDiff_run_sync_and_run/","title":"Opagentsdiff Run Sync And Run","text":"Source code in OpAgentsOlympus/practice/OpAgentsDiff_run_sync_and_run.py OpAgentsOlympus/practice/OpAgentsDiff_run_sync_and_run.py<pre><code>import asyncio\nimport os\nimport time\nimport dotenv\nimport threading\nfrom agents import Agent, Runner, function_tool, OpenAIChatCompletionsModel, AsyncOpenAI, set_tracing_disabled # type: ignore\n\ndotenv.load_dotenv()\nset_tracing_disabled(True)\n\napi_key = os.environ.get(\"GEMINI_API_KEY\")\nclient = AsyncOpenAI(\n    api_key=api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\nllm = OpenAIChatCompletionsModel(model='gemini-1.5-flash', openai_client=client)\n\n@function_tool\ndef echo(text: str) -&gt; str:\n    return text\n\ndef spin_blocking():\n    \"\"\"A blocking spinner that runs in the main thread - this WON'T work with run_sync\"\"\"\n    while True:\n        print(\"\u27f3\", end=\"\", flush=True)\n        time.sleep(0.1)\n        # This loop blocks the entire thread\n\ndef spin_threaded():\n    \"\"\"A spinner that runs in a separate thread - the only way to get concurrency with run_sync\"\"\"\n    while not stop_spinner:\n        print(\"\u27f3\", end=\"\", flush=True)\n        time.sleep(0.1)\n\ndef main_with_run_sync():\n    global stop_spinner\n    stop_spinner = False\n\n    # Start spinner in a separate thread (only way to get concurrency with run_sync)\n    spinner_thread = threading.Thread(target=spin_threaded, daemon=True)\n    spinner_thread.start()\n\n    agent = Agent(\n        name=\"EchoAgent\",\n        instructions=\"Echo whatever the user says.\",\n        tools=[echo],\n        model=llm,\n    )\n\n    print(\"Starting agent (run_sync)\u2026\", end=\"\")\n    start_time = time.time()\n\n    # This blocks the entire thread until completion\n    result = Runner.run_sync(agent, \"Hello, world!\")\n\n    end_time = time.time()\n    stop_spinner = True\n\n    print(\"\\nAgent done!\")\n    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n    print(\"Final output:\", result.final_output)\n\nasync def main_with_async():\n    \"\"\"For comparison - the original async version\"\"\"\n    async def spin():\n        while True:\n            print(\"\u27f3\", end=\"\", flush=True)\n            await asyncio.sleep(0.1)\n\n    spinner = asyncio.create_task(spin())\n\n    agent = Agent(\n        name=\"EchoAgent\",\n        instructions=\"Echo whatever the user says.\",\n        tools=[echo],\n        model=llm,\n    )\n\n    print(\"Starting agent (async)\u2026\", end=\"\")\n    start_time = time.time()\n    result = await Runner.run(agent, \"Hello, world!\")\n    end_time = time.time()\n    spinner.cancel()\n    print(\"\\nAgent done!\")\n    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n    print(\"Final output:\", result.final_output)\n\ndef demonstrate_blocking_issue():\n    \"\"\"This demonstrates why run_sync blocks everything\"\"\"\n    print(\"=== DEMONSTRATING run_sync BLOCKING ISSUE ===\")\n    print(\"This will NOT show a spinner because run_sync blocks the thread:\")\n\n    agent = Agent(\n        name=\"EchoAgent\",\n        instructions=\"Echo whatever the user says.\",\n        tools=[echo],\n        model=llm,\n    )\n\n    print(\"Starting agent\u2026\", end=\"\")\n\n    # Try to start a spinner - this won't work because run_sync will block\n    # spin_blocking()  # If you uncomment this, it will block before run_sync even starts\n\n    start_time = time.time()\n    result = Runner.run_sync(agent, \"Hello, world!\")  # This blocks everything\n    end_time = time.time()\n\n    # We can't start both spinner and the agent with run_sync at the same time!\n    print(\"Agent done! (no spinner was shown)\")\n    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n    print(\"Final output:\", result.final_output)\n\nif __name__ == \"__main__\":\n    choice = input(\"Choose demo:\\n1. run_sync with threading workaround\\n2. async version (original)\\n3. demonstrate blocking issue\\nEnter 1, 2, or 3: \")\n\n    if choice == \"1\":\n        main_with_run_sync()\n    elif choice == \"2\":\n        asyncio.run(main_with_async())\n    elif choice == \"3\":\n        demonstrate_blocking_issue()\n    else:\n        print(\"Invalid choice, running blocking demonstration:\")\n        demonstrate_blocking_issue()\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgentsEfficiencyBug/","title":"Efficiency Bug","text":""},{"location":"OpAgentsOlympus/OpAgentsEfficiencyBug/#openai-agents-sdk-is-wasting-your-resources-heres-why","title":"\u26a0\ufe0f OpenAI Agents SDK Is Wasting Your Resources  Here's Why","text":"<p>If you're using the <code>stop_on_first_tool</code> behavior in the OpenAI Agents SDK with parallel tool calls enabled... there's a silent inefficiency you need to know.</p> <p>Let me break it down:</p>"},{"location":"OpAgentsOlympus/OpAgentsEfficiencyBug/#what-actually-happens-under-the-hood","title":"\ud83d\udd0d What Actually Happens Under the Hood","text":"<p>When you do this:</p> Python<pre><code>model_settings = ModelSettings(\n    tool_choice=\"auto\",\n    parallel_tool_calls=True,\n),\ntool_use_behavior=\"stop_on_first_tool\"\n</code></pre> <p>And the LLM selects multiple tools like:</p> Python<pre><code>tool_calls = [test_tool_one, test_tool_two]\n</code></pre> <p>Here\u2019s what the agent actually does:</p> <ol> <li>The LLM chooses both tools in a single step</li> <li>The SDK uses <code>asyncio.gather()</code> to run all tool calls in parallel</li> <li>All tools execute to completion even if <code>test_tool_two</code> is expensive or unnecessary</li> <li>Then it simply discards every result except <code>tool_results[0]</code></li> <li>Your system never even uses the output of the other tools</li> </ol>"},{"location":"OpAgentsOlympus/OpAgentsEfficiencyBug/#that-means-tool-results-are-wasted","title":"\ud83d\uddd1\ufe0f That Means: Tool Results Are Wasted","text":"<ul> <li>\u26fd API tokens spent for unused calls</li> <li>\ud83d\udd52 Time lost on long-running or blocking tools</li> <li>\u26a0\ufe0f Side effects triggered even when their results are thrown away</li> <li>\ud83d\udcb8 You're billed, they\u2019re ignored</li> </ul> <p>If you're calling external APIs or hitting databases, this can get really expensive.</p>"},{"location":"OpAgentsOlympus/OpAgentsEfficiencyBug/#why-does-this-happen","title":"\u2753 Why Does This Happen?","text":"<p>It\u2019s a design trade-off.</p> <p>The current SDK chooses determinism over efficiency. It guarantees predictable behavior by always using <code>tool_results[0]</code>, regardless of execution order or timing.</p> <p>But it does not cancel remaining tasks once the first completes. It does not use <code>asyncio.wait(..., return_when=FIRST_COMPLETED)</code>.</p> <p>This means: you pay for all tools, use only one.</p>"},{"location":"OpAgentsOlympus/OpAgentsEfficiencyBug/#what-should-you-do","title":"\u2705 What Should You Do?","text":"<p>If you're using <code>stop_on_first_tool</code>, do one of the following:</p> <ol> <li>Only register one tool per step don't give it choices</li> <li>Avoid <code>parallel_tool_calls=True</code> with <code>stop_on_first_tool</code> behavior</li> <li>Write a custom async executor that uses <code>FIRST_COMPLETED</code> and cancels the rest</li> </ol>"},{"location":"OpAgentsOlympus/OpAgentsEfficiencyBug/#whats-missing-in-the-sdk","title":"\u2699\ufe0f What's Missing in the SDK?","text":"<p>The SDK should ideally support:</p> <ul> <li>Efficient first-response wins behavior</li> <li>Tool cancellation on-the-fly</li> <li>Configurable execution modes for budget-conscious agents</li> </ul> <p>\ud83d\udd01 Until then, it\u2019s up to us to optimize tool usage manually.</p> <p>\ud83d\udcac If you\u2019re building agents with expensive tools, keep this in mind or you might be burning tokens and time for nothing.</p> <p>To see a practical example of this: https://github.com/DanielHashmi/AgenticAIProjects/blob/main/openai_agents_sdk_parallel_tool_calls.py</p> <p>If you found this helpful \u2b50 this repo! Keep Coding \ud83d\udc96</p>"},{"location":"OpAgentsOlympus/OpAgentsFilteringPrecedence/","title":"Filtering Precedence","text":""},{"location":"OpAgentsOlympus/OpAgentsFilteringPrecedence/#input-filter-precedence","title":"Input Filter Precedence","text":"Python<pre><code>input_filter = handoff.input_filter or (\n    run_config.handoff_input_filter if run_config else None\n)\n</code></pre> <p>The precedence logic is implemented in the <code>execute_handoffs</code> method where it first checks for a handoff-specific <code>input_filter</code>, and only falls back to the global <code>run_config.handoff_input_filter</code> if no specific filter is defined</p> <p>This means: 1. If <code>handoff.input_filter</code> is set, it takes precedence and is used 2. If <code>handoff.input_filter</code> is <code>None</code>, then <code>run_config.handoff_input_filter</code> is used as a fallback 3. Only one filter is applied - they are not applied in parallel or combined</p>"},{"location":"OpAgentsOlympus/OpAgentsFilteringPrecedence/#notes","title":"Notes","text":"<p>The handoff input filtering system provides a mechanism to transform conversation state data before it's passed to the target agent during handoffs. The HandoffInputData structure is a frozen dataclass that encapsulates three distinct components of the conversation state:</p> <ul> <li> <p><code>input_history</code>: The original input provided to Runner.run() (either a string or tuple of input items)</p> </li> <li> <p><code>pre_handoff_items</code>: Items generated before the current agent turn, that triggered the handoff </p> </li> <li> <p><code>new_items</code>: Items generated during the current turn, including the handoff trigger and handoff output message</p> </li> </ul> <p>The filter function receives this complete HandoffInputData object and must return a modified HandoffInputData object</p> <p>The filtered data then replaces the original conversation state that gets passed to the target agent, The filtering happens during handoff execution in RunImpl.execute_handoffs().</p> <p>Input Filter in Red Box \ud83d\udd34</p> <p></p>"},{"location":"OpAgentsOlympus/OpAgentsLocalLLMStreamed/","title":"Opagentslocalllmstreamed","text":"Source code in OpAgentsOlympus/practice/OpAgentsLocalLLMStreamed.py OpAgentsOlympus/practice/OpAgentsLocalLLMStreamed.py<pre><code># Imports\nfrom openai import AsyncOpenAI  \nfrom agents import Agent, Runner, OpenAIChatCompletionsModel, set_tracing_disabled\nfrom openai.types.responses import ResponseTextDeltaEvent\nimport asyncio\n\n# Settings\nset_tracing_disabled(True) # Disabled Tracing\ncustom_model = OpenAIChatCompletionsModel(  \n    model=\"cognito-v1:3b\", # Local LLM Using Jan\n    openai_client=AsyncOpenAI(  \n        base_url=\"http://localhost:1337/v1\", # BASE URL of Jan Local Server\n        api_key=\"12345\" # Custom API KEY set in Jan\n    )  \n)  \n\n# Agent\nagent = Agent(  \n    name=\"Assistant\",  \n    instructions=\"You are a helpful assistant\",   \n    model=custom_model  \n)\n\n# Streaming\nasync def main():\n    result = Runner.run_streamed(agent, \"write a haiku about recursion?\")  \n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n            print(event.data.delta, end=\"\", flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgentsNeedsPydantic/","title":"Opagentsneedspydantic","text":"Source code in OpAgentsOlympus/practice/OpAgentsNeedsPydantic.py OpAgentsOlympus/practice/OpAgentsNeedsPydantic.py<pre><code>from datetime import datetime, date\nfrom decimal import Decimal\nfrom typing import Optional, Union, Annotated, Any\nfrom enum import Enum\n\nfrom pydantic import (\n    BaseModel,\n    Field,\n    ConfigDict,\n    field_validator,\n    model_validator,\n    field_serializer,\n    model_serializer,\n    ValidationError,\n    PositiveInt,\n    EmailStr,\n    HttpUrl,\n    SecretStr\n)\n\nclass UserRole(str, Enum):\n    ADMIN = \"admin\"\n    USER = \"user\"\n    GUEST = \"guest\"\n\nclass Address(BaseModel):\n    model_config = ConfigDict(\n        str_strip_whitespace=True,  # Automatically remove leading/trailing spaces from string inputs\n        validate_assignment=True,   # Validate field values when attributes are set after model creation\n        extra='forbid'              # Raise ValidationError when unknown fields are provided in input data\n    )\n\n    street: str = Field(\n        min_length=1, max_length=100\n    )  # Ensure street is between 1-100 characters\n    city: str = Field(\n        min_length=1, max_length=50\n    )  # Ensure city is between 1-50 characters\n    postal_code: str = Field(\n        pattern=r'^\\d{5}(-\\d{4})?$'\n    )  # US postal code validation (5 digits or 5+4 format)\n    country: str = Field(\n        default=\"USA\", frozen=True\n    )  # Default \"USA\", immutable after creation\n\nclass User(BaseModel):\n    model_config = ConfigDict(\n        validate_assignment=True,      # Validate new values when fields are modified after instantiation\n        validate_default=True,         # Validate default values on every model creation for safety\n        strict=False,                  # Enable lax mode allowing type coercion (string \"123\" -&gt; int 123)\n        coerce_numbers_to_str=True,    # Allow number-to-string conversion (123 -&gt; \"123\")\n        extra='allow',                 # Permit additional fields not defined in model schema\n        str_strip_whitespace=True,     # Automatically trim whitespace from all string inputs\n        str_to_lower=False,            # Preserve original string casing (set True for lowercase conversion)\n        str_max_length=1000,           # Global string length limit for all string fields\n        ser_json_timedelta='iso8601',  # Serialize timedelta objects in ISO8601 format\n        ser_json_bytes='base64',       # Serialize bytes objects as base64 encoded strings\n        title=\"User Model\",            # Human-readable model name in generated schemas\n        use_attribute_docstrings=True, # Include docstrings in generated JSON schema\n        frozen=False,                  # Allow field modification after model creation (True makes immutable)\n        populate_by_name=True,         # Accept both field names and aliases during validation\n        from_attributes=True           # Create models from objects with attributes (replaces orm_mode)\n    )\n\n    id: PositiveInt = Field(\n        description=\"Unique user identifier\",\n        examples=[1, 42, 123],\n        gt=0, le=999999\n    )  # PositiveInt type with constraints\n\n    name: str = Field(\n        min_length=2, max_length=50,\n        description=\"User's full name\",\n        alias=\"full_name\"\n    )  # String field with length constraints and alias\n\n    email: EmailStr = Field(\n        description=\"User's email address\",\n        validation_alias=\"email_address\"\n    )  # EmailStr validation with input-only alias\n\n    password: SecretStr = Field(\n        min_length=8,\n        description=\"User password (will be hidden in output)\"\n    )  # SecretStr field with min_length=8\n\n    role: UserRole = Field(\n        default=UserRole.USER\n    )  # Enum field with default value\n\n    is_active: bool = Field(\n        default=True\n    )  # Boolean field with default=True\n\n    age: Optional[int] = Field(\n        default=None, ge=0, le=150,\n        description=\"User's age in years\"\n    )  # Optional integer with age constraints\n\n    balance: Decimal = Field(\n        default=Decimal('0.00'),\n        max_digits=10, decimal_places=2,\n        description=\"Account balance\"\n    )  # Decimal field with precision constraints\n\n    created_at: datetime = Field(\n        default_factory=datetime.now\n    )  # Datetime field with factory function\n\n    birth_date: Optional[date] = None  # Optional date field for birth date tracking\n\n    website: Optional[HttpUrl] = None  # Optional HttpUrl field for website validation\n\n    address: Optional[Address] = None  # Optional nested model field for address composition\n\n    tags: list[str] = Field(\n        default_factory=list,\n        max_length=10,\n        description=\"User tags\"\n    )  # List field with max_length=10 constraint\n\n    metadata: dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Additional user metadata\"\n    )  # Flexible metadata dict field\n\n    phone: Union[str, int] = Field(\n        description=\"Phone number as string or int\",\n        union_mode='left_to_right'\n    )  # Union field for phone number flexibility\n\n    score: Annotated[float, Field(ge=0.0, le=100.0, multiple_of=0.1)] = 0.0  # Float field with range and precision constraints\n\n    @field_validator('name')\n    @classmethod\n    def validate_name(cls, v: str) -&gt; str:\n        if not v.replace(' ', '').isalpha():\n            raise ValueError('Name must contain only letters and spaces')\n        return v.title()  # Auto-title case\n\n    @field_validator('tags')\n    @classmethod\n    def validate_tags(cls, v: list[str]) -&gt; list[str]:\n        return [tag.lower().strip() for tag in v if tag.strip()]  # Clean and normalize tag list\n\n    @model_validator(mode='after')\n    def validate_age_birth_date(self) -&gt; 'User':\n        if self.age is not None and self.birth_date is not None:\n            calculated_age = (date.today() - self.birth_date).days // 365\n            if abs(calculated_age - self.age) &gt; 1:\n                raise ValueError('Age and birth date do not match')\n        return self  # Cross-field validation between age and birth_date\n\n    @field_serializer('password')\n    def serialize_password(self, value: SecretStr) -&gt; str:\n        return \"***HIDDEN***\"  # Hide password content in output\n\n    @model_serializer(mode='wrap')\n    def serialize_model(self, serializer, info):\n        data = serializer(self)\n        data['display_name'] = f\"{self.name} ({self.role.value})\"\n        return data  # Inject computed display_name field in output\n\nif __name__ == \"__main__\":\n    user_data = {\n        \"id\": \"123\",  # String to int coercion\n        \"full_name\": \"  john doe  \",  # Whitespace stripping + title casing\n        \"email_address\": \"john@example.com\",  # Email validation\n        \"password\": \"secretpassword123\",  # Secret string handling\n        \"age\": \"25\",  # String to int coercion\n        \"balance\": \"1234.56\",  # String to Decimal coercion\n        \"birth_date\": \"1998-01-01\",  # String to date coercion\n        \"website\": \"https://johndoe.com\",  # String to HttpUrl coercion\n        \"phone\": 1234567890,  # Union type handling\n        \"tags\": [\"  Developer  \", \"Python\", \"  \"],  # List cleaning\n        \"score\": \"85.5\",  # String to float coercion\n        \"metadata\": {\"department\": \"engineering\", \"level\": 3},  # Dict handling\n        \"extra_field\": \"This will be allowed due to extra='allow'\"  # Extra field allowance\n    }\n\n    try:\n        user = User(**user_data)\n        print(\"User created successfully!\")\n        print(f\"Serialized: {user.model_dump()}\")\n        print(f\"JSON: {user.model_dump_json(indent=2)}\")\n\n    except ValidationError as e:\n        print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgentsOrderOfTools%26CustomToolUseBehavior/","title":"Tool Behavior","text":"<p>When you pass a function to <code>tool_use_behavior</code>, tools are called first, and then your function is called with the tool results to determine if execution should stop or continue.</p> <p>The execution flow works as follows:</p> <ol> <li> <p>Tools execute first: All function tools are executed in parallel using <code>asyncio.gather()</code></p> </li> <li> <p>Your function is called with results: After tools complete, the system calls <code>_check_for_final_output_from_tools()</code> which evaluates your custom function, passing it the <code>RunContextWrapper</code> and list of <code>FunctionToolResult</code> objects</p> </li> <li> <p>Decision based on your function's return: Your function returns a <code>ToolsToFinalOutputResult</code> that determines whether to stop execution (using tool output as final result) or continue the conversation loop</p> </li> </ol>"},{"location":"OpAgentsOlympus/OpAgentsOrderOfTools%26CustomToolUseBehavior/#notes","title":"Notes","text":"<p>This behavior is specific to <code>FunctionTool</code> objects. Hosted tools (like file search, web search) are always processed by the LLM regardless of the <code>tool_use_behavior</code> setting.</p>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/","title":"Prompt Engineering","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#all-you-need-to-know-about-prompt-engineering","title":"All You Need To Know About Prompt Engineering","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#1-be-precise-and-explicit","title":"1. Be precise and explicit","text":"<p>Don\u2019t assume hidden meaning. You must spell out every detail: format, tone, constraints.</p>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#2-structure-your-prompts-carefully","title":"2. Structure your prompts carefully","text":"<ul> <li>Start with clear roles:   <code>\"You are a Technical Writing Assistant\u2026\"</code></li> <li>Define objective, format, and negative constraints explicitly:   *\u201cList 3 best-performing Q1\u202f2025 products, then give 5 strategic bullets, don\u2019t use paragraphs.\u201d.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#3-use-agentic-workflows","title":"3. Use agentic workflows","text":"<p>Turn your LLM into an autonomous agent:</p> <ul> <li>Remind it it's a multi\u2011message agent (\u201cdon\u2019t stop until fully solved\u201d).</li> <li>Encourage planning + reflection between tool/API calls.</li> <li>Insist on tool\u2011calls, not guesses</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#4-utilize-context-adeptly","title":"4. Utilize context adeptly","text":"<ul> <li>Feed long inputs and place essential prompts at beginning and end.</li> <li>Remind it of its role mid\u2011doc to maintain context.</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#5-guide-reasoning-with-chain-of-thought","title":"5. Guide reasoning with chain-of-thought","text":"<p>Despite not being a pure reasoning model, LLMs performs better when asked to \"think step\u2011by\u2011step.\" Phrasing like \u201cFirst X, then Y\u201d yields better accuracy on logical tasks.</p>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#6-few-shot-examples","title":"6. Few-shot / examples","text":"<p>Show desired output patterns. Including examples, that helps ensure structure, tone, and format are followed.</p>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#7-iterate-empirically","title":"7. Iterate empirically","text":"<p>Treat prompt tuning like debugging:</p> <ol> <li>Try basic version</li> <li>Evaluate</li> <li>Adjust one element</li> <li>Repeat Empirical tweaks are key.</li> </ol>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#sample-prompt-template","title":"Sample Prompt Template","text":"<p>Markdown<pre><code>You are a data extractor.\nExtract the following fields:\n- Title\n- Author\n- Date\n- Summary\n\nOutput in JSON.\n[TEXT]\n</code></pre> Markdown<pre><code>You are a math tutor.\nSolve the problem step by step and explain your reasoning.\nProblem: [PROBLEM]\n</code></pre></p>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#types-of-prompting","title":"Types of Prompting","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#zero-shot","title":"\u2726 Zero-shot","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#few-shot","title":"\u2726 Few-shot","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#chain-of-thought-cot","title":"\u2726 Chain-of-Thought (CoT)","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#react-prompting","title":"\u2726 ReAct Prompting","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#tree-of-thoughts-tot","title":"\u2726 Tree of Thoughts (ToT)","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#llm-tuning","title":"LLM Tuning","text":"<p>\u2705 Use temperature:</p>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#0-for-deterministic-answers","title":"0 for deterministic answers","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#07-for-creativity","title":"&gt;0.7 for creativity","text":""},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#react-prompting-example","title":"ReAct Prompting Example","text":"Markdown<pre><code>You are an intelligent agent that reasons step by step and uses tools.\n\nQuestion: What is 23 * 47?\n\nThought: I should calculate step by step.\nAction: Multiply(23, 47)\n\nObservation: 1081\n\nAnswer: 1081\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#tree-of-thoughts-tot-example","title":"Tree of Thoughts (ToT) Example","text":"Markdown<pre><code>You are solving: How to reduce energy consumption in a data center by 30% in 6 months.\n\nGenerate 3 different high-level strategies.\nFor each, list pros and cons.\nThen pick the most promising one to elaborate.\n\nStrategies:\n1. Migrate to cloud.\n2. Optimize cooling.\n3. Use renewable energy.\n\n...\n\n[Model continues]\n</code></pre> <p>\u2705 Role Prompting:</p> <p>\"You are a senior full-stack developer and technical architect...\"</p> <p>\u2705 Zero-shot instructions:</p> <p>Clear goals for each part of the system.</p> <p>\u2705 Few-shot examples:</p> <p>Provide a sample output example.</p> <p>\u2705 Chain-of-Thought:</p> <p>\"Explain your reasoning step by step.\"</p> <p>\u2705 ReAct:</p> <p>\"Thought \u2192 Action \u2192 Observation.\"</p> <p>\u2705 Tree of Thoughts:</p> <p>\"Generate multiple alternatives and evaluate them.\"</p> <p>\u2705 Constraints:</p> <p>Next.js 15, Tailwind.</p> <p>\u2705 Format Control:</p> <p>Markdown and code blocks.</p> <p>\u2705 Proactive Edge-Case Handling:</p> <p>\"Highlight edge cases, scalability concerns, and performance optimizations.\"</p>"},{"location":"OpAgentsOlympus/OpAgentsPromptEng/#summery","title":"Summery","text":"<p>\ud83d\udd39 Clarity: Be precise and explicit. Vague prompts = vague answers.</p> <p>\ud83d\udd39 Context: Give the model everything it needs to know: background, tone, examples.</p> <p>\ud83d\udd39 Constraints: Specify format, length, style, and detail level.</p> <p>\ud83d\udd39 Examples: Provide examples if you want structure or style imitation.</p> <p>\ud83d\udd39 Iteration: Tweak, test, and compare variations.</p>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/","title":"Everything you need to know about RunConfig!","text":""},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#what-is-runconfig","title":"What is RunConfig?","text":"<p><code>RunConfig</code> is a dataclass that holds all the global settings for an entire agent run. When you start a workflow with <code>Runner.run()</code>, <code>Runner.run_sync()</code>, or <code>Runner.run_streamed()</code>, you can pass a <code>RunConfig</code> to override default behaviors and apply consistent settings across all agents in your workflow.</p>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#core-configuration-options","title":"Core Configuration Options","text":""},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#model-configuration","title":"\ud83e\udd16 Model Configuration","text":"<ul> <li><code>model</code>: Override which AI model to use for ALL agents in the run</li> <li>Can be a string like <code>\"gpt-4\"</code> or a <code>Model</code> object</li> <li>If set, this completely overrides each agent's individual model setting</li> <li><code>model_provider</code>: The service that resolves model names (defaults to <code>MultiProvider</code>)</li> <li><code>model_settings</code>: Global model parameters (temperature, max tokens, etc.) that override agent-specific settings</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#guardrails-configuration","title":"\ud83d\udee1\ufe0f Guardrails Configuration","text":"<ul> <li><code>input_guardrails</code>: Safety checks that run on the very first input to your workflow</li> <li><code>output_guardrails</code>: Safety checks that run on the final output before returning results</li> <li>These get combined with any guardrails already defined on individual agents</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#handoff-configuration","title":"\ud83d\udd04 Handoff Configuration","text":"<ul> <li><code>handoff_input_filter</code>: A function that can modify inputs when agents hand off to each other</li> <li>This applies globally unless a specific handoff has its own filter defined</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#tracing-configuration","title":"\ud83d\udcca Tracing Configuration","text":"<p>Tracing helps you monitor and debug your agent workflows: - <code>tracing_disabled</code>: Turn off all tracing for this run - <code>trace_include_sensitive_data</code>: Whether to include actual inputs/outputs in traces (vs just metadata) - <code>workflow_name</code>: A human-readable name for this workflow (like \"Customer Support Bot\") - <code>trace_id</code>: Custom ID for this specific run (auto-generated if not provided) - <code>group_id</code>: Links multiple runs together (like all runs in the same chat conversation) - <code>trace_metadata</code>: Extra information to attach to the trace</p>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#how-to-use-runconfig","title":"How to Use RunConfig","text":""},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#basic-example","title":"Basic Example","text":"Python<pre><code>from agents import Runner, RunConfig\n\n# Create your configuration\nconfig = RunConfig(\n    model=\"gpt-4\",  # Use GPT-4 for all agents\n    workflow_name=\"Email Assistant\",\n    tracing_disabled=False  # Keep tracing on\n)\n\n# Run your agent with the config\nresult = await Runner.run(\n    my_agent,\n    \"Help me write an email\",\n    run_config=config\n)\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#advanced-example","title":"Advanced Example","text":"Python<pre><code>config = RunConfig(\n    model=\"gpt-4o\",\n    model_settings=ModelSettings(temperature=0.1),\n    input_guardrails=[safety_checker, content_filter],\n    output_guardrails=[quality_checker],\n    workflow_name=\"Content Generation Pipeline\",\n    group_id=\"user_session_123\",\n    trace_metadata={\"user_id\": \"user_456\", \"version\": \"v2.1\"}\n)\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#how-model-selection-works","title":"How Model Selection Works","text":"<p>The system picks which model to use in this priority order: 1. <code>RunConfig.model</code> (if it's a Model object) \u2190 Highest priority 2. <code>RunConfig.model</code> (if it's a string, resolved through <code>model_provider</code>) 3. <code>Agent.model</code> (if it's a Model object) 4. <code>Agent.model</code> (if it's a string, resolved through <code>model_provider</code>) \u2190 Lowest priority</p> <p>This means <code>RunConfig</code> always wins over individual agent settings!</p>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#what-happens-behind-the-scenes","title":"What Happens Behind the Scenes","text":"<p>When you pass a <code>RunConfig</code> to <code>Runner.run()</code>:</p> <ol> <li>Initialization: If you don't provide one, it creates <code>RunConfig()</code> with defaults</li> <li>Tracing Setup: Your tracing settings are applied using <code>TraceCtxManager</code></li> <li>Model Resolution: Each time an agent needs to run, the system uses <code>_get_model()</code> to pick the right model based on your config</li> <li>Guardrails Merging: Your global guardrails get combined with each agent's individual guardrails</li> <li>Execution: All agents in the workflow use these consistent settings</li> </ol>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#key-benefits","title":"Key Benefits","text":"<ul> <li>Consistency: All agents in your workflow use the same model and settings</li> <li>Runtime Flexibility: Change behavior without modifying agent code</li> <li>Environment Control: Different configs for development, testing, and production</li> <li>Monitoring: Comprehensive tracing and debugging capabilities</li> <li>Safety: Global guardrails ensure all agents follow the same safety rules</li> </ul>"},{"location":"OpAgentsOlympus/OpAgentsRunConfig/#important-notes","title":"Important Notes","text":"<ul> <li>If you don't provide a <code>RunConfig</code>, the system creates one with sensible defaults</li> <li>Global settings in <code>RunConfig</code> always override individual agent settings</li> <li>Only the first agent's input guardrails run, but they're combined with global ones</li> <li>Output guardrails run on the final result, combining agent-specific and global ones</li> <li>Tracing can be completely disabled for privacy-sensitive applications</li> </ul> <p>If this really helped you please give a \u2b50</p>"},{"location":"OpAgentsOlympus/OpAgentsSlots/","title":"Slots","text":"<p><code>__slots__</code> is a Python class attribute that fundamentally changes how Python stores instance attributes. </p>"},{"location":"OpAgentsOlympus/OpAgentsSlots/#normal-python-classes-without-__slots__","title":"Normal Python Classes (without <code>__slots__</code>)","text":"<p>By default, Python stores instance attributes in a dictionary called <code>__dict__</code> for each object.  This means: - You can add any attribute to an instance at runtime - Each instance has its own <code>__dict__</code> dictionary - Memory overhead includes the dictionary structure</p>"},{"location":"OpAgentsOlympus/OpAgentsSlots/#with-__slots__","title":"With <code>__slots__</code>","text":"<p>When you define <code>__slots__</code>, Python: 1. Eliminates the <code>__dict__</code> - No dictionary is created for each instance  2. Pre-allocates fixed slots - Creates a fixed number of memory slots for the specified attributes 3. Restricts attribute assignment - You can only set attributes listed in <code>__slots__</code> </p>"},{"location":"OpAgentsOlympus/OpAgentsSlots/#concrete-example-from-the-codebase","title":"Concrete Example from the Codebase","text":"<p>Looking at <code>AgentSpanData</code>, it defines <code>__slots__ = (\"name\", \"handoffs\", \"tools\", \"output_type\")</code>. This means:</p> <ul> <li>Each <code>AgentSpanData</code> instance can only have these 4 attributes</li> <li>No <code>__dict__</code> is created</li> <li>Memory is pre-allocated for exactly these 4 slots</li> <li>You cannot do <code>instance.some_random_attribute = value</code> - it will raise an <code>AttributeError</code></li> </ul>"},{"location":"OpAgentsOlympus/OpAgentsSlots/#memory-impact","title":"Memory Impact","text":"<p>For classes like <code>FunctionSpanData</code> with <code>__slots__ = (\"name\", \"input\", \"output\", \"mcp_data\")</code>, each instance uses significantly less memory because it avoids the overhead of a dictionary. In a tracing system that might create thousands of span objects, this memory savings adds up quickly. </p>"},{"location":"OpAgentsOlympus/OpAgentsSlots/#notes","title":"Notes","text":"<p>The trade-off is flexibility vs efficiency - you lose the ability to dynamically add attributes but gain substantial memory and performance benefits. This is why it's used extensively in the span data classes where the structure is well-defined and performance matters.</p>"},{"location":"OpAgentsOlympus/OpAgentsStreaming%26Filtering/","title":"Streaming & Filtering","text":""},{"location":"OpAgentsOlympus/OpAgentsStreaming%26Filtering/#question","title":"Question","text":"<p>What does this mean in docs? IMPORTANT: in streaming mode, we will not stream anything as a result of this function. The items generated before will already have been streamed.</p>"},{"location":"OpAgentsOlympus/OpAgentsStreaming%26Filtering/#answer","title":"Answer","text":"<p>This comment refers to the <code>input_filter</code> property of the <code>Handoff</code> class, specifically explaining its behavior in streaming mode.</p> <p>The comment explains that when using streaming execution <code>Runner.run_streamed()</code>, the <code>input_filter</code> function in handoffs will not generate any new streaming events.</p> <p>\ud83d\udca0 Here's what this means:</p> <p>In streaming mode: When you call <code>Runner.run_streamed()</code>, the system streams events as they're generated during agent execution. However, when a handoff occurs and the <code>input_filter</code> function modifies the conversation history, those modifications don't trigger new streaming events.</p> <p>Why this matters: The <code>input_filter</code> allows you to modify inputs passed to the next agent (like removing older messages or filtering tool calls). But since this filtering happens after items have already been streamed to the client, the filtered/modified items won't be re-streamed.</p> <p>Practical implication: If you're building a UI that shows streaming updates, you'll see all the original items as they're generated, but you won't see additional events when the handoff's <code>input_filter</code> modifies those items for the next agent. The filtering is purely internal to prepare the input for the next agent in the chain.</p> <p>The reason is timing - by the time the handoff's <code>input_filter</code> runs, the items it's filtering have already been streamed to the client. The filtering only affects what the next agent in the chain will see as input, not what the client receives in the stream. Input filtering happens after streaming has already occurred for the current turn.</p> <p></p> <p>https://openai.github.io/openai-agents-PyDeepOlympus/ref/handoffs/#agents.handoffs.Handoff</p>"},{"location":"OpAgentsOlympus/OpAgentsStrictMode/","title":"Strict Mode","text":""},{"location":"OpAgentsOlympus/OpAgentsStrictMode/#the-real-difference-between-strict-and-non-strict-mode","title":"The Real Difference Between Strict and Non-Strict Mode","text":"<p>The <code>strict_json_schema</code> setting doesn't control whether errors are raised during tool execution, it controls what schema is sent to the LLM to guide its JSON generation.</p> <p>Strict Mode (<code>strict_json_schema=True</code>): - The schema sent to the LLM includes <code>additionalProperties: false</code> - This instructs the LLM not to generate extra properties in the first place - The dynamically created Pydantic model processes whatever JSON is received, extracting only the expected fields</p> <p>Non-Strict Mode (<code>strict_json_schema=False</code>): - The schema sent to the LLM allows additional properties - The LLM is more likely to generate extra properties since the schema permits them - The dynamically created Pydantic model processes the JSON the same way, extracting only expected fields</p>"},{"location":"OpAgentsOlympus/OpAgentsStrictMode/#why-both-cases-work-without-errors","title":"Why Both Cases \"Work\" Without Errors","text":"<p>In both cases the tool execution succeeds because:</p> <ol> <li>The JSON is valid regardless of extra properties</li> <li>The SDK creates Pydantic models dynamically using <code>create_model()</code> with <code>BaseModel</code> as the base</li> <li>Pydantic's default behavior with <code>BaseModel</code> is to ignore extra fields during validation</li> <li>The function receives the expected parameters after validation</li> </ol> <p>Key Technical Detail: The SDK relies on standard Pydantic behavior where models created with <code>BaseModel</code> ignore extra fields by default. This isn't special SDK logic, it's how Pydantic works when validating JSON input against dynamically created models.</p>"},{"location":"OpAgentsOlympus/OpAgentsStrictMode/#the-real-purpose-of-strict-mode","title":"The Real Purpose of Strict Mode","text":"<p>Strict mode's value is prevention, not error handling. It reduces the likelihood that the LLM will generate malformed or inefficient JSON. This leads to:</p> <ul> <li>LLM doesn't waste tokens on unnecessary properties</li> <li>LLM follows the schema more precisely  </li> <li>When issues occur, they're more likely to be in your expected parameters</li> </ul> <p>Important Caveat: Strict mode can raise <code>UserError</code> exceptions during schema creation if certain types (like mappings) cannot be made strict-compliant, but this happens at tool definition time, not during execution.</p> <p>Happy Coding \u2728</p>"},{"location":"OpAgentsOlympus/OpAgentsTokenConsumption/","title":"Token Consumption","text":"<p>Only LLM consume tokens, the token consumption includes various components that are part of those LLM requests and responses. </p> <p>1. Input Tokens Include Tool Schemas When tools are provided to an agent, their schemas are sent as part of the input tokens to the model.</p> <p>The <code>Converter.convert_tools()</code> method processes tool definitions into the format expected by the API, and these tool schemas consume input tokens.</p> <p>2. Output Tokens Include Function Call Arguments When the model decides to call a tool, the function call arguments are generated as output tokens.</p> <p>The usage tracking captures both <code>completion_tokens</code> (output) and <code>prompt_tokens</code> (input) from the model response.</p> <p>3. Usage Tracking Captures All Components The SDK's <code>Usage</code> class tracks comprehensive token consumption including input tokens (system instructions, conversation history, tool schemas), output tokens (responses, function calls), reasoning tokens, and cached tokens. </p> <p>This pattern is consistent across all model implementations: - OpenAI Responses API:  - OpenAI Chat Completions:  - LiteLLM: </p> <p>Token consumption is fundamentally tied to LLM Requests and Responses, but those include more than just the user's message, they encompass tool schemas, system instructions, conversation history, and generated function calls. The SDK provides comprehensive tracking of all these token-consuming components through the <code>Usage</code> class.</p>"},{"location":"OpAgentsOlympus/OpAgentsTools/","title":"Everything you need to know about Tools In OpenAI Agents SDK","text":""},{"location":"OpAgentsOlympus/OpAgentsTools/#tool-system-architecture","title":"Tool System Architecture","text":"<p>The SDK supports three primary tool types:</p> <p>Hosted Tools run on OpenAI's servers alongside AI models, including <code>WebSearchTool</code>, <code>FileSearchTool</code>, <code>ComputerTool</code>, <code>CodeInterpreterTool</code>, and others.</p> <p>Function Tools allow any Python function to become an agent tool through automatic schema generation.</p> <p>Agent as Tools enable agents to orchestrate other agents without handoff using the <code>agent.as_tool()</code> method.</p>"},{"location":"OpAgentsOlympus/OpAgentsTools/#function-schema-core-components","title":"Function Schema Core Components","text":""},{"location":"OpAgentsOlympus/OpAgentsTools/#funcschema-dataclass","title":"FuncSchema Dataclass","text":"<p>The <code>FuncSchema</code> class captures all metadata required to represent a Python function as an LLM tool:</p> <p>Key fields include the function name, description extracted from docstrings, a dynamically generated Pydantic model for parameters, the resulting JSON schema, and metadata about context parameter usage. </p> <p>The <code>to_call_args()</code> method converts validated Pydantic data back into Python function arguments, handling, <code>*args</code>, and <code>**kwargs</code> parameters appropriately.</p>"},{"location":"OpAgentsOlympus/OpAgentsTools/#documentation-extraction","title":"Documentation Extraction","text":"<p>The <code>FuncDocumentation</code> class extracts metadata from function docstrings using the <code>griffe</code> library:</p> <p>The system supports multiple docstring styles with automatic detection, including Google (<code>Args:</code>, <code>Returns:</code>), Sphinx (<code>:param</code>, <code>:type</code>, <code>:return:</code>), and NumPy (<code>Parameters\\n-------</code>) formats</p>"},{"location":"OpAgentsOlympus/OpAgentsTools/#the-function_tool-decorator","title":"The @function_tool Decorator","text":"<p>The <code>@function_tool</code> decorator automatically converts Python functions into <code>FunctionTool</code> instances through a multi-step process: </p> Python<pre><code>@function_tool\nasync def fetch_weather(location: Location) -&gt; str:\n    \"\"\"Fetch the weather for a given location.\n\n    Args:\n        location: The location to fetch the weather for.\n    \"\"\"\n    return \"sunny\"\n</code></pre> <p>The decorator extracts function signatures using Python's <code>inspect</code> module, parses docstrings with <code>griffe</code>, and creates Pydantic models for schema generation. </p>"},{"location":"OpAgentsOlympus/OpAgentsTools/#context-parameter-support","title":"Context Parameter Support","text":"<p>Functions can optionally accept context as their first parameter, which is automatically detected and excluded from the JSON schema: </p> Python<pre><code>@function_tool\ndef read_file(ctx: RunContextWrapper[Any], path: str) -&gt; str:\n    \"\"\"Read file contents with access to run context.\"\"\"\n    return \"&lt;file contents&gt;\"\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgentsTools/#schema-generation-process","title":"Schema Generation Process","text":"<p>The <code>function_schema()</code> function performs comprehensive analysis of Python functions: </p> <ol> <li> <p>Docstring Analysis: Extracts descriptions and parameter documentation using <code>generate_func_documentation()</code> </p> </li> <li> <p>Signature Inspection: Uses <code>inspect.signature()</code> and <code>get_type_hints()</code> to analyze function parameters </p> </li> <li> <p>Context Detection: Identifies <code>RunContextWrapper</code> or <code>ToolContext</code> parameters </p> </li> <li> <p>Dynamic Model Creation: Builds Pydantic models using <code>create_model()</code> with proper field definitions </p> </li> <li> <p>JSON Schema Generation: Creates strict JSON schemas with <code>\"additionalProperties\": false</code> by default </p> </li> </ol>"},{"location":"OpAgentsOlympus/OpAgentsTools/#parameter-type-handling","title":"Parameter Type Handling","text":"<p>The system handles various parameter types including <code>VAR_POSITIONAL</code> (args) and <code>VAR_KEYWORD</code> (*kwargs): </p> <p>For <code>*args</code>, it converts tuple type hints to list types and provides empty list defaults.</p> <p>For <code>**kwargs</code>, it handles dictionary type hints and provides empty dictionary defaults.</p>"},{"location":"OpAgentsOlympus/OpAgentsTools/#tool-invocation-and-error-handling","title":"Tool Invocation and Error Handling","text":"<p>The <code>FunctionTool</code> class wraps the generated schema and provides invocation logic: </p> <p>During invocation, the system parses JSON input, validates it against the Pydantic model, converts to function arguments, and executes the function: </p> <p>Error handling is managed through configurable error functions, with <code>default_tool_error_function</code> providing standard error message formatting for LLMs: </p>"},{"location":"OpAgentsOlympus/OpAgentsTools/#manual-tool-creation","title":"Manual Tool Creation","text":"<p>For advanced use cases, you can create <code>FunctionTool</code> instances manually by providing the name, description, JSON schema, and invocation handler: </p> Python<pre><code>tool = FunctionTool(\n    name=\"process_user\",\n    description=\"Processes extracted user data\",\n    params_json_schema=FunctionArgs.model_json_schema(),\n    on_invoke_tool=run_function,\n)\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgentsTools/#notes","title":"Notes","text":"<p>The function schema system uses strict JSON schemas by default to improve LLM compliance, automatically detects docstring formats, and supports complex parameter patterns including variadic arguments. The schema generation code lives in <code>src/OpAgentsOlympus/function_schema.py</code> and integrates with the broader tool system through the <code>@function_tool</code> decorator and <code>FunctionTool</code> class. </p> <p>To Learn More About Strict Mode:</p> <p>The Real Difference Between Strict and Non-Strict Mode</p> <p>None-Strict Mode Behavior (Code Example)</p>"},{"location":"OpAgentsOlympus/OpAgentsTrickStrict/","title":"Opagentstrickstrict","text":"Source code in OpAgentsOlympus/practice/OpAgentsTrickStrict.py OpAgentsOlympus/practice/OpAgentsTrickStrict.py<pre><code>import asyncio  \nfrom typing import Optional  \nfrom agents import (  \n    Agent,   \n    Runner,   \n    function_tool,   \n    set_tracing_disabled,   \n    OpenAIChatCompletionsModel,\n    AsyncOpenAI,\n    enable_verbose_stdout_logging\n)  \nimport os\nimport dotenv\ndotenv.load_dotenv()\nGEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\nenable_verbose_stdout_logging()\n\nexternal_client = AsyncOpenAI(\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai\",\n    api_key=GEMINI_API_KEY\n)\nset_tracing_disabled(True)\n\nmodel = OpenAIChatCompletionsModel(\n    model='gemini-1.5-flash-8b',\n    openai_client=external_client\n)\n\n@function_tool(strict_mode=True)  \ndef strict_calculator(a: int, b: Optional[int] = None) -&gt; str:  \n    \"\"\"Calculate three given numbers \n\n    Args:  \n        a: First number  \n        b: Second number (optional)\n        c: Third number (optional)  \n    \"\"\"  \n    print(\"strict_calculator called\")\n    if b is None:  \n        return f\"Strict Result: {a}\"  \n    return f\"Strict Result: {a + b}\"  \n\n@function_tool(strict_mode=False)  \ndef non_strict_calculator(a: int, b: Optional[int] = None) -&gt; str:  \n    \"\"\"Calculate three given numbers \n\n    Args:  \n        a: First number    \n        b: Second number (optional)  \n        c: Third number (optional)  \n    \"\"\"  \n    print(\"non_strict_calculator called\")\n    if b is None:  \n        return f\"Non-strict Result: {a}\"  \n    return f\"Non-strict Result: {a + b}\"  \n\nstrict_agent = Agent(  \n    name=\"Strict Agent\",  \n    model=model,\n    instructions=\"You are a calculator assistant. You MUST use the strict_calculator tool for all calculations, strict_calculator tool has 3 arguments to be passed, a: int, b: Optional[int] = None, c: Optional[int] = None\",  \n    tools=[strict_calculator]  \n)  \n\nnon_strict_agent = Agent(  \n    name=\"Non-Strict Agent\",\n    model=model,\n    instructions=\"You are a calculator assistant. You MUST use the non_strict_calculator tool for all calculations, non_strict_calculator tool has 3 arguments to be passed, a: int, b: Optional[int] = None, c: Optional[int] = None\",  \n    tools=[non_strict_calculator]  \n)  \n\nasync def main():    \n    result1 = await Runner.run(  \n        starting_agent=strict_agent,  \n        input=\"What is 2 + 2 + 2\"  \n    )  \n    print(f\"Strict agent result: {result1.final_output}\")\n\n    result2 = await Runner.run(  \n        starting_agent=non_strict_agent,  \n        input=\"What is 2 + 2 + 2\"  \n    )  \n    print(f\"Non-strict agent result: {result2.final_output}\")  \n\nif __name__ == \"__main__\":  \n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/OpAgents_get_single_step_result_from_response/","title":"Step Results","text":"<p>The <code>_get_single_step_result_from_response</code> method is a core orchestration function in the <code>AgentRunner</code> class that processes a model response and determines what should happen next in the agent workflow</p>"},{"location":"OpAgentsOlympus/OpAgents_get_single_step_result_from_response/#purpose-and-functionality","title":"Purpose and Functionality","text":"<p>This method serves as the bridge between receiving a model response and executing the appropriate actions based on that response. It takes a raw <code>ModelResponse</code> and converts it into a structured <code>SingleStepResult</code> that contains all the information needed to continue the agent execution loop</p> <p>The method performs two key operations:</p> <ol> <li> <p>Response Processing: It calls <code>RunImpl.process_model_response()</code> to parse the model's output into structured actions like tool calls, handoffs, and computer actions</p> </li> <li> <p>Tool Use Tracking and Execution: It updates the <code>AgentToolUseTracker</code> with tools used, then executes <code>RunImpl.execute_tools_and_side_effects()</code> to run any tools that were called and determine the next step in the workflow</p> </li> </ol>"},{"location":"OpAgentsOlympus/OpAgents_get_single_step_result_from_response/#usage-in-the-execution-flow","title":"Usage in the Execution Flow","text":"<p>The method is called in two critical places in the agent execution system:</p>"},{"location":"OpAgentsOlympus/OpAgents_get_single_step_result_from_response/#non-streaming-execution","title":"Non-Streaming Execution","text":"<p>In the standard execution path, it's called after getting a new response from the model in <code>_run_single_turn()</code></p>"},{"location":"OpAgentsOlympus/OpAgents_get_single_step_result_from_response/#streaming-execution","title":"Streaming Execution","text":"<p>In streaming mode, it's called after the streaming response is complete to process the final result in <code>_run_single_turn_streamed()</code></p>"},{"location":"OpAgentsOlympus/OpAgents_get_single_step_result_from_response/#return-value-and-decision-making","title":"Return Value and Decision Making","text":"<p>The method returns a <code>SingleStepResult</code> that contains a <code>next_step</code> field indicating what should happen next, The execution loop then uses this to decide whether to:</p> <ul> <li>Continue the loop (<code>NextStepRunAgain</code>) - when tools were executed but no final output was determined</li> <li>Hand off to another agent (<code>NextStepHandoff</code>) - when a handoff tool was called</li> <li>Terminate with final output (<code>NextStepFinalOutput</code>) - when the agent produces structured output or completes without more tools to run</li> </ul>"},{"location":"OpAgentsOlympus/OpAgents_get_single_step_result_from_response/#method-signature-and-parameters","title":"Method Signature and Parameters","text":"<p>The method accepts comprehensive parameters including the agent, all available tools, original input, pre-step items, the new response, output schema, handoffs, hooks, context wrapper, run config, and tool use tracker</p>"},{"location":"OpAgentsOlympus/OpAgents_get_single_step_result_from_response/#notes","title":"Notes","text":"<p>This method is essential to the turn-based execution model where each \"turn\" represents one model invocation plus all resulting side effects. It encapsulates the complex logic of determining what actions to take based on the model's response and ensures the agent workflow continues appropriately. The method also handles tool use tracking to support features like automatic tool choice reset to prevent infinite loops.</p>"},{"location":"OpAgentsOlympus/OpDeepTContext/","title":"Deep Context","text":""},{"location":"OpAgentsOlympus/OpDeepTContext/#what-is-tcontext","title":"\ud83d\udd38 What is <code>TContext</code>?","text":"<p>In the OpenAI Agents SDK, <code>TContext</code> means \u201ctype of context.\u201d You can think of it like this:</p> <ul> <li>You create your own class, such as <code>UserContext</code></li> <li>This class becomes your context</li> <li>The SDK calls it <code>TContext</code>, but you control what it contains</li> </ul> <p>Example:</p> Python<pre><code>@dataclass\nclass UserContext:\n    user_id: str\n    is_admin: bool\n</code></pre> <p>Here, <code>UserContext</code> is your TContext. The SDK will treat your <code>UserContext</code> as the main context during the agent's execution.</p>"},{"location":"OpAgentsOlympus/OpDeepTContext/#two-types-of-context-important-to-know","title":"\ud83d\udd38 Two Types of Context (Important to Know)","text":"<p>There are two completely different types of \"context\" in this SDK. Many developers get confused here.</p>"},{"location":"OpAgentsOlympus/OpDeepTContext/#1-tcontext-local-context","title":"1. <code>TContext</code> (Local Context)","text":"<p>This is what you define.</p> <ul> <li>It is your Python class</li> <li>It is passed to <code>Runner.run(...)</code></li> <li>It is used by tools, hooks, and guardrails</li> <li>It is not sent to the language model (LLM)</li> <li>It is used for internal logic, state, and access</li> </ul>"},{"location":"OpAgentsOlympus/OpDeepTContext/#2-llm-context","title":"2. LLM Context","text":"<p>This is the input the LLM sees.</p> <ul> <li>Includes messages, instructions or system prompt, and tool results</li> <li>The LLM uses this to understand and reply</li> <li>You do not define this with a Python class</li> <li>This is not your <code>TContext</code></li> </ul> <p>\u2705 Important: These two contexts are separate. Your <code>TContext</code> is not seen by the LLM.</p>"},{"location":"OpAgentsOlympus/OpDeepTContext/#how-tcontext-works-in-your-code","title":"\ud83d\udd38 How <code>TContext</code> Works in Your Code","text":"<p>Let\u2019s define a custom context:</p> Python<pre><code>@dataclass\nclass UserContext:\n    user_id: str\n    is_pro: bool\n\n    async def get_purchases(self) -&gt; list[str]:\n        ...\n</code></pre> <p>You use this class when creating the agent:</p> Python<pre><code>agent = Agent[UserContext](...)\n</code></pre> <p>Then, in tools, you get access like this:</p> Python<pre><code>@function_tool\nasync def check_pro_status(ctx: RunContextWrapper[UserContext]) -&gt; str:\n    return f\"User is pro: {ctx.context.is_pro}\"\n</code></pre> <p>\u2705 The <code>ctx</code> object is a wrapper. It wraps your <code>UserContext</code>.</p>"},{"location":"OpAgentsOlympus/OpDeepTContext/#what-is-runcontextwrapper","title":"\ud83d\udd38 What is <code>RunContextWrapper</code>?","text":"<p>You do not use <code>UserContext</code> directly in tools.</p> <p>You use <code>RunContextWrapper[UserContext]</code>. This gives extra features.</p>"},{"location":"OpAgentsOlympus/OpDeepTContext/#what-it-does","title":"What it does:","text":"<ul> <li>It gives access to your context: <code>ctx.context</code></li> <li>It lets you track tokens used: <code>ctx.usage.input_tokens</code>, <code>ctx.usage.output_tokens</code></li> <li>It makes all tools work in a consistent way</li> <li>It makes the code type-safe</li> </ul> <p>Example:</p> Python<pre><code>input_tokens = ctx.usage.input_tokens\noutput_tokens = ctx.usage.output_tokens\n</code></pre> <p>\u2705 <code>RunContextWrapper</code> is always used when your tool or hook needs context.</p>"},{"location":"OpAgentsOlympus/OpDeepTContext/#type-safety-rules","title":"\ud83d\udd38 Type Safety Rules","text":"<p>You must follow these simple rules:</p>"},{"location":"OpAgentsOlympus/OpDeepTContext/#rule-1-all-parts-of-the-agent-must-use-the-same-context-type","title":"Rule 1: All parts of the agent must use the same context type","text":"Python<pre><code>agent = Agent[UserContext](...)  # OK\n# Do not mix with AdminContext in the same agent run\n</code></pre>"},{"location":"OpAgentsOlympus/OpDeepTContext/#rule-2-always-declare-your-context-type","title":"Rule 2: Always declare your context type","text":"Python<pre><code>agent = Agent[UserContext](...)  # Good\n</code></pre>"},{"location":"OpAgentsOlympus/OpDeepTContext/#where-tcontext-is-used","title":"\ud83d\udd38 Where <code>TContext</code> Is Used","text":"<p>Here is where your context (like <code>UserContext</code>) is used:</p> <ol> <li>Function Tools</li> </ol> Python<pre><code>def my_tool(ctx: RunContextWrapper[MyContext], ...) -&gt; ...:\n</code></pre> <ol> <li> <p>Agent Hooks    Lifecycle functions like <code>on_start</code>, <code>on_tool_start</code>, etc.</p> </li> <li> <p>Run Hooks    Global workflow logic</p> </li> <li> <p>Dynamic Instructions    Custom system messages based on context</p> </li> <li> <p>Tool Enabling Logic    Turn tools on/off depending on context</p> </li> </ol>"},{"location":"OpAgentsOlympus/OpDeepTContext/#summary","title":"\ud83d\udd38 Summary","text":"<p>Here is everything again, very short:</p> Term Meaning / Use <code>TContext</code> The context class you define (like <code>UserContext</code>) <code>RunContextWrapper</code> SDK gives this to tools/hooks instead of raw context <code>.context</code> Access to your actual context object <code>.usage</code> Info about tokens used Not sent to LLM Your context is not seen by the LLM Must match Same context type used across tools/hooks <p>If Helped Star \u2b50 Happy Coding \ud83d\udc96 </p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/","title":"OpenAI Agents SDK Guide (v0.0.17)","text":"<p>The OpenAI Agents SDK is a toolkit to help you build agentic AI applications easily. An agent here means a large language model (LLM) like ChatGPT, but with added instructions and tools. The SDK makes it easy to connect agents, tools, and rules (called guardrails) in code. It comes with:</p> <ul> <li>Agents: LLMs with instructions and tools.</li> <li>Handoffs: A way for one agent to hand off tasks to another agent.</li> <li>Guardrails: Checks that run alongside your agents to validate inputs or outputs.</li> </ul> <p>The design goals of the SDK are:</p> <ul> <li>Powerful but simple: It has all the necessary features, but few basic concepts so it's quick to learn.</li> <li>Customizable: Works well out of the box, but you can tweak everything to fit your needs.</li> </ul> <p>Key features include:</p> <ul> <li>Agent loop: Built-in logic that calls the LLM repeatedly, handles tool calls, and loops until done.</li> <li>Python-first design: Use normal Python code to chain and orchestrate agents.</li> <li>Handoffs: Easily coordinate between multiple agents.</li> <li>Guardrails: Run input/output validations in parallel with your agents to stop bad inputs early.</li> <li>Function tools: Turn any Python function into a tool, with automatic input schema and validation.</li> <li>Tracing: Built-in tracing to visualize, debug, and monitor your flows. (Works with OpenAI\u2019s trace dashboard.)</li> </ul>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#why-use-the-agents-sdk","title":"Why use the Agents SDK","text":"<p>The SDK\u2019s main idea is to make complex multi-agent applications easy to build. It\u2019s like a \"production-ready upgrade\" of earlier experiments (e.g. Swarm). It comes with everything you need to express interactions between tools and agents without a steep learning curve.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#installation","title":"Installation","text":"<p>Install the Agents SDK with pip:</p> Text Only<pre><code>pip install openai-agents\n</code></pre> <p>Make sure you also set your <code>OPENAI_API_KEY</code> environment variable before running code that uses the SDK.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#hello-world-example","title":"Hello World Example","text":"<p>The simplest example:</p> Python<pre><code>from agents import Agent, Runner\n\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n\nresult = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\")\nprint(result.final_output)\n\n# Code within the code,\n# Functions calling themselves,\n# Infinite loop's dance.\n</code></pre> <p>This code creates an agent with a name and instructions, then uses <code>Runner.run_sync</code> to have the agent write a haiku about recursion. The <code>result.final_output</code> is the haiku generated by the agent. Make sure to set the <code>OPENAI_API_KEY</code> before running this.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#quickstart","title":"Quickstart","text":"<p>This section shows step-by-step how to set up a project and run agents.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#create-a-project-and-virtual-environment","title":"Create a Project and Virtual Environment","text":"<p>On a new project folder, set up a Python virtual environment:</p> Bash<pre><code>mkdir my-project\ncd my-project\npython3 -m venv env\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#activate-the-virtual-environment","title":"Activate the Virtual Environment","text":"<p>Activate it (on Unix):</p> Bash<pre><code>source env/bin/activate\n</code></pre> <p>On Windows, you might run:</p> Text Only<pre><code>env\\Scripts\\activate\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#install-the-agents-sdk","title":"Install the Agents SDK","text":"<p>With the environment active, install the SDK:</p> Bash<pre><code>pip install openai-agents\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#set-the-openai-api-key","title":"Set the OpenAI API Key","text":"<p>The SDK uses OpenAI\u2019s API for the language models. You must provide an API key. The simplest way is to set the environment variable:</p> Bash<pre><code>export OPENAI_API_KEY=sk-...  \n# On Windows, use: set OPENAI_API_KEY=sk-...\n</code></pre> <p>Alternatively, you can set the key in code using <code>agents.set_default_openai_key</code>, but using the environment variable is easier for quickstart.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#create-your-first-agent","title":"Create Your First Agent","text":"<p>In code, create an Agent object. Give it a name and some instructions. For example:</p> Python<pre><code>from agents import Agent, Runner\n\nagent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant.\")\n</code></pre> <p>This creates an agent that will behave like a helpful assistant.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#add-more-agents","title":"Add More Agents","text":"<p>You can create multiple agents. For example, imagine a Spanish-speaking agent and an English-speaking agent:</p> Python<pre><code>spanish_agent = Agent(name=\"Spanish\", instructions=\"You only respond in Spanish.\")\nenglish_agent = Agent(name=\"English\", instructions=\"You only respond in English.\")\n</code></pre> <p>Each agent has its own name and instructions.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#define-handoffs","title":"Define Handoffs","text":"<p>Handoffs let one agent delegate work to another. Specify in an agent\u2019s configuration the agents it can hand off to. For example, a triage agent that decides whether to use Spanish or English agent:</p> Python<pre><code>from agents import handoff\n\ntriage_agent = Agent(\n    name=\"Triage\",\n    instructions=\"Handoff to the correct language agent based on the query.\",\n    handoffs=[spanish_agent, handoff(english_agent)]\n)\n</code></pre> <p>Here, <code>handoffs</code> lists two options: <code>spanish_agent</code> and a customized handoff to <code>english_agent</code>. (We used <code>handoff(english_agent)</code> to illustrate custom settings; with just <code>english_agent</code> it would use defaults.)</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#add-tools-optional","title":"Add Tools (Optional)","text":"<p>Agents can use tools to take actions (like web search, code execution, etc.). For example, you could add a weather tool:</p> Python<pre><code>from agents import function_tool\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    # Imagine calling a real weather API here\n    return f\"The weather in {city} is sunny.\"\n</code></pre> <p>Then assign tools to your agent:</p> Python<pre><code>agent_with_tool = Agent(\n    name=\"Weather Assistant\",\n    instructions=\"You answer with weather info when asked.\",\n    tools=[get_weather]\n)\n</code></pre> <p>Now this agent can call <code>get_weather</code> when it needs weather info.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#define-a-guardrail-optional","title":"Define a Guardrail (Optional)","text":"<p>Guardrails are checks that run alongside your agent to validate inputs or outputs. For example, you could write a quick guardrail that blocks math homework queries:</p> Python<pre><code>from agents import input_guardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered, Runner\n\nasync def homework_guardrail(ctx, agent, input_text: str):\n    # Simple check for math homework in user query\n    triggered = \"homework\" in input_text.lower()\n    return GuardrailFunctionOutput(\n        output_info=None,\n        tripwire_triggered=triggered\n    )\n\nagent_with_guardrail = Agent(\n    name=\"Support\",\n    instructions=\"You help customers.\",\n    input_guardrails=[homework_guardrail]\n)\n</code></pre> <p>This guardrail will stop the agent if it sees \"homework\" in the input (demonstration only). See the Guardrails section in official docs for more details.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#run-the-agent-workflow","title":"Run the Agent Workflow","text":"<p>Once you have agents, tools, handoffs, and guardrails set up, you can run them. Use the <code>Runner</code> class:</p> Python<pre><code>result = Runner.run_sync(triage_agent, \"Hello, how are you?\")\nprint(result.final_output)\n</code></pre> <p>This runs the <code>triage_agent</code> on the given input. The <code>Runner</code> handles calling the LLM, executing tool calls, running guardrails, and performing handoffs. The final answer appears in <code>result.final_output</code>.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#put-it-all-together","title":"Put It All Together","text":"<p>A more complete example:</p> Python<pre><code>from agents import Agent, Runner, function_tool, handoff, input_guardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered\n\nasync def homework_guardrail(ctx, agent, input_text: str):\n    # Simple check for math homework in user query\n    triggered = \"homework\" in input_text.lower()\n    return GuardrailFunctionOutput(\n        output_info=None,\n        tripwire_triggered=triggered\n    )\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is sunny.\"\n\nspanish_agent = Agent(\n    name=\"Spanish\",\n    instructions=\"You only answer in Spanish.\",\n)\n\nenglish_agent = Agent(\n    name=\"English\",\n    instructions=\"You only answer in English.\",\n)\n\nweather_agent = Agent(\n    name=\"Weather Agent\",\n    instructions=\"You provide weather info.\",\n    tools=[get_weather]\n)\n\ntriage_agent = Agent(\n    name=\"Triage\",\n    instructions=\"Call the right agent based on the language and request.\",\n    handoffs=[spanish_agent, handoff(english_agent), weather_agent],\n    input_guardrails=[homework_guardrail]\n)\n\nresult = Runner.run_sync(triage_agent, \"What's the weather in Tokyo?\")\nprint(result.final_output)\n</code></pre> <p>Here, <code>triage_agent</code> could hand off the conversation to one of the language agents or the weather agent. The <code>Runner</code> takes care of running the loop and choosing tools or agents as needed.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#view-traces-optional","title":"View Traces (Optional)","text":"<p>If you have tracing enabled (default), you can log into OpenAI\u2019s Traces dashboard (platform.openai.com) to see the detailed steps of the agent run. This is useful for debugging your application.</p> <p>If needed, you can disable tracing or control it via configuration (see Configuring the SDK). For now, just know that tracing is on by default and captures everything.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#next-steps","title":"Next Steps","text":"<p>You\u2019ve seen the basic workflow: set up agents with instructions, add tools and handoffs, and run with <code>Runner.run</code> or <code>Runner.run_sync</code>. In the following sections we explain each part in depth:</p> <ul> <li>Agents: How to configure agents and their properties.</li> <li>Running agents: The underlying loop of how agents are executed.</li> <li>Results: How to inspect the output of a run.</li> <li>Streaming: Getting updates from an agent run in real-time.</li> <li>REPL utility: A quick interactive console to test agents.</li> <li>Tools: How to use built-in, function, and agent tools.</li> <li>MCP: Integrate with external tool providers via the Model Context Protocol.</li> <li>Handoffs: How one agent can delegate to another.</li> <li>Tracing: How execution is traced and how to use it.</li> <li>Context management: Passing context (data or objects) through runs.</li> <li>Guardrails: Writing input/output checks for safety.</li> <li>Multi-agent: Patterns for orchestrating multiple agents in your app.</li> <li>Models: LLMs, including non-OpenAI models via LiteLLM.</li> <li>Configuration: SDK-level settings like API keys and logging.</li> <li>Visualization: Tools for visualizing agent graphs.</li> <li>Release process: How the SDK versions are managed.</li> </ul> <p>Each section below covers these topics.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#agents","title":"Agents","text":"<p>An Agent is like a virtual assistant powered by an LLM. You create an agent by giving it a name and instructions, and optionally tools or other settings. Here are the main points about agents:</p> <ul> <li>Name: A unique name to identify the agent.</li> <li>Instructions: A prompt or guidelines telling the model how to behave.</li> <li>Tools: A list of tools this agent can call.</li> <li>Handoffs: A list of other agents to which it can delegate tasks.</li> <li>Guardrails: Input/output checks for this agent.</li> <li>Context (optional): Type of context object if you want to pass custom data.</li> <li>Model: Optionally, which LLM to use (by default it uses OpenAI's GPT models).</li> </ul> <p>Agents are defined by creating an <code>Agent</code> object. For example:</p> Python<pre><code>from agents import Agent\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"You are a helpful assistant.\",\n)\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#basic-configuration","title":"Basic Configuration","text":"<p>When creating an <code>Agent</code>, you can pass:</p> <ul> <li><code>name</code>: The agent\u2019s name.</li> <li><code>instructions</code>: A string or list of messages that tell the agent how to act.</li> <li><code>tools</code>: A list of tools this agent can use.</li> <li><code>handoffs</code>: Other agents to hand off tasks to.</li> <li><code>input_type</code> / <code>output_type</code>: (Advanced) Specify the type of input/output expected.</li> <li><code>guardrails</code>: Input or output guardrails for this agent.</li> <li><code>model</code>/<code>model_settings</code>: Specify which LLM model to use (if different from defaults).</li> </ul> <p>For example, to make an agent always respond in French with GPT-4, you might write:</p> Python<pre><code>from agents import Agent, ModelSettings, OpenAIChatCompletionsModel\n\nfrench_agent = Agent(\n    name=\"FrenchAgent\",\n    instructions=\"Please answer in French only.\",\n    model=OpenAIChatCompletionsModel(model=\"gpt-4\"),\n    model_settings=ModelSettings(temperature=0.5),\n)\n</code></pre> <p>This agent uses GPT-4 via the Chat Completions API and will speak only French.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#context","title":"Context","text":"<p>Agents can have access to a context object in your code. This is data you provide to each run. For example, you might want the agent to know the user\u2019s name or have access to a database connection.</p> <ul> <li>Define any Python object (dataclass, Pydantic model, etc.) for your context.</li> <li>Pass this object in <code>Runner.run(..., context=...)</code>.</li> <li>All tools and hooks get a <code>RunContextWrapper</code> that includes <code>context</code>.</li> </ul> <p>Example:</p> Python<pre><code>from dataclasses import dataclass\nfrom agents import Agent, Runner, RunContextWrapper, function_tool\n\n@dataclass\nclass UserInfo:\n    name: str\n    uid: int\n\n@function_tool\nasync def greet(wrapper: RunContextWrapper[UserInfo]) -&gt; str:\n    # Access context: wrapper.context.name\n    return f\"Hello, {wrapper.context.name}!\"\n\nasync def main():\n    user = UserInfo(name=\"Alice\", uid=42)\n    agent = Agent[UserInfo](\n        name=\"Greeter\",\n        instructions=\"Greet the user by name.\",\n        tools=[greet]\n    )\n    result = await Runner.run(agent, \"Hi\", context=user)\n    print(result.final_output)  # \"Hello, Alice!\"\n</code></pre> <p>Here, <code>UserInfo</code> is the local context type. The tool <code>greet</code> reads <code>wrapper.context.name</code> to personalize the greeting.</p> <p>Important: The context object is not sent to the model. It\u2019s only available in your Python code (tools, hooks). If you need data to go to the LLM, include it in <code>instructions</code> or the prompt.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#dynamic-instructions","title":"Dynamic Instructions","text":"<p>You can make an agent\u2019s instructions dynamic using Python functions. Instead of a fixed string, <code>instructions</code> can be a function that returns a string based on context.</p> <p>For example:</p> Python<pre><code>def dynamic_instr(ctx: RunContextWrapper[UserInfo]) -&gt; str:\n    return f\"You are talking to user {ctx.context.name}.\"\n\nagent = Agent(\n    name=\"PersonalAgent\",\n    instructions=dynamic_instr,\n)\n</code></pre> <p>Now each time <code>Runner</code> calls the model, it will call <code>dynamic_instr</code> with the context object to get the system prompt.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#handoffs","title":"Handoffs","text":"<p>A handoff is a way for one agent to transfer control to another agent. This is useful when different agents specialize in different tasks. In the agent\u2019s configuration, you set <code>handoffs</code> to a list of other agents or <code>Handoff</code> objects.</p> <p>For example:</p> Python<pre><code>from agents import Agent, handoff\n\nsales_agent = Agent(name=\"Sales\", instructions=\"Help with sales queries.\")\nsupport_agent = Agent(name=\"Support\", instructions=\"Handle general support.\")\n\nagent = Agent(\n    name=\"FrontDesk\",\n    instructions=\"Decide who should answer the question.\",\n    handoffs=[sales_agent, handoff(support_agent)]\n)\n</code></pre> <p>In the above, the <code>FrontDesk</code> agent can hand off to either <code>Sales</code> agent (using defaults) or <code>Support</code> agent (we used <code>handoff(support_agent)</code> to show how to customize it, though not necessary here).</p> <p>Handoffs work as tools that the LLM can call. If the LLM calls <code>transfer_to_Support</code>, the SDK will switch to that agent.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#lifecycle-events","title":"Lifecycle Events","text":"<p>Agents support lifecycle hooks that run at certain points during execution. You can define:</p> <ul> <li><code>pre_prompt</code>, <code>post_prompt</code>: Functions to modify the prompt before or after sending to LLM.</li> <li><code>pre_agent</code>, <code>post_agent</code>: Functions to run before or after the agent\u2019s LLM call (e.g., logging).</li> <li><code>pre_tool</code>, <code>post_tool</code>: Run before/after each tool invocation.</li> <li><code>on_tool_error</code>: Handle errors when calling a tool.</li> <li><code>pre_user_input</code>, <code>post_user_input</code>: Handle user inputs.</li> </ul> <p>These are advanced and help customize the run. For example, you might use <code>pre_tool</code> to log that a tool is about to run.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#guardrails-on-agents","title":"Guardrails on Agents","text":"<p>You can attach input and output guardrails to an agent (see Guardrails section). For example:</p> Python<pre><code>from agents import Agent\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"You are helpful.\",\n    input_guardrails=[some_guardrail_function],\n    output_guardrails=[some_other_guardrail_function]\n)\n</code></pre> <p>Then before running the agent or after, the SDK checks these guardrails and can stop the run if a rule is violated.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#cloningcopying-agents","title":"Cloning/Copying Agents","text":"<p>You can copy an agent\u2019s configuration using the <code>.copy()</code> method if you need a similar agent with small changes. For example:</p> Python<pre><code>agent1 = Agent(name=\"A\", instructions=\"Do A things.\")\nagent2 = agent1.copy()\nagent2.name = \"B\"\nagent2.instructions = \"Do B things.\"\n</code></pre> <p>This avoids writing the same settings twice.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#forcing-tool-use","title":"Forcing Tool Use","text":"<p>If you want to force the agent to use tools instead of directly answering, you can use the <code>tool_choice='required'</code> method on an agent. This will force the agent to use tools, Be careful as it can lead to infinite tool calling.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#running-agents","title":"Running agents","text":"<p>To execute (run) agents, use the <code>Runner</code> class. There are three ways to run an agent:</p> <ol> <li><code>Runner.run(agent, input)</code>: An asynchronous method. Returns a <code>RunResult</code>. You should <code>await</code> this in an async function.</li> <li><code>Runner.run_sync(agent, input)</code>: A synchronous wrapper around <code>run()</code>. Useful if you\u2019re not using async.</li> <li><code>Runner.run_streamed(agent, input)</code>: An asynchronous method that streams events as the LLM runs. Returns a <code>RunResultStreaming</code> which you can iterate over for updates (tokens, tool calls, etc.).</li> </ol> <p>Example of using <code>run</code> (async):</p> Python<pre><code>from agents import Agent, Runner\nimport asyncio\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant.\")\n    result = await Runner.run(agent, \"Write a haiku about recursion in programming.\")\n    print(result.final_output)\n\nasyncio.run(main())\n</code></pre> <p>This prints:</p> Text Only<pre><code>Code within the code,\nFunctions calling themselves,\nInfinite loop's dance.\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#the-agent-loop","title":"The Agent Loop","text":"<p>When you call <code>Runner.run(...)</code>, the SDK performs this loop internally:</p> <ol> <li>Call the LLM for the current agent, giving it the current input and instructions.</li> <li> <p>The LLM generates output. Then:</p> </li> <li> <p>If the LLM returns a <code>final_output</code> (like a complete answer) and no tool calls, the loop ends and that output is returned.</p> </li> <li>If the LLM calls a handoff tool, the SDK switches to the new agent and updates the input, then continues the loop with the new agent.</li> <li>If the LLM produces tool calls, the SDK executes those tools (in order), collects their outputs as new input, and continues the loop with the same agent.</li> <li>If the loop runs more times than <code>max_turns</code> (a limit you can set), a <code>MaxTurnsExceeded</code> error is raised.</li> </ol> <p>In short, <code>Runner</code> keeps calling agents and running tools or handoffs until an agent finishes with a final answer.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#streaming","title":"Streaming","text":"<p>If you use <code>Runner.run_streamed()</code>, you get real-time events as the run proceeds. For example, you might stream partial text to a user.</p> Python<pre><code>result = Runner.run_streamed(agent, input=\"Hello\")\nasync for event in result.stream_events():\n    print(event)\n</code></pre> <p>There are two main kinds of streaming events:</p> <ul> <li>Raw response events (<code>RawResponsesStreamEvent</code>): These are low-level tokens or data from the LLM as it generates text. You can use these to display each token as it comes.</li> <li>Run item / agent events (<code>RunItemStreamEvent</code>, <code>AgentUpdatedStreamEvent</code>): Higher-level events for when a message is finished, a tool call completed, or the current agent changed. These let you update UI at logical steps (\u201cAgent said this\u201d, \u201cTool ran\u201d, etc.), instead of every token.</li> </ul> <p>For example, to print each token as it\u2019s generated:</p> Python<pre><code>import asyncio\nfrom openai.types.responses import ResponseTextDeltaEvent\nfrom agents import Agent, Runner\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"You are helpful.\")\n    result = Runner.run_streamed(agent, input=\"Tell me a joke.\")\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n            print(event.data.delta, end=\"\", flush=True)\n\nasyncio.run(main())\n</code></pre> <p>Or, to handle higher-level events and skip raw tokens:</p> Python<pre><code>import asyncio\nfrom agents import Agent, Runner, function_tool, ItemHelpers\n\n@function_tool\ndef how_many_jokes() -&gt; int:\n    import random\n    return random.randint(1, 10)\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n        tools=[how_many_jokes],\n    )\n\n    result = Runner.run_streamed(agent, input=\"Hello\")\n    print(\"=== Run starting ===\")\n\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\":\n            continue\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(f\"-- Message output:\\n{ItemHelpers.text_message_output(event.item)}\")\n\n    print(\"=== Run complete ===\")\n</code></pre> <p>In this example, we ignore raw tokens and only print when the agent changes or a message is generated.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#run-configuration-run_config","title":"Run Configuration (<code>run_config</code>)","text":"<p>When running agents, you can pass a <code>run_config</code> argument to control global settings. For example:</p> Python<pre><code>from agents import RunConfig\n\nconfig = RunConfig(\n    model=\"gpt-3.5-turbo\",\n    model_settings=ModelSettings(temperature=0.2),\n    tracing_disabled=True,\n    workflow_name=\"MyWorkflow\"\n)\nresult = Runner.run(agent, input_data, run_config=config)\n</code></pre> <p>Some key <code>run_config</code> options:</p> <ul> <li><code>model</code>: Override the agent\u2019s model and use this one instead.</li> <li><code>model_provider</code>: Set a custom model provider (default is OpenAI).</li> <li><code>model_settings</code>: Override settings like temperature or top_p for all agents.</li> <li><code>input_guardrails</code> / <code>output_guardrails</code>: Global guardrails to apply to every run.</li> <li><code>handoff_input_filter</code>: A global filter for inputs when handoffs happen.</li> <li><code>tracing_disabled</code>: Turn off tracing for this run.</li> <li><code>trace_include_sensitive_data</code>: Whether to include actual text or not in traces.</li> <li><code>workflow_name</code>, <code>trace_id</code>, <code>group_id</code>: Set custom tracing metadata. <code>workflow_name</code> should be set to something meaningful; <code>group_id</code> can link traces across multiple turns (like a chat thread).</li> <li><code>trace_metadata</code>: Additional metadata for traces.</li> </ul> <p>These let you customize how agents run at a high level.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#conversations-chat-threads","title":"Conversations / Chat Threads","text":"<p>Each call to <code>Runner.run</code> is like one turn in a conversation, even if multiple agents or tools ran internally. For example:</p> <ol> <li>User turn: User asks a question.</li> <li>Runner run: An agent answers (maybe using handoffs and tools).</li> <li>User sees answer.</li> </ol> <p>If the user then asks a follow-up, you would call <code>Runner.run</code> again. To keep context, use the <code>result.to_input_list()</code> method to get a list of all messages from the last run, and then append the new user message. For example:</p> Python<pre><code>async def chat():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply briefly.\")\n    # First turn\n    res1 = await Runner.run(agent, \"Where is the Golden Gate Bridge?\")\n    print(res1.final_output)  # e.g. \"San Francisco\"\n    # Prepare next turn\n    new_input = res1.to_input_list() + [{\"role\": \"user\", \"content\": \"What state is it in?\"}]\n    res2 = await Runner.run(agent, new_input)\n    print(res2.final_output)  # \"California\"\n</code></pre> <p>The <code>to_input_list()</code> takes all the items (user message, agent messages, tool outputs) from the run and makes them into a list of conversation messages. You can then add the next user prompt and run again.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#exceptions","title":"Exceptions","text":"<p>The SDK raises exceptions for various error cases. Key ones include:</p> <ul> <li><code>MaxTurnsExceeded</code>: Ran out of allowed turns.</li> <li><code>ModelBehaviorError</code>: The model returned something malformed (like broken JSON or an invalid tool call).</li> <li><code>UserError</code>: Your code used the SDK incorrectly.</li> <li><code>InputGuardrailTripwireTriggered</code>, <code>OutputGuardrailTripwireTriggered</code>: A guardrail\u2019s tripwire was hit.</li> </ul> <p>All exceptions inherit from <code>AgentsException</code>. Check <code>agents.exceptions</code> for full list.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#results","title":"Results","text":"<p>When you run an agent, the result is a <code>RunResult</code> (or <code>RunResultStreaming</code> for streaming). Both inherit from <code>RunResultBase</code>. The main pieces of information in the result are:</p> <ul> <li> <p><code>final_output</code>: The final answer from the last agent that ran. If the last agent didn\u2019t specify an <code>output_type</code>, this will be a plain string. If <code>output_type</code> was set, <code>final_output</code> will be an object of that type. For example, if the agent outputs JSON parsed into a Pydantic model, <code>final_output</code> is that model object.</p> </li> <li> <p><code>to_input_list()</code>: A method that gives you all the conversation items (original input, agent messages, tool outputs, etc.) as a list you can feed to the next run.</p> </li> <li> <p><code>last_agent</code>: The <code>Agent</code> object that was the final one to run. Handy if you need to know which agent answered last.</p> </li> <li> <p><code>new_items</code>: A list of <code>RunItem</code> objects for each new item generated during the run. There are different types of items:</p> </li> <li> <p><code>MessageOutputItem</code>: A message from the LLM (like the agent\u2019s response).</p> </li> <li><code>ToolCallItem</code>: The LLM invoked a tool (before execution).</li> <li><code>ToolCallOutputItem</code>: A tool was called and returned output.</li> <li><code>HandoffCallItem</code>: The LLM called a handoff tool (before switching).</li> <li><code>HandoffOutputItem</code>: A handoff was executed and returned a result.</li> <li><code>ReasoningItem</code>: An internal reasoning or trace item (if your agent outputs something like that).</li> </ul> <p>Each of these has the raw item inside it, plus easy accessors. For example, for a <code>MessageOutputItem</code>, you can get the text with <code>ItemHelpers.text_message_output(item)</code>.</p> <ul> <li> <p>Other info:</p> </li> <li> <p><code>input_guardrail_results</code> and <code>output_guardrail_results</code>: Results of any guardrails run. Useful for logging or analysis.</p> </li> <li><code>raw_responses</code>: The raw model responses (each LLM API response) during the run.</li> <li><code>input</code>: The original input you passed. (Usually you won\u2019t need this.)</li> </ul> <p>Example use:</p> Python<pre><code>result = Runner.run_sync(agent, \"Hello\")\nprint(\"Answer:\", result.final_output)\nprint(\"Last agent:\", result.last_agent.name)\nprint(\"Items generated:\")\nfor item in result.new_items:\n    if item.type == \"message_output_item\":\n        print(\"- Message:\", ItemHelpers.text_message_output(item))\n    elif item.type == \"tool_call_item\":\n        print(\"- Tool call:\", item.tool_name)\n    elif item.type == \"tool_call_output_item\":\n        print(\"- Tool output:\", item.output)\n</code></pre> <p>This will show the final answer, which agent gave it, and details about each message or tool used.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#streaming_1","title":"Streaming","text":"<p>You saw how to get streaming updates with <code>run_streamed()</code>. In the Streaming section above, we covered how to subscribe to streaming events while an agent run is happening.</p> <p>Recap:</p> <ul> <li>Call <code>Runner.run_streamed(agent, input)</code> to start streaming.</li> <li><code>result.stream_events()</code> is an async iterator of <code>StreamEvent</code> objects.</li> <li> <p>Event types:</p> </li> <li> <p><code>raw_response_event</code>: Contains raw LLM stream data (token deltas, etc.).</p> </li> <li><code>run_item_stream_event</code>: High-level events for when a run item (message or tool output) is completed.</li> <li><code>agent_updated_stream_event</code>: When the current agent changes (e.g. after a handoff).</li> </ul> <p>Use these events to provide live feedback in your application.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#repl-utility","title":"REPL Utility","text":"<p>The SDK has a quick REPL (read-eval-print loop) for interactive testing. It uses <code>run_demo_loop</code>.</p> <p>Example usage:</p> Python<pre><code>import asyncio\nfrom agents import Agent, run_demo_loop\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant.\")\n    await run_demo_loop(agent)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>When you run this script, it will prompt you to enter text in the console. It sends your input to the agent, and streams the model\u2019s output back in real time. It keeps a conversation history automatically. To exit the loop, type <code>quit</code> or <code>exit</code> (or press Ctrl-D).</p> <p>This is great for quick experiments.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#tools","title":"Tools","text":"<p>Agents can use tools to interact with the outside world or run code. The SDK provides three categories of tools:</p> <ol> <li> <p>Hosted tools: Built-in tools running on OpenAI\u2019s servers (with the <code>OpenAIResponsesModel</code>):</p> </li> <li> <p>WebSearchTool: Search the web.</p> </li> <li>FileSearchTool: Search files in your OpenAI vector stores.</li> <li>ComputerTool: Automate computer tasks.</li> <li>CodeInterpreterTool: Run code in a sandbox.</li> <li>HostedMCPTool: Expose tools from an MCP server (see MCP section).</li> <li>ImageGenerationTool: Generate images from text.</li> <li>LocalShellTool: Run shell commands locally.</li> </ol> <p>Example of using hosted tools:</p> Python<pre><code>from agents import Agent, Runner, WebSearchTool, FileSearchTool\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[\n        WebSearchTool(),\n        FileSearchTool(max_num_results=3, vector_store_ids=[\"VECTOR_STORE_ID\"]),\n    ],\n)\nresult = await Runner.run(agent, \"Which coffee shop should I go to, considering my preferences and today's weather in SF?\")\nprint(result.final_output)\n</code></pre> <ol> <li> <p>Function tools: Turn any Python function into a tool using the <code>@function_tool</code> decorator. The SDK automatically generates the tool schema from the function signature:</p> </li> <li> <p>Tool name = function name (or override with <code>name_override</code>).</p> </li> <li>Description = function docstring (or provide one).</li> <li>Input schema = function arguments schema (using Python types).</li> <li>Input descriptions = from docstring (unless disabled).</li> <li>The function can be <code>async</code> or normal, and can take a <code>RunContextWrapper</code> as the first argument if you need context.</li> </ol> <p>Example:</p> Python<pre><code>from agents import Agent, function_tool\n\n@function_tool\nasync def fetch_weather(location: dict) -&gt; str:\n    \"\"\"Fetch the weather for a given location.\n\n    Args:\n        location: A dict with 'lat' and 'long' keys.\n    \"\"\"\n    # Pretend to call a weather API\n    return \"sunny\"\n\nagent = Agent(name=\"WeatherAgent\", tools=[fetch_weather])\n</code></pre> <p>The SDK will inspect <code>fetch_weather</code> and create a <code>FunctionTool</code> with a JSON schema for its inputs. For complex inputs, you can use Pydantic models or TypedDicts. The example in the original docs showed how the tool\u2019s <code>params_json_schema</code> is auto-generated from the function signature and docstring.</p> <p>You can also create custom function tools manually by instantiating <code>FunctionTool</code> and providing <code>name</code>, <code>description</code>, <code>params_json_schema</code> and an <code>on_invoke_tool</code> async function. But using <code>@function_tool</code> is usually easier.</p> <ol> <li>Agents as tools: You can treat an agent itself as a tool. This is useful if you want one agent to call another without a full handoff. Use the <code>agent.as_tool()</code> method. For example:</li> </ol> Python<pre><code>spanish_agent = Agent(name=\"Spanish agent\", instructions=\"Translate to Spanish.\")\nfrench_agent = Agent(name=\"French agent\", instructions=\"Translate to French.\")\n\norchestrator_agent = Agent(\n    name=\"Orchestrator\",\n    instructions=\"You are a translation agent. Use the tools below.\",\n    tools=[\n        spanish_agent.as_tool(tool_name=\"to_spanish\", tool_description=\"Translate the message to Spanish\"),\n        french_agent.as_tool(tool_name=\"to_french\", tool_description=\"Translate the message to French\"),\n    ]\n)\n\nresult = await Runner.run(orchestrator_agent, input=\"Say 'Hello' in French.\")\nprint(result.final_output)\n</code></pre> <p>Here, <code>to_spanish</code> and <code>to_french</code> are tools backed by running the respective agents under the hood.</p> <p>You can customize agent-tools further:</p> <ul> <li>Use <code>custom_output_extractor</code> to process the sub-agent\u2019s output before returning it.</li> <li> <p>For advanced use, you can manually call <code>Runner.run</code> inside a <code>@function_tool</code> instead of <code>as_tool</code>.</p> </li> <li> <p>Error handling in tools: If a function tool raises an error, by default the LLM will be told an error occurred. You can supply a <code>failure_error_function</code> to provide a custom message to the LLM when a tool fails. Or set it to <code>None</code> to let exceptions propagate (which will raise errors in your Python code). For custom-created <code>FunctionTool</code>, you should catch errors inside <code>on_invoke_tool</code>.</p> </li> </ul>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#model-context-protocol-mcp","title":"Model Context Protocol (MCP)","text":"<p>MCP stands for Model Context Protocol, a standard for giving LLMs access to external tools and data sources. The Agents SDK supports MCP so you can integrate external servers.</p> <p>MCP servers come in three types:</p> <ol> <li>Stdio servers: Run as subprocesses of your app (local).</li> <li>HTTP SSE servers: Remote, accessed via HTTP+Server-Sent Events.</li> <li>Streamable HTTP servers: Remote, using a streaming HTTP protocol.</li> </ol> <p>The SDK has classes:</p> <ul> <li><code>MCPServerStdio</code></li> <li><code>MCPServerSse</code></li> <li><code>MCPServerStreamableHttp</code></li> </ul> <p>You use them by connecting to an MCP server. For example, using the official MCP filesystem server via npm:</p> Python<pre><code>async with MCPServerStdio(params={\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir]}) as server:\n    tools = await server.list_tools()\n    # Use the tools as needed\n</code></pre> <p>Once you have an <code>mcp_server</code> object, add it to an agent:</p> Python<pre><code>agent = Agent(\n    name=\"Assistant\",\n    instructions=\"Use the tools provided by MCP servers.\",\n    mcp_servers=[mcp_server]\n)\n</code></pre> <p>The SDK will call <code>list_tools()</code> on each MCP server at the start of every run. The agent will see those tools available. If the agent calls one of those tools, the SDK will call <code>mcp_server.call_tool(...)</code> to execute it.</p> <p>Caching: By default, <code>list_tools()</code> is called on every run (which could be slow for remote servers). You can enable caching by passing <code>cache_tools_list=True</code> when creating the MCP server object. This will reuse the same tool list each time. Only use caching if the tool list never changes. You can invalidate the cache manually with <code>invalidate_tools_cache()</code>.</p> <p>For examples, see the official examples repo (<code>https://github.com/openai/openai-agents-PyDeepOlympus/tree/main/examples/mcp</code>). Note that tracing will automatically capture MCP calls too.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#handoffs_1","title":"Handoffs","text":"<p>Handoffs are a way for one agent to hand off a task to another agent. This is useful when different agents have different specialties. For example, a support agent could hand off to a <code>Refund agent</code> or a <code>Sales agent</code> depending on the question.</p> <p>When an agent does a handoff, it\u2019s like it calls a special tool. If the agent named <code>Refund agent</code> is configured as a handoff target, the tool would be called <code>transfer_to_refund_agent</code> by default.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#creating-a-handoff","title":"Creating a Handoff","text":"<p>In an agent\u2019s configuration, the <code>handoffs</code> parameter lists the possible handoff targets. You can just list agents, or use the <code>handoff()</code> helper to customize. For example:</p> Python<pre><code>from agents import Agent, handoff\n\nbilling_agent = Agent(name=\"Billing agent\")\nrefund_agent = Agent(name=\"Refund agent\")\n\n# Simple usage: just list agents and use handoff() for custom settings\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Delegate to the correct agent based on topic.\",\n    handoffs=[billing_agent, handoff(refund_agent)]\n)\n</code></pre> <p>When listed as <code>handoffs=[refund_agent]</code>, By default, <code>refund_agent</code> will create a tool named <code>transfer_to_refund_agent</code> with a default description like \"handoff to Refund agent\".</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#customizing-with-handoff","title":"Customizing with <code>handoff()</code>","text":"<p>The <code>handoff()</code> function lets you fine-tune the handoff:</p> <ul> <li><code>agent</code>: The <code>Agent</code> object to hand off to.</li> <li><code>tool_name_override</code>: Provide a custom name for the tool (otherwise <code>Handoff.default_tool_name()</code> is used).</li> <li><code>tool_description_override</code>: Custom description text for the tool.</li> <li><code>on_handoff</code>: A callback function that runs when the handoff is invoked. It receives a context and optionally the LLM-provided input. Useful for triggering side-effects or logging.</li> <li><code>input_type</code>: The type (e.g. Pydantic model) of data the LLM should supply when doing the handoff.</li> <li><code>input_filter</code>: A function to filter or modify the conversation history passed to the new agent.</li> </ul> <p>Example with custom settings:</p> Python<pre><code>from agents import Agent, handoff, RunContextWrapper\n\ndef on_handoff(ctx: RunContextWrapper[None]):\n    print(\"Handoff called!\")\n\nhandoff_obj = handoff(\n    agent=agent2,\n    on_handoff=on_handoff,\n    tool_name_override=\"custom_handoff_tool\",\n    tool_description_override=\"Custom description\"\n)\n\nagent1 = Agent(\n    name=\"My agent\",\n    instructions=\"Good Agent\",\n    handoffs=[handoff_obj]\n    )\n</code></pre> <p>Here <code>handoff_obj</code> is a <code>Handoff</code> with custom tool name and description, and an <code>on_handoff</code> callback that prints something.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#handoff-inputs","title":"Handoff Inputs","text":"<p>Sometimes you want the user (via the LLM) to provide some data when calling a handoff. For instance, if handing off to an \"Escalation agent\", you might ask for a reason. You specify <code>input_type</code> for this:</p> Python<pre><code>from pydantic import BaseModel\nfrom agents import Agent, handoff, RunContextWrapper\n\nclass EscalationData(BaseModel):\n    reason: str\n\nasync def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):\n    print(f\"Escalation with reason: {input_data.reason}\")\n\nescalation_agent = Agent(name=\"Escalation agent\")\n\nhandoff_obj = handoff(\n    agent=escalation_agent,\n    on_handoff=on_handoff,\n    input_type=EscalationData,\n)\n\nagent1 = Agent(\n    name=\"My agent\",\n    instructions=\"Good Agent\",\n    handoffs=[handoff_obj]\n    )\n</code></pre> <p>Now the LLM should provide a JSON (or equivalent) matching <code>EscalationData</code> when it calls this handoff. Your <code>on_handoff</code> function gets that parsed data.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#input-filters","title":"Input Filters","text":"<p>By default, when a handoff happens, the next agent sees the entire conversation history. If you want to change what the new agent sees, use an <code>input_filter</code>. An input filter takes a <code>HandoffInputData</code> (which has the context and previous items) and returns a new <code>HandoffInputData</code> to use.</p> <p>For common use-cases, the SDK provides filters. For example, to remove all tool calls from history when handing off to the FAQ agent:</p> Python<pre><code>from agents import Agent, handoff\nfrom agents.extensions import handoff_filters\n\nagent = Agent(name=\"FAQ agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    input_filter=handoff_filters.remove_all_tools  # Removes tool items\n)\n</code></pre> <p>This means the FAQ agent only sees the user\u2019s messages, without any internal tool calls from before.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#recommended-prompts-for-handoffs","title":"Recommended Prompts for Handoffs","text":"<p>To help LLMs handle handoffs properly, the SDK provides some prompt templates. You can either prepend <code>agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX</code> to your instructions, or use <code>prompt_with_handoff_instructions()</code> to automatically add instructions about handoff to your agent\u2019s prompt.</p> <p>For example:</p> Python<pre><code>from agents import Agent\nfrom agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n\nbilling_agent = Agent(\n    name=\"Billing agent\",\n    instructions=f\"{RECOMMENDED_PROMPT_PREFIX} Then proceed with billing help.\"\n)\n</code></pre> <p>This ensures your agents know how the handoff system works.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#tracing","title":"Tracing","text":"<p>The Agents SDK automatically traces (logs) everything that happens during a run. This includes:</p> <ul> <li>LLM calls</li> <li>Tool calls</li> <li>Handoffs</li> <li>Guardrails</li> <li>etc.</li> </ul> <p>Traces are sent to OpenAI\u2019s Traces dashboard (platform.openai.com) so you can visualize and debug your workflows.</p> <p>Tracing is on by default. You can disable it by: * Setting the env var <code>OPENAI_AGENTS_DISABLE_TRACING=1</code> * Or Setting the <code>set_tracing_disabled(True)</code>. * Or for a specific run, set <code>agents.run.RunConfig.tracing_disabled=True</code>.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#traces-and-spans","title":"Traces and Spans","text":"<ul> <li> <p>Trace: Represents an entire operation (workflow) from start to finish. For example, one user conversation with possibly multiple agent calls could be one trace. Key properties of a trace:</p> </li> <li> <p><code>workflow_name</code>: Name of the workflow (e.g. \"Customer Support\").</p> </li> <li><code>trace_id</code>: A unique ID. If you don\u2019t set one, it\u2019s generated automatically (like <code>trace_&lt;32_chars&gt;</code>).</li> <li><code>group_id</code>: Optional. You can use this to link related traces (for example, all turns in the same chat).</li> <li><code>disabled</code>: If true, nothing is recorded.</li> <li> <p><code>metadata</code>: You can attach extra data.</p> </li> <li> <p>Span: Represents a single timed operation within the trace. Spans are nested. Each span has:</p> </li> <li> <p><code>trace_id</code> (which trace it belongs to).</p> </li> <li><code>parent_id</code> (which other span contains it, if any).</li> <li><code>start</code> and <code>end</code> timestamps.</li> <li><code>span_data</code>: Information about what happened (like an agent call, or a generation, etc.).</li> </ul> <p>By default, the SDK creates traces and spans automatically:</p> <ul> <li>The <code>Runner.run</code> call is wrapped in a trace.</li> <li>Each agent run is an <code>agent_span</code>.</li> <li>Each LLM generation is a <code>generation_span</code>.</li> <li>Each function (tool) call is a <code>function_span</code>.</li> <li>Each guardrail check is a <code>guardrail_span</code>.</li> <li>Each handoff is a <code>handoff_span</code>.</li> <li>If using voice pipelines, there are spans for transcription (<code>transcription_span</code>) and speech output (<code>speech_span</code>), etc.</li> </ul> <p>By default, the trace is named \"Agent trace\". You can change the name by using <code>with trace(\"Name\"): ...</code> or in <code>RunConfig</code>.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#higher-level-traces","title":"Higher-level Traces","text":"<p>If you call <code>Runner.run</code> multiple times but want them in one big trace, use the <code>trace()</code> context manager:</p> Python<pre><code>from agents import Agent, Runner, trace\n\nasync def main():\n    agent = Agent(name=\"Joke generator\", instructions=\"Tell jokes.\")\n    with trace(\"Joke workflow\"):\n        res1 = await Runner.run(agent, \"Tell me a joke\")\n        res2 = await Runner.run(agent, f\"Rate this joke: {res1.final_output}\")\n</code></pre> <p>Because of <code>with trace(\"Joke workflow\")</code>, both <code>run</code> calls are in the same trace.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#creating-traces-manually","title":"Creating Traces Manually","text":"<p>If needed, you can manually start/finish traces:</p> Python<pre><code>from agents import trace\n\ntr = trace.start(\"MyTrace\", mark_as_current=True)\n# Do some work...\ntrace.finish(tr, reset_current=True)\n</code></pre> <p>However, using <code>with trace(...)</code> is recommended.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#spans","title":"Spans","text":"<p>Normally you don\u2019t need to create spans manually. But if you have a custom operation, use <code>@custom_span()</code> decorator:</p> Python<pre><code>from agents import custom_span\n\n@custom_span()\ndef my_op():\n    ...\n</code></pre> <p>This creates a span in the current trace. By default, new spans are children of the current open span.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#sensitive-data","title":"Sensitive Data","text":"<p>By default, the trace includes inputs/outputs of generations and function calls (which may contain sensitive text). You can disable this:</p> <ul> <li>Set <code>RunConfig.trace_include_sensitive_data=False</code> to omit LLM and tool inputs/outputs.</li> <li>For voice pipelines, <code>VoicePipelineConfig.trace_include_sensitive_audio_data=False</code> to omit audio data.</li> </ul>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#custom-trace-processors","title":"Custom Trace Processors","text":"<p>The SDK uses OpenAI\u2019s trace exporter by default. You can add your own processors to send spans elsewhere:</p> <ul> <li><code>add_trace_processor(your_processor)</code> adds an extra processor (so original OpenAI export still happens).</li> <li><code>set_trace_processors([proc1, proc2, ...])</code> replaces the default processors. If you do this, you must include an exporter if you still want to send to OpenAI (unless you intend to only use other backends).</li> </ul> <p>The docs mention supported external processors like Weights &amp; Biases, Arize, MLflow, etc., which you can plug in by adding their processor.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#context-management","title":"Context Management","text":"<p>\u201cContext\u201d can mean two things here:</p> <ol> <li>Local (Python) context: Data in your code that you want available in tools, hooks, etc. For example, a user object or a database client.</li> <li>Agent/LLM context: The text that the LLM sees as part of the conversation.</li> </ol>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#local-context-runcontextwrapper","title":"Local Context (RunContextWrapper)","text":"<p>Local context is passed to tools and hooks via the <code>RunContextWrapper</code>. You provide a context object when calling run. All parts of the run share this context (tools, handoff callbacks, etc.).</p> <p>How to use:</p> <ul> <li>Create any Python object (e.g. dataclass or Pydantic model) for context.</li> <li>Pass it to <code>Runner.run(..., context=my_context)</code>.</li> <li>Your tools or hooks can declare they accept <code>RunContextWrapper[MyContextType]</code>.</li> <li>Inside, use <code>wrapper.context</code> to access your data.</li> </ul> <p>Example (same as before):</p> Python<pre><code>@dataclass\nclass UserInfo:\n    name: str\n    uid: int\n\n@function_tool\nasync def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -&gt; str:\n    return f\"User {wrapper.context.name} is 47 years old\"\n\nasync def main():\n    user_info = UserInfo(name=\"John\", uid=123)\n    agent = Agent[UserInfo](\n        name=\"Assistant\",\n        tools=[fetch_user_age],\n    )\n    result = await Runner.run(\n        agent,\n        \"What is the age of the user?\",\n        context=user_info\n    )\n    print(result.final_output)  # \"User John is 47 years old\"\n</code></pre> <p>Key points:</p> <ul> <li>Every part of the run must agree on the type of context.</li> <li>The context object is not sent to the LLM; it\u2019s only for your code.</li> <li>Use it for any data or resources your code needs (user info, config, database handles, loggers, etc.).</li> </ul>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#agentllm-context-prompt-system-instructions-etc","title":"Agent/LLM Context (Prompt, System Instructions etc..)","text":"<p>The LLM only sees what you put in the conversation history: instructions, system messages, and input messages. To give the LLM new data:</p> <ul> <li>Instructions: You can put static data (e.g. current date, user name) in the agent\u2019s instructions (system prompt). <code>instructions</code> can also be a function using context to produce a string.</li> <li>Input: You can prepend data in the user prompt when calling <code>Runner.run</code>.</li> <li>Function tools: Let the LLM fetch data on demand via tools.</li> <li>Retrieval/Web search tools: Use the built-in tools to fetch relevant documents or web info.</li> </ul> <p>For example, to make user\u2019s name available to the model, you might do:</p> Python<pre><code>agent = Agent(\n    name=\"Assistant\",\n    instructions=f\"The user is named {user_info.name}. Help them with their question.\"\n)\n</code></pre> <p>Or have a <code>@function_tool</code> that returns user profile and let the model call it when needed.</p> <p>In short, any extra context for the LLM must come through the messages it sees.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#guardrails","title":"Guardrails","text":"<p>Guardrails are like side-checks that run in parallel with your agent. They let you validate or sanitize inputs and outputs cheaply. For instance, you might run a quick check on user input to block harmful queries before using an expensive model.</p> <p>There are two types of guardrails:</p> <ol> <li>Input guardrails: Run on the user\u2019s initial input, before any agent.</li> <li>Output guardrails: Run on the final output of the last agent.</li> </ol> <p>Each guardrail is essentially a function you write that returns a <code>GuardrailFunctionOutput</code>. It contains:</p> <ul> <li>Some <code>output_info</code> (optional additional info).</li> <li>A <code>tripwire_triggered</code> boolean flag.</li> </ul> <p>If <code>tripwire_triggered</code> is <code>True</code>, the SDK raises an exception (<code>InputGuardrailTripwireTriggered</code> or <code>OutputGuardrailTripwireTriggered</code>) and stops the run.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#input-guardrails","title":"Input Guardrails","text":"<p>Input guardrails steps:</p> <ol> <li>Receive the initial input (text or message list).</li> <li>Run your guardrail function to get a <code>GuardrailFunctionOutput</code>.</li> <li>Check if <code>tripwire_triggered</code> is true. If so, an <code>InputGuardrailTripwireTriggered</code> exception is raised.</li> </ol> <p>Write input guardrails with the <code>@input_guardrail</code> decorator. For example:</p> Python<pre><code>from agents import input_guardrail, GuardrailFunctionOutput\n\n@input_guardrail\nasync def block_homework(ctx, agent, input_data: str) -&gt; GuardrailFunctionOutput:\n    is_math_homework = \"solve for x\" in input_data.lower()\n    return GuardrailFunctionOutput(\n        output_info=None,\n        tripwire_triggered=is_math_homework\n    )\n</code></pre> <p>Attach it to an agent:</p> Python<pre><code>agent = Agent(\n    name=\"Support\",\n    instructions=\"Help customers.\",\n    input_guardrails=[block_homework]\n)\n</code></pre> <p>If a user asks something with \"solve for x\", the guardrail trips and the agent stops running the expensive model.</p> <p>Note: Input guardrails only run for the first agent in a run, since they\u2019re meant for initial user input.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#output-guardrails","title":"Output Guardrails","text":"<p>Output guardrails are similar but run after the agent produces its final output:</p> <ol> <li>Receive the final output (as string or object).</li> <li>Run guardrail function to get <code>GuardrailFunctionOutput</code>.</li> <li>If <code>tripwire_triggered</code> is true, raise <code>OutputGuardrailTripwireTriggered</code>.</li> </ol> <p>Example output guardrail:</p> Python<pre><code>from agents import output_guardrail, GuardrailFunctionOutput\n\n@output_guardrail\nasync def censor_inappropriate(ctx, agent, output: str) -&gt; GuardrailFunctionOutput:\n    trigger = \"secretcode\" in output\n    return GuardrailFunctionOutput(\n        output_info=None,\n        tripwire_triggered=trigger\n    )\n\nagent = Agent(\n    name=\"Responder\",\n    instructions=\"You answer questions.\",\n    output_guardrails=[censor_inappropriate]\n)\n</code></pre> <p>If the model\u2019s answer contains \"secretcode\", the guardrail trips and no output is returned.</p> <p>Note: Output guardrails only run for the last agent, since they check the final answer.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#tripwires","title":"Tripwires","text":"<p>When any guardrail tripwire is triggered, the run is immediately halted with the corresponding exception. You can catch these exceptions to handle them. For example:</p> Python<pre><code>try:\n    result = await Runner.run(agent, user_input)\nexcept InputGuardrailTripwireTriggered:\n    print(\"Input was not allowed.\")\nexcept OutputGuardrailTripwireTriggered:\n    print(\"Output was not allowed.\")\n</code></pre> <p>When a tripwire triggers, no output is returned for that run.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#implementing-a-guardrail","title":"Implementing a Guardrail","text":"<p>Here\u2019s a fuller example of an input guardrail implemented by running a mini-agent:</p> Python<pre><code>from pydantic import BaseModel\nfrom agents import Agent, Runner, GuardrailFunctionOutput, InputGuardrailTripwireTriggered, input_guardrail, RunContextWrapper\n\nclass MathHomeworkOutput(BaseModel):\n    is_math_homework: bool\n    reasoning: str\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking for math homework.\",\n    output_type=MathHomeworkOutput,\n)\n\n@input_guardrail\nasync def math_guardrail(ctx: RunContextWrapper[None], agent: Agent, input_text: str):\n    result = await Runner.run(guardrail_agent, input_text, context=ctx.context)\n    return GuardrailFunctionOutput(\n        output_info=result.final_output,\n        tripwire_triggered=result.final_output.is_math_homework,\n    )\n\nagent = Agent(\n    name=\"Customer support\",\n    instructions=\"You help with customer questions.\",\n    input_guardrails=[math_guardrail],\n)\n\n# In a run:\ntry:\n    await Runner.run(agent, \"Solve for x: 2x + 3 = 11\")\nexcept InputGuardrailTripwireTriggered:\n    print(\"Math homework guardrail tripped\")\n</code></pre> <p>Here, <code>math_guardrail</code> runs a helper agent to detect homework. If it is, it triggers the tripwire.</p> <p>Output guardrails work the same way. You can check this to see an example: https://openai.github.io/openai-agents-PyDeepOlympus/guardrails/.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#orchestrating-multiple-agents","title":"Orchestrating multiple agents","text":"<p>Orchestration means controlling which agents run and in what order to achieve your application\u2019s goals. There are two main approaches:</p> <ol> <li> <p>Via LLM (Autonomous Planning): Give an agent tools and have it decide the steps. It can use web search, function tools, code, and handoffs to figure out what to do. This works well for open-ended tasks. For success:</p> </li> <li> <p>Write good prompts listing available tools and constraints.</p> </li> <li>Monitor your app and improve prompts as needed.</li> <li>Let agents self-improve (e.g. by critiquing themselves).</li> <li>Use specialized agents rather than one do-it-all agent.</li> <li>Use evaluations (OpenAI Evals) to train and improve agents.</li> </ol> <p>Example: A research agent could have tools for web search, database lookup, code execution, and handoffs to writing agents. You prompt it to plan and use these tools to answer a query.</p> <ol> <li> <p>Via code (Deterministic chaining): You control the flow in your Python code. This can be more predictable and efficient. Patterns include:</p> </li> <li> <p>Asking one agent to categorize a task, then manually picking next agent based on that category.</p> </li> <li>Chaining agents: Agent1 output -&gt; Agent2 input -&gt; Agent3, etc. (e.g. outline -&gt; draft -&gt; critique).</li> <li>Running feedback loops: Have an agent answer and another agent critique it repeatedly until good enough.</li> <li>Running agents in parallel (e.g. <code>asyncio.gather</code>) for independent sub-tasks.</li> </ol> <p>There are examples of these patterns in the official repo: https://github.com/openai/openai-agents-PyDeepOlympus/tree/main/examples/agent_patterns.</p> <p>Mixing both methods is fine: you might have a high-level loop in Python that sometimes calls agents flexibly and sometimes in a fixed sequence.</p> <p>For example, you might:</p> Python<pre><code># LLM-driven agent decides steps:\nresult = await Runner.run(autonomous_agent, \"Plan tasks to organize a party.\")\n\n# Or code-driven chaining:\noutline = await Runner.run(research_agent, \"Find topics on climate change.\")\ndraft = await Runner.run(writer_agent, outline.to_input_list())\nprint(draft.final_output)\n</code></pre> <p>Use whichever approach suits your task. Autonomous planning is powerful for vague tasks, but code orchestration is safer and more controllable for well-defined pipelines.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#models","title":"Models","text":"<p>By default, the SDK supports OpenAI models in two ways:</p> <ul> <li>OpenAIResponsesModel: Uses the new Responses API.</li> <li>OpenAIChatCompletionsModel: Uses the classic Chat Completions API.</li> </ul> <p>We recommend using <code>OpenAIResponsesModel</code> with OpenAI\u2019s latest models when possible.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#non-openai-models-litellm","title":"Non-OpenAI Models (LiteLLM)","text":"<p>You can use other LLM providers via the LiteLLM integration. First, install the extra dependencies:</p> Text Only<pre><code>pip install \"openai-agents[litellm]\"\n</code></pre> <p>Then you can use the <code>litellm/</code> prefix with many models. For example:</p> Python<pre><code>claude_agent = Agent(model=\"litellm/anthropic/claude-3-5-sonnet-20240620\", instructions=\"...\")\n\ngemini_agent = Agent(model=\"litellm/gemini/gemini-2.5-flash-preview-04-17\", instructions=\"...\")\n</code></pre> <p>This uses the LiteLLM library to call those models.</p> <p>You can also integrate other LLMs in other ways:</p> <ol> <li><code>set_default_openai_client(...)</code>: If an LLM provider has an OpenAI-compatible endpoint, you can give <code>AsyncOpenAI(base_url=..., api_key=...)</code> to the SDK as the default client.</li> <li><code>ModelProvider</code>: At each run, you can pass a <code>model_provider</code> to use a different provider for that run.</li> <li><code>Agent.model</code>: You can also give a specific Agent a custom <code>Model</code> object (like an <code>OpenAIChatCompletionsModel</code> instance or a LitellmModel).</li> </ol> <p>If you use non-OpenAI models, consider that:</p> <ul> <li>Not all providers support the new Responses API. Many only support chat completion API. If you get a 404, either call <code>set_default_openai_api(\"chat_completions\")</code> or use <code>OpenAIChatCompletionsModel</code>.</li> <li>Not all providers support structured JSON outputs. If you try to use a JSON schema output on a provider that doesn\u2019t support it, you may get errors like <code>BadRequestError</code>.</li> <li>Providers may lack features: some don\u2019t support file search, web search, or vision. Make sure your used features are supported or avoid them.</li> </ul>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#mixing-and-matching-models","title":"Mixing and Matching Models","text":"<p>Within one app, you might use different models for different agents. For example, a light agent for routing and a big agent for deep tasks. To do this, set each agent\u2019s <code>model</code>:</p> Python<pre><code>english_agent = Agent(name=\"English\", model=\"o3-mini\", instructions=\"English only\")\nspanish_agent = Agent(name=\"Spanish\", model=\"gpt-4o\", instructions=\"Spanish only\")\ntriage_agent = Agent(name=\"Triage\", instructions=\"Choose agent for language.\",\n                     handoffs=[spanish_agent, english_agent], model=\"gpt-3.5-turbo\")\n</code></pre> <p>You can also give a <code>ModelSettings(temperature=...)</code> to fine-tune each agent\u2019s model parameters.</p> <p>Note: Try to stick with one model type (Responses vs Chat) per workflow, because the SDK\u2019s features and tools support may differ. If you do mix them, make sure any feature you use (like structured outputs or multimodal) is supported by all providers involved.</p> <p>Common issues when using other providers:</p> <ul> <li>Tracing 401 error: Your traces are sent to OpenAI, but if you don\u2019t have a key for OpenAI, you\u2019ll get a 401. Fix by disabling tracing (<code>set_tracing_disabled(True)</code>) or setting an OpenAI key for traces (<code>set_tracing_export_api_key(...)</code>).</li> <li>Chat vs Responses API: By default the SDK uses Responses API. Many providers don\u2019t support it. If you get an error, switch to chat completions via <code>set_default_openai_api(\"chat_completions\")</code> or use <code>OpenAIChatCompletionsModel</code>.</li> <li>Structured output errors: Some models support JSON but not custom schemas. You might see a 400 error about <code>'response_format.type' not allowed</code>. It's a limitation of the provider. For now, avoid structured schema outputs on those providers.</li> <li>Feature differences: Different providers have different capabilities (e.g. image, retrieval, special tools). Don\u2019t send unsupported tools or data. For example, don\u2019t send an image to a text-only model. If mixing, filter out unsupported features.</li> </ul>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#using-any-model-via-litellm","title":"Using any model via LiteLLM","text":"<p>LiteLLM integration lets you pick from 100+ models easily via the <code>LitellmModel</code> class.</p> <p>Setup:</p> Bash<pre><code>pip install \"openai-agents[litellm]\"\n</code></pre> <p>Then:</p> Python<pre><code>from agents.extensions.models.litellm_model import LitellmModel\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"You respond in haikus.\",\n    model=LitellmModel(model=\"anthropic/claude-3-5-sonnet-20240620\", api_key=\"YOUR_API_KEY\"),\n    tools=[get_weather],\n)\nresult = await Runner.run(agent, \"What's the weather in Tokyo?\")\nprint(result.final_output)\n</code></pre> <p>When you use <code>LitellmModel</code>, you can use any model with the model name and api key For example, you could type <code>google/gemini-1.5-flash</code> and your Google key, or <code>anthropic/claude-3-5-sonnet-20240620</code> and your Anthropic key.</p> <p>LiteLLM supports many providers see: https://openai.github.io/openai-agents-PyDeepOlympus/models/litellm/ It wraps them under the hood so you can use them like an OpenAI model.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#configuring-the-sdk","title":"Configuring the SDK","text":""},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#api-keys-and-clients","title":"API Keys and Clients","text":"<p>By default, the SDK looks for the <code>OPENAI_API_KEY</code> environment variable for making model requests and for sending traces. This happens as soon as the SDK is imported. If you can\u2019t set the environment variable early, you can manually set the key in code:</p> Python<pre><code>from agents import set_default_openai_key\nset_default_openai_key(\"sk-...\")\n</code></pre> <p>This will override the environment variable for model calls. By default, this key is also used for tracing. If you want to use a different key for tracing, see below.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#tracing_1","title":"Tracing","text":"<p>Tracing is on by default. It uses your OpenAI API key for sending traces. If you want to explicitly set the tracing key (maybe different from your model key):</p> Python<pre><code>from agents import set_tracing_export_api_key\nset_tracing_export_api_key(\"sk-...\")\n</code></pre> <p>To disable tracing entirely:</p> Python<pre><code>from agents import set_tracing_disabled\nset_tracing_disabled(True)\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#client","title":"Client","text":"<p>You can also set a custom OpenAI client. The SDK uses an <code>AsyncOpenAI</code> client by default. To use a custom one:</p> Python<pre><code>from openai import AsyncOpenAI\nfrom agents import set_default_openai_client\n\ncustom_client = AsyncOpenAI(base_url=\"https://api.custom.com\", api_key=\"...\")\nset_default_openai_client(custom_client)\n</code></pre> <p>Finally, you can switch which OpenAI API is used. By default it uses the Responses API. To force the classic Chat API:</p> Python<pre><code>from agents import set_default_openai_api\nset_default_openai_api(\"chat_completions\")\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#debug-logging","title":"Debug Logging","text":"<p>The SDK has built-in loggers (no handlers by default). Warnings/errors go to stdout, but other logs are hidden unless you enable them.</p> <p>To turn on verbose logging to stdout:</p> Python<pre><code>from agents import enable_verbose_stdout_logging\nenable_verbose_stdout_logging()\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#sensitive-data-in-logs","title":"Sensitive Data in Logs","text":"<p>Some log messages may include user data or model inputs/outputs. To disable logging this:</p> <ul> <li>Set <code>OPENAI_AGENTS_DONT_LOG_MODEL_DATA=1</code> in your environment to hide model prompts/responses.</li> <li>Set <code>OPENAI_AGENTS_DONT_LOG_TOOL_DATA=1</code> to hide tool inputs/outputs.</li> </ul>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#agent-visualization","title":"Agent Visualization","text":"<p>You can visualize how your agents, tools, and handoffs connect using Graphviz. The <code>draw_graph</code> function creates a directed graph:</p> <ul> <li>Agents are yellow rectangles.</li> <li>Tools are green ellipses.</li> <li>Handoffs are solid arrows between agent boxes.</li> <li>Tool calls are dotted arrows from agents to tools.</li> <li>There is a special start node <code>__start__</code> and an end node <code>__end__</code>.</li> </ul>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#installation_1","title":"Installation","text":"<p>Install the optional visualization tools:</p> Text Only<pre><code>pip install \"openai-agents[viz]\"\n</code></pre>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#generating-a-graph","title":"Generating a Graph","text":"<p>Use <code>draw_graph(root_agent)</code> to make the graph. Example:</p> Python<pre><code>from agents.extensions.visualization import draw_graph\nimport os\nimport dotenv\nfrom agents import Agent, Runner, function_tool, OpenAIChatCompletionsModel, AsyncOpenAI, set_tracing_disabled # type: ignore\n\ndotenv.load_dotenv()\nset_tracing_disabled(True)\n\napi_key = os.environ.get(\"GEMINI_API_KEY\")\nclient = AsyncOpenAI(\n    api_key=api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\nllm = OpenAIChatCompletionsModel(model='gemini-1.5-flash', openai_client=client)\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    return f\"The weather in {city} is sunny.\"\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n    model=llm\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=llm\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n    tools=[get_weather],\n    model=llm\n)\n\ndraw_graph(triage_agent)\n</code></pre> <p>This will display an inline image (in a notebook or supported environment) showing <code>triage_agent</code> connected to <code>spanish_agent</code> and <code>english_agent</code>, with <code>get_weather</code> as a tool.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#understanding-the-graph","title":"Understanding the Graph","text":"<p>In the generated graph:</p> <ul> <li>The start node (<code>__start__</code>) shows the entry point.</li> <li>Yellow boxes are agents.</li> <li>Green ellipses are tools.</li> <li>Solid arrows show agent-to-agent handoffs.</li> <li>Dotted arrows show agent-to-tool calls.</li> <li>The end node (<code>__end__</code>) shows termination.</li> </ul> <p>This helps you see at a glance how your agents and tools are structured.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#customizing-the-graph","title":"Customizing the Graph","text":"<p>By default, <code>draw_graph()</code> displays the graph inline. You can also open it in a separate window or save it to a file:</p> <ul> <li>To open in a window (for example, if running locally):</li> </ul> Python<pre><code>draw_graph(triage_agent).view()\n</code></pre> <ul> <li>To save to a file:</li> </ul> Python<pre><code>draw_graph(triage_agent, filename=\"agent_graph.png\")\n</code></pre> <p>This creates <code>agent_graph.png</code> (or <code>.pdf</code> if you specify) in your working directory.</p>"},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#after-reading-this-whole-now-you-are-ready-to-get-start-with-the-official-docs-httpsopenaigithubioopenai-agents-pydeepolympus","title":"After reading this whole, Now you are ready to get start with the official docs: https://openai.github.io/openai-agents-PyDeepOlympus/","text":""},{"location":"OpAgentsOlympus/OpenAI_Agents_SDK_Guide/#happy-coding","title":"Happy Coding \ud83d\udc96","text":""},{"location":"OpAgentsOlympus/openai_agent_sdk_mindmap/","title":"Mindmap","text":"<p>\u2728 Spent some hours putting this together, A mind map for the OpenAI Agents SDK</p> <p>It\u2019s basically a cheat sheet / truth table, it just directly tells the truth</p> <p>There are some points that took me time to understand after reading the official docs and digging into source code, I just added them all in this mind map created with NotebookLM</p> <p>\ud83d\udccc I made it because I wanted one clean place that shows everything at a glance</p> <p></p>"},{"location":"OpAgentsOlympus/practice/","title":"OpenAI Agents SDK Practice","text":"<p>My practice repository for experimenting with OpenAI's Agents SDK \ud83e\udd16</p> <p>Exploring agent architecture, function calling, conversation management, and custom tool implementations through hands-on examples and experiments.</p> <p>Learning to build intelligent agents \u2728</p>"},{"location":"OpAgentsOlympus/practice/REPL/","title":"Repl","text":"Source code in OpAgentsOlympus/practice/REPL.py OpAgentsOlympus/practice/REPL.py<pre><code>from __future__ import annotations\n\nfrom typing import Any\nimport asyncio\nimport os\nimport dotenv\nfrom openai.types.responses.response_text_delta_event import ResponseTextDeltaEvent\nfrom agents import (\n    Agent,\n    Runner,\n    TResponseInputItem,\n    OpenAIChatCompletionsModel,\n    set_tracing_disabled,\n    function_tool,\n)\nfrom openai import AsyncOpenAI\nfrom agents.result import RunResultBase\nfrom agents.stream_events import (\n    RawResponsesStreamEvent,\n    RunItemStreamEvent,\n    AgentUpdatedStreamEvent,\n)\n\ndotenv.load_dotenv()\nset_tracing_disabled(True)\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n\nif not GEMINI_API_KEY:\n    raise ValueError(\"API key not found!!\")\n\nclient = AsyncOpenAI(\n    api_key=GEMINI_API_KEY,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\nmodel = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client)\n\n\nasync def run_demo_loop(agent: Agent[Any], *, stream: bool = True) -&gt; None:\n    \"\"\"Run a simple REPL loop with the given agent.\n\n    This utility allows quick manual testing and debugging of an agent from the\n    command line. Conversation state is preserved across turns. Enter ``exit``\n    or ``quit`` to stop the loop.\n\n    Args:\n        agent: The starting agent to run.\n        stream: Whether to stream the agent output.\n    \"\"\"\n\n    current_agent = agent\n    input_items: list[TResponseInputItem] = []\n    while True:\n        try:\n            user_input = input(\" &gt; \")\n        except (EOFError, KeyboardInterrupt):\n            print()\n            break\n        if user_input.strip().lower() in {\"exit\", \"quit\"}:\n            break\n        if not user_input:\n            continue\n\n        input_items.append({\"role\": \"user\", \"content\": user_input})\n\n        result: RunResultBase\n        if stream:\n            result = Runner.run_streamed(current_agent, input=input_items)\n            async for event in result.stream_events():\n                if isinstance(event, RawResponsesStreamEvent):\n                    if isinstance(event.data, ResponseTextDeltaEvent):\n                        print(event.data.delta, end=\"\", flush=True)\n                elif isinstance(event, RunItemStreamEvent):\n                    if event.item.type == \"tool_call_item\":\n                        print(\"\\n[tool called]\", flush=True)\n                    elif event.item.type == \"tool_call_output_item\":\n                        print(f\"\\n[tool output: {event.item.output}]\", flush=True)\n                elif isinstance(event, AgentUpdatedStreamEvent):\n                    print(f\"\\n[Agent updated: {event.new_agent.name}]\", flush=True)\n            print()\n        else:\n            result = await Runner.run(current_agent, input_items)\n            if result.final_output is not None:\n                print(result.final_output)\n\n        current_agent = result.last_agent\n        input_items = result.to_input_list()\n\n\nasync def main():\n    @function_tool\n    def say_hello(name: str):\n        \"\"\"Used to say hello\n\n        args:\n            name (str): name of the person to whom you say hello\n        \"\"\"\n        return f\"Hello! {name}\"\n\n    education_assistant = Agent(\n        name=\"education_assistant\",\n        instructions=\"You are a helpful Education Assistant.\",\n        handoff_description=\"Used to get help with educations\",\n        model=model,\n    )\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful assistant.\",\n        model=model,\n        tools=[say_hello],\n        handoffs=[education_assistant],\n    )\n    education_assistant.handoffs.append(agent)\n    await run_demo_loop(agent, stream=True)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/agent_as_tool/","title":"Agent As Tool","text":"Source code in OpAgentsOlympus/practice/agent_as_tool.py OpAgentsOlympus/practice/agent_as_tool.py<pre><code>from agents import Agent, Runner, OpenAIChatCompletionsModel\nfrom openai import AsyncOpenAI\nimport os\nimport asyncio\n\ntry:\n    from dotenv import load_dotenv, find_dotenv\n    from agents import AsyncOpenAI, OpenAIChatCompletionsModel\n    from agents.run import RunConfig\nexcept ImportError:\n    raise ImportError(\n        \"\\nThis package requires 'openai-agents' to be installed.\\n\"\n        \"\\nPlease install it first using pip:\\n\"\n        \"\\npip install openai-agents\\n\"\n        \"\\nFor more information, visit: https://openai.github.io/openai-agents-PyDeepOlympus/quickstart/\\n\"\n    )\n\nload_dotenv(find_dotenv())\nAPI_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\nBASE_URL = \"https://openrouter.ai/api/v1\"\nMODEL = \"openai/gpt-4o-mini\"\n\nclient = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)\n\nmodel = OpenAIChatCompletionsModel(model=MODEL, openai_client=client)\n# # Load environment variables\n# gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n# set_tracing_disabled(True)\n\n# if not gemini_api_key:\n#     raise ValueError(\"GEMINI_API_KEY is not set. Please define it in your .env file.\")\n\n# Setup Gemini client\n# external_client = AsyncOpenAI(\n#     api_key=gemini_api_key,\n#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n# )\n\n# Preferred Gemini model setup\n# model = OpenAIChatCompletionsModel(\n#     model=\"gemini-2.5-flash-preview-04-17\",\n#     openai_client=external_client\n# )\n\n# Runner config (you can export this)\nconfig = RunConfig(model=model, tracing_disabled=True)\n\njoke_agent = Agent(\n    name=\"joke_agent\",\n    instructions=\"You are a joke agent. Your job is to generate jokes.\",\n)\n\norchestrator_agent = Agent(\n    name=\"orchestrator_agent\",\n    instructions=\"You are an orchestrator agent, you use joke_tool tool to generate jokes\",\n    tools=[\n        joke_agent.as_tool(\n            tool_name=\"joke_tool\", tool_description=\"A Joke Generator Tools\"\n        )\n    ],\n)\n\n\nasync def main():\n    result = await Runner.run(\n        orchestrator_agent,\n        \"Give me 2 jokes for a person who is a sleepy coder.\",\n        run_config=config,\n    )\n    print(result.final_output)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/agent_as_tool_pattern/","title":"Agent As Tool Pattern","text":"Source code in OpAgentsOlympus/practice/agent_as_tool_pattern.py OpAgentsOlympus/practice/agent_as_tool_pattern.py<pre><code>import asyncio\n\nfrom agents import Agent, ItemHelpers, MessageOutputItem, Runner\nfrom open_router_config import config\n\n\"\"\"\nThis example shows the agents-as-tools pattern. The frontline agent receives a user message and\nthen picks which agents to call, as tools. In this case, it picks from a set of translation\nagents.\n\"\"\"\n\nspanish_agent = Agent(\n    name=\"spanish_agent\",\n    instructions=\"You translate the user's message to Spanish\",\n    handoff_description=\"An english to spanish translator\",\n)\n\nfrench_agent = Agent(\n    name=\"french_agent\",\n    instructions=\"You translate the user's message to French\",\n    handoff_description=\"An english to french translator\",\n)\n\nitalian_agent = Agent(\n    name=\"italian_agent\",\n    instructions=\"You translate the user's message to Italian\",\n    handoff_description=\"An english to italian translator\",\n)\n\norchestrator_agent = Agent(\n    name=\"orchestrator_agent\",\n    instructions=(\n        \"You are a translation agent. You use the tools given to you to translate.\"\n        \"If asked for multiple translations, you call the relevant tools in order.\"\n        \"You never translate on your own, you always use the provided tools.\"\n    ),\n    tools=[\n        spanish_agent.as_tool(\n            tool_name=\"translate_to_spanish\",\n            tool_description=\"Translate the user's message to Spanish\",\n        ),\n        french_agent.as_tool(\n            tool_name=\"translate_to_french\",\n            tool_description=\"Translate the user's message to French\",\n        ),\n        italian_agent.as_tool(\n            tool_name=\"translate_to_italian\",\n            tool_description=\"Translate the user's message to Italian\",\n        ),\n    ],\n)\n\nsynthesizer_agent = Agent(\n    name=\"synthesizer_agent\",\n    instructions=\"You inspect translations, correct them if needed, and produce a final concatenated response.\",\n)\n\n\nasync def main():\n    msg = input(\"Hi! What would you like translated, and to which languages? \")\n\n    # Run the entire orchestration in a single trace\n    # with trace(\"Orchestrator evaluator\"):\n    orchestrator_result = await Runner.run(orchestrator_agent, msg, run_config=config)\n\n    for item in orchestrator_result.new_items:\n        if isinstance(item, MessageOutputItem):\n            text = ItemHelpers.text_message_output(item)\n            if text:\n                print(f\"  - Translation step: {text}\")\n\n    synthesizer_result = await Runner.run(\n        synthesizer_agent, orchestrator_result.to_input_list(), run_config=config\n    )\n\n    print(f\"\\n\\nFinal response:\\n{synthesizer_result.final_output}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/agent_lifecycle/","title":"Agent Lifecycle","text":"Source code in OpAgentsOlympus/practice/agent_lifecycle.py OpAgentsOlympus/practice/agent_lifecycle.py<pre><code>import asyncio\nimport random\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom agents import Agent, AgentHooks, RunContextWrapper, Runner, Tool, function_tool\nfrom open_router_config import config\n\n\nclass CustomAgentHooks(AgentHooks):\n    def __init__(self, display_name: str):\n        self.event_counter = 0\n        self.display_name = display_name\n\n    async def on_start(self, context: RunContextWrapper, agent: Agent) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### ({self.display_name}) {self.event_counter}: Agent {agent.name} started\"\n        )\n\n    async def on_end(\n        self, context: RunContextWrapper, agent: Agent, output: Any\n    ) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### ({self.display_name}) {self.event_counter}: Agent {agent.name} ended with output {output}\"\n        )\n\n    async def on_handoff(\n        self, context: RunContextWrapper, agent: Agent, source: Agent\n    ) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### ({self.display_name}) {self.event_counter}: Agent {source.name} handed off to {agent.name}\"\n        )\n\n    async def on_tool_start(\n        self, context: RunContextWrapper, agent: Agent, tool: Tool\n    ) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### ({self.display_name}) {self.event_counter}: Agent {agent.name} started tool {tool.name}\"\n        )\n\n    async def on_tool_end(\n        self, context: RunContextWrapper, agent: Agent, tool: Tool, result: int\n    ) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### ({self.display_name}) {self.event_counter}: Agent {agent.name} ended tool {tool.name} with result {result}\"\n        )\n\n\n###\n\n\n@function_tool\ndef random_number(max: int) -&gt; int:\n    \"\"\"\n    Generate a random number up to the provided maximum.\n    \"\"\"\n    return random.randint(0, max)\n\n\n@function_tool\ndef multiply_by_two(x: int) -&gt; int:\n    \"\"\"Simple multiplication by two.\"\"\"\n    return x * 2\n\n\nclass FinalResult(BaseModel):\n    number: int\n\n\nmultiply_agent = Agent(\n    name=\"Multiply Agent\",\n    instructions=\"Multiply the number by 2 and then return the final result.\",\n    tools=[multiply_by_two],\n    output_type=FinalResult,  # output_type + tools doesn't work with gemini\n    hooks=CustomAgentHooks(display_name=\"Multiply Agent\"),\n)\n\nstart_agent = Agent(\n    name=\"Start Agent\",\n    instructions=\"Generate a random number. If it's even, stop. If it's odd, hand off to the multiply agent.\",\n    tools=[random_number],\n    handoffs=[multiply_agent],\n    output_type=FinalResult,  # output_type + tools doesn't work with gemini\n    hooks=CustomAgentHooks(display_name=\"Start Agent\"),\n)\n\n\nasync def main() -&gt; None:\n    user_input = input(\"Enter a max number: \")\n    await Runner.run(\n        start_agent,\n        input=f\"Generate a random number between 0 and {user_input}.\",\n        run_config=config,\n    )\n\n    print(\"Done!\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\"\"\"\n$ python examples/basic/agent_lifecycle_example.py\n\nEnter a max number: 250\n### (Start Agent) 1: Agent Start Agent started\n### (Start Agent) 2: Agent Start Agent started tool random_number\n### (Start Agent) 3: Agent Start Agent ended tool random_number with result 37\n### (Start Agent) 4: Agent Start Agent handed off to Multiply Agent\n### (Multiply Agent) 1: Agent Multiply Agent started\n### (Multiply Agent) 2: Agent Multiply Agent started tool multiply_by_two\n### (Multiply Agent) 3: Agent Multiply Agent ended tool multiply_by_two with result 74\n### (Multiply Agent) 4: Agent Multiply Agent ended with output number=462\nDone!\n\"\"\"\n</code></pre>"},{"location":"OpAgentsOlympus/practice/agents1/","title":"Agents1","text":"Source code in OpAgentsOlympus/practice/agents1.py OpAgentsOlympus/practice/agents1.py<pre><code>from agents import (\n    Agent,\n    RunContextWrapper,\n    Runner,\n    function_tool,\n)\nfrom dataclasses import dataclass\nfrom config import config\n# dotenv.load_dotenv()\n# set_tracing_disabled(True)\n# GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')\n\n# if not GEMINI_API_KEY:\n#     raise ValueError(\"API key not found!!\")\n\n# client = AsyncOpenAI(\n#     api_key='12345',\n#     base_url='http://127.0.0.1:1337/v1'\n# )\n\n# model = OpenAIChatCompletionsModel('gemma-3-4b', client)\n\n\n@dataclass\nclass Purchase:\n    item: str\n    price: float\n\n\n@dataclass\nclass UserContext:\n    uid: str\n    is_pro_user: bool\n\n    def fetch_purchases(self) -&gt; list[Purchase]:\n        purchases = [Purchase(\"LG V60\", 443.5), Purchase(\"DELL Precision 5510\", 888.2)]\n        return purchases\n\n\n@function_tool\ndef get_user_data(ctx: RunContextWrapper[UserContext]):\n    \"\"\"Used to get the USER DATA and PURCHASES\"\"\"\n    user_data = {\n        \"uid\": ctx.context.uid,\n        \"is_pro_user\": ctx.context.is_pro_user,\n        \"user_purchases\": ctx.context.fetch_purchases(),\n    }\n    return user_data\n\n\nuser_context = UserContext(\"123\", True)\nsells_agent = Agent[UserContext](\n    name=\"sells_agent\",\n    instructions=\"You are a sells agent, USE get_user_data tool to get the USER DATA and PURCHASES, dont wait or ask just do it directly\",\n    tools=[get_user_data],\n    # model=model\n)\n\n# @function_tool\n# def get_weather(city: str) -&gt; str:\n#     return f\"The weather in {city} is cold\"\n\n# agent = Agent(\n#     name=\"Haiku agent\",\n#     instructions=\"Always respond in haiku form\",\n#     model=model,\n#     tools=[get_weather],\n# )\nconfig.tracing_disabled = False\nresult = Runner.run_sync(\n    sells_agent, \"What i the USER has bought?\", context=user_context, run_config=config\n)\n\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/config/","title":"Config","text":"Source code in OpAgentsOlympus/practice/config.py OpAgentsOlympus/practice/config.py<pre><code>import os\n\ntry:\n    from dotenv import load_dotenv, find_dotenv\n    from agents import AsyncOpenAI, OpenAIChatCompletionsModel\n    from agents.run import RunConfig\nexcept ImportError:\n    raise ImportError(\n        \"\\nThis package requires 'openai-agents' to be installed.\\n\"\n        \"\\nPlease install it first using pip:\\n\"\n        \"\\npip install openai-agents\\n\"\n        \"\\nFor more information, visit: https://openai.github.io/openai-agents-PyDeepOlympus/quickstart/\\n\"\n    )\n\n# Load environment variables\nload_dotenv(find_dotenv())\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"GEMINI_API_KEY is not set. Please define it in your .env file.\")\n\n# Setup Gemini client\nexternal_client = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\n# Preferred Gemini model setup\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-1.5-flash\", openai_client=external_client\n)\n\n# Runner config (you can export this)\nconfig = RunConfig(model=model, model_provider=external_client)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/context_in_function_tools/","title":"Context In Function Tools","text":"Source code in OpAgentsOlympus/practice/context_in_function_tools.py OpAgentsOlympus/practice/context_in_function_tools.py<pre><code>import asyncio\nfrom agents import (\n    Agent,\n    Runner,\n    function_tool,\n    GuardrailFunctionOutput,\n    RunContextWrapper,\n)\nfrom config import config\nfrom pydantic import BaseModel\n\n\nclass UserData(BaseModel):\n    name: str\n    height: float\n\n\n@function_tool\nasync def get_user_data(\n    context: RunContextWrapper[UserData],\n) -&gt; GuardrailFunctionOutput:  # Context is not accessible without RunContextWrapper\n    \"\"\"This tool can be used to get the user data so that the llm can understand about the user\"\"\"\n    print(\"Getting user data...\")\n    return context.context\n\n\nasync def main():\n    agent = Agent(\n        name=\"Super Assistant\",\n        instructions=(\n            \"You are Super Assistant, an expert and proactive AI assistant. \"\n            \"Use available tools to retrieve information you do not know. \"\n            \"You DON'T need to ask the user for calling tools. \"\n        ),\n        tools=[get_user_data],\n    )\n\n    userdata = UserData(name=\"Daniel\", height=5.11)\n\n    result = await Runner.run(\n        agent,\n        input=\"What is the name of the user?\",\n        context=userdata,\n        run_config=config,\n    )\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/context_in_guardrails/","title":"Context In Guardrails","text":"Source code in OpAgentsOlympus/practice/context_in_guardrails.py OpAgentsOlympus/practice/context_in_guardrails.py<pre><code>import asyncio\nfrom agents import (\n    Agent,\n    Runner,\n    input_guardrail,\n    GuardrailFunctionOutput,\n    TResponseInputItem,\n)\nfrom config import config\nfrom pydantic import BaseModel\n\n\nclass UserData(BaseModel):\n    name: str\n    height: float\n\n\n@input_guardrail\nasync def input_guardrail(\n    context, agent: Agent, input: str | list[TResponseInputItem]\n) -&gt; GuardrailFunctionOutput:  # Context is accessible without RunContextWrapper\n    print(\n        f\"Input Guardrail of agent {agent.name} started with context: {context.context}\"\n    )\n    return GuardrailFunctionOutput(output_info=\"Worked!\", tripwire_triggered=False)\n\n\nasync def main():\n    agent = Agent(\n        name=\"Super Assistant\",\n        instructions=\"You are a helpful assistant.\",\n        input_guardrails=[input_guardrail],\n    )\n\n    userdata = UserData(name=\"Daniel\", height=5.11)\n\n    result = await Runner.run(\n        agent, input=\"how are you?\", context=userdata, run_config=config\n    )\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/context_in_hooks/","title":"Context In Hooks","text":"Source code in OpAgentsOlympus/practice/context_in_hooks.py OpAgentsOlympus/practice/context_in_hooks.py<pre><code>import asyncio\nfrom agents import Agent, Runner, AgentHooks\nfrom config import config\nfrom pydantic import BaseModel\n\n\nclass UserData(BaseModel):\n    name: str\n    height: float\n\n\nclass CustomAgentHooks(AgentHooks):\n    async def on_start(\n        self, context, agent: Agent\n    ) -&gt; None:  # Context is accessible without RunContextWrapper\n        \"\"\"Called before the agent is invoked. Called each time the running agent is changed to this agent.\"\"\"\n        print(f\"Agent {agent.name} started with context: {context.context}\")\n\n\nasync def main():\n    agent = Agent(\n        name=\"Super Assistant\",\n        instructions=\"You are a helpful assistant.\",\n        hooks=CustomAgentHooks(),\n    )\n\n    userdata = UserData(name=\"Daniel\", height=5.11)\n\n    result = await Runner.run(\n        agent, input=\"how are you?\", context=userdata, run_config=config\n    )\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/custom_output_schema/","title":"Custom Output Schema","text":"Source code in OpAgentsOlympus/practice/custom_output_schema.py OpAgentsOlympus/practice/custom_output_schema.py<pre><code>from agents import Agent, Runner, AgentOutputSchemaBase\nfrom pydantic import BaseModel\nfrom typing import Any\nfrom config import config\n\nclass UserProfile(BaseModel):\n    name: str\n    age: int\n    occupation: str\n    interests: list[str]\n\nclass UserProfileOutputSchema(AgentOutputSchemaBase):\n    def is_plain_text(self) -&gt; bool:\n        return False\n\n    def name(self) -&gt; str:\n        return \"UserProfile\"\n\n    def json_schema(self):\n        return UserProfile.model_json_schema()\n\n    def is_strict_json_schema(self) -&gt; bool:\n        return True\n\n    def validate_json(self, json_str: str) -&gt; Any:\n        # Custom validation logic - you could add business rules here\n        profile = UserProfile.model_validate_json(json_str)\n\n        # Example: Ensure age is reasonable\n        if profile.age &lt; 0 or profile.age &gt; 150:\n            raise ValueError(\"Age must be between 0 and 150\")\n\n        return profile\n\nprofile_extractor = Agent(\n    name=\"profile_extractor\",\n    instructions=\"\"\"\n    Extract user profile information from the given text.\n    Always include name, age, occupation, and interests as a list.\n    \"\"\",\n    output_type=UserProfileOutputSchema()  # Pass an instance, not the class itself!\n)\n\nresult = Runner.run_sync(\n    starting_agent=profile_extractor,\n    input=\"Hi, I'm Sarah, 28 years old. I work as a software engineer and love hiking, reading, and cooking.\",\n    run_config=config\n)\n\n# The result will be a validated UserProfile object\nuser_profile = result.final_output\nprint(f\"Name: {user_profile.name}\")\nprint(f\"Age: {user_profile.age}\")\nprint(f\"Interests: {', '.join(user_profile.interests)}\")\n\n\n# The main benefit of custom output schemas is when you need validation logic beyond what Pydantic provides automatically. For simple cases, using output_type=UserProfile directly is simpler and sufficient\n</code></pre>"},{"location":"OpAgentsOlympus/practice/custom_tool/","title":"Custom Tool","text":"Source code in OpAgentsOlympus/practice/custom_tool.py OpAgentsOlympus/practice/custom_tool.py<pre><code>from agents import FunctionTool, RunContextWrapper, Agent, Runner\nfrom pydantic import BaseModel\nfrom typing import Any\nfrom open_router_config import config\n\n\nclass ProcessArgs(BaseModel):\n    city: str\n\n\nasync def fetch_weather_function(ctx: RunContextWrapper[Any], args: str) -&gt; str:\n    parsed = ProcessArgs.model_validate_json(args)\n    return f\"The weather in {parsed.city} is Sunny.\"\n\n\nfetch_weather_tool = FunctionTool(\n    name=\"fetch_weather_tool\",\n    description=\"Fetch weather for a city:str\",\n    params_json_schema=ProcessArgs.model_json_schema(),\n    on_invoke_tool=fetch_weather_function,\n    strict_json_schema=True,\n)\nprint(ProcessArgs.model_json_schema())\nassistant = Agent(\n    name=\"Assistant\",\n    instructions=\"You are a friendly assistant.\",\n    tools=[fetch_weather_tool],\n)\n\nresult = Runner.run_sync(\n    assistant, \"What is the weather in karachi?\", run_config=config\n)\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/deterministic_pattern/","title":"Deterministic Pattern","text":"Source code in OpAgentsOlympus/practice/deterministic_pattern.py OpAgentsOlympus/practice/deterministic_pattern.py<pre><code>import asyncio\n\nfrom pydantic import BaseModel\nfrom config import config\nfrom agents import Agent, Runner\n\n\"\"\"\nThis example demonstrates a deterministic flow, where each step is performed by an agent.\n1. The first agent generates a story outline\n2. We feed the outline into the second agent\n3. The second agent checks if the outline is good quality and if it is a scifi story\n4. If the outline is not good quality or not a scifi story, we stop here\n5. If the outline is good quality and a scifi story, we feed the outline into the third agent\n6. The third agent writes the story\n\"\"\"\n\nstory_outline_agent = Agent(\n    name=\"story_outline_agent\",\n    instructions=\"Generate a very short story outline based on the user's input.\",\n)\n\n\nclass OutlineCheckerOutput(BaseModel):\n    good_quality: bool\n    is_scifi: bool\n\n\noutline_checker_agent = Agent(\n    name=\"outline_checker_agent\",\n    instructions=\"Read the given story outline, and judge the quality. Also, determine if it is a scifi story.\",\n    output_type=OutlineCheckerOutput,\n)\n\nstory_agent = Agent(\n    name=\"story_agent\",\n    instructions=\"Write a short story based on the given outline.\",\n    output_type=str,\n)\n\n\nasync def main():\n    input_prompt = input(\"What kind of story do you want? \")\n\n    # Ensure the entire workflow is a single trace\n    # with trace(\"Deterministic story flow\"):\n    # 1. Generate an outline\n    outline_result = await Runner.run(\n        story_outline_agent, input_prompt, run_config=config\n    )\n    print(\"Outline generated\")\n\n    # 2. Check the outline\n    outline_checker_result = await Runner.run(\n        outline_checker_agent, outline_result.final_output, run_config=config\n    )\n\n    # 3. Add a gate to stop if the outline is not good quality or not a scifi story\n    assert isinstance(outline_checker_result.final_output, OutlineCheckerOutput)\n    if not outline_checker_result.final_output.good_quality:\n        print(\"Outline is not good quality, so we stop here.\")\n        exit(0)\n\n    if not outline_checker_result.final_output.is_scifi:\n        print(\"Outline is not a scifi story, so we stop here.\")\n        exit(0)\n\n    print(\n        \"Outline is good quality and a scifi story, so we continue to write the story.\"\n    )\n\n    # 4. Write the story\n    story_result = await Runner.run(\n        story_agent, outline_result.final_output, run_config=config\n    )\n    print(f\"Story: {story_result.final_output}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/dynamic_prompt/","title":"Dynamic Prompt","text":"<p>The \"playground\" refers to OpenAI's web-based Prompt Playground at https://platform.openai.com/playground/prompts . This is a visual interface where you can create, test, and manage prompt templates that work with OpenAI's Responses API .</p>"},{"location":"OpAgentsOlympus/practice/dynamic_prompt/#what-is-the-playground-prompt-system","title":"What is the Playground Prompt System","text":"<p>The Playground allows you to create prompt templates with variables using double curly brace syntax like <code>{{variable_name}}</code> . In the example code, you would create a system prompt template containing \"Write a poem in {{poem_style}}\" where <code>poem_style</code> is a variable that gets replaced at runtime .</p> <p>Each prompt template gets assigned a unique ID (like <code>pmpt_686a4b884b708193b5e81a4ce03c707f0422d8b0bac332ce</code> in the example) that you reference in your code.</p>"},{"location":"OpAgentsOlympus/practice/dynamic_prompt/#benefits-and-use-cases","title":"Benefits and Use Cases","text":"<p>The prompt template system provides several key benefits:</p> <p>1. External Configuration: You can modify agent behavior without changing code. The prompt field allows you to \"dynamically configure the instructions, tools and other config for an agent outside of your code\".</p> <p>2. Variable Substitution: Templates support dynamic variables that get populated at runtime, enabling personalized or context-aware prompts .</p> <p>3. Version Control: The playground provides versioning for prompts, allowing you to iterate and test different versions .</p> <p>4. Team Collaboration: Non-technical team members can modify prompts without touching code .</p>"},{"location":"OpAgentsOlympus/practice/dynamic_prompt/#beyond-dynamic-prompts","title":"Beyond Dynamic Prompts","text":"<p>The prompt system does more than just provide dynamic text. When the agent runs, <code>get_prompt()</code> converts your prompt configuration using <code>PromptUtil.to_model_input()</code> configuration is then passed directly to OpenAI's Responses API.</p> <p>The prompt templates can potentially configure not just instructions but also \"tools and other config\" as mentioned in the documentation, though the specific extent of this configuration isn't detailed in the available code snippets.</p>"},{"location":"OpAgentsOlympus/practice/dynamic_prompt/#integration-pattern","title":"Integration Pattern","text":"<p>The system supports both static and dynamic prompt usage: - Static: Pass a dictionary with <code>id</code>, <code>version</code>, and <code>variables</code> directly to the agent - Dynamic: Use a <code>DynamicPromptFunction</code> that generates the prompt configuration at runtime based on context</p>"},{"location":"OpAgentsOlympus/practice/dynamic_prompt/#notes","title":"Notes","text":"<p>This prompt system is specifically designed for OpenAI's Responses API and won't work with the Chat Completions API. The playground-based approach represents a shift toward external prompt management, separating prompt engineering from application code.</p>"},{"location":"OpAgentsOlympus/practice/dynamic_system_prompt/","title":"Dynamic System Prompt","text":"Source code in OpAgentsOlympus/practice/dynamic_system_prompt.py OpAgentsOlympus/practice/dynamic_system_prompt.py<pre><code>import asyncio\nimport random\nfrom typing import Literal\nfrom config import config\nfrom agents import Agent, RunContextWrapper, Runner\n\n\nclass CustomContext:\n    def __init__(self, style: Literal[\"haiku\", \"pirate\", \"robot\"]):\n        self.style = style\n\n\ndef custom_instructions(\n    run_context: RunContextWrapper[CustomContext], agent: Agent[CustomContext]\n) -&gt; str:\n    context = run_context.context\n    if context.style == \"haiku\":\n        return \"Only respond in haikus.\"\n    elif context.style == \"pirate\":\n        return \"Respond as a pirate.\"\n    else:\n        return \"Respond as a robot and say 'beep boop' a lot.\"\n\n\nagent = Agent(\n    name=\"Chat agent\",\n    instructions=custom_instructions,\n)\n\n\nasync def main():\n    choice: Literal[\"haiku\", \"pirate\", \"robot\"] = random.choice(\n        [\"haiku\", \"pirate\", \"robot\"]\n    )\n    context = CustomContext(style=choice)\n    print(f\"Using style: {choice}\\n\")\n\n    user_message = \"Tell me a joke.\"\n    print(f\"User: {user_message}\")\n    result = await Runner.run(agent, user_message, context=context, run_config=config)\n\n    print(f\"Assistant: {result.final_output}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\"\"\"\n$ python examples/basic/dynamic_system_prompt.py\n\nUsing style: haiku\n\nUser: Tell me a joke.\nAssistant: Why don't eggs tell jokes?\nThey might crack each other's shells,\nleaving yolk on face.\n\n$ python examples/basic/dynamic_system_prompt.py\nUsing style: robot\n\nUser: Tell me a joke.\nAssistant: Beep boop! Why was the robot so bad at soccer? Beep boop... because it kept kicking up a debug! Beep boop!\n\n$ python examples/basic/dynamic_system_prompt.py\nUsing style: pirate\n\nUser: Tell me a joke.\nAssistant: Why did the pirate go to school?\n\nTo improve his arrr-ticulation! Har har har! \ud83c\udff4\u200d\u2620\ufe0f\n\"\"\"\n</code></pre>"},{"location":"OpAgentsOlympus/practice/forcing_tool_use/","title":"Forcing Tool Use","text":"Source code in OpAgentsOlympus/practice/forcing_tool_use.py OpAgentsOlympus/practice/forcing_tool_use.py<pre><code>from __future__ import annotations\n\nimport asyncio\nfrom typing import Any, Literal\n\nfrom pydantic import BaseModel\nfrom config import config\nfrom agents import (\n    Agent,\n    FunctionToolResult,\n    ModelSettings,\n    RunContextWrapper,\n    Runner,\n    ToolsToFinalOutputResult,\n    function_tool,\n)\n\n\"\"\"\nThis example shows how to force the agent to use a tool. It uses `ModelSettings(tool_choice=\"required\")`\nto force the agent to use any tool.\n\nYou can run it with 3 options:\n1. `default`: The default behavior, which is to send the tool output to the LLM. In this case,\n    `tool_choice` is not set, because otherwise it would result in an infinite loop - the LLM would\n    call the tool, the tool would run and send the results to the LLM, and that would repeat\n    (because the model is forced to use a tool every time.)\n2. `first_tool_result`: The first tool result is used as the final output.\n3. `custom`: A custom tool use behavior function is used. The custom function receives all the tool\n    results, and chooses to use the first tool result to generate the final output.\n\nUsage:\npython examples/agent_patterns/forcing_tool_use.py -t default\npython examples/agent_patterns/forcing_tool_use.py -t first_tool\npython examples/agent_patterns/forcing_tool_use.py -t custom\n\"\"\"\n\n\nclass Weather(BaseModel):\n    city: str\n    temperature_range: str\n    conditions: str\n\n\n@function_tool\ndef get_weather(city: str) -&gt; Weather:\n    print(\"[debug] get_weather called\")\n    return Weather(city=city, temperature_range=\"14-20C\", conditions=\"Sunny with wind\")\n\n\nasync def custom_tool_use_behavior(\n    context: RunContextWrapper[Any], results: list[FunctionToolResult]\n) -&gt; ToolsToFinalOutputResult:\n    weather: Weather = results[0].output\n    print(weather)\n    return ToolsToFinalOutputResult(\n        is_final_output=True, final_output=f\"{weather.city} is {weather.conditions}.\"\n    )\n\n\nasync def main(\n    tool_use_behavior: Literal[\"default\", \"first_tool\", \"custom\"] = \"default\",\n):\n    if tool_use_behavior == \"first_tool\":\n        behavior = \"stop_on_first_tool\"\n    elif tool_use_behavior == \"custom\":\n        behavior = custom_tool_use_behavior\n    else:\n        behavior = \"run_llm_again\"\n\n    agent = Agent(\n        name=\"Weather agent\",\n        instructions=\"You are a weather agent.\",\n        tools=[get_weather],\n        tool_use_behavior=behavior,\n        model_settings=ModelSettings(\n            tool_choice=\"required\" if tool_use_behavior != \"default\" else None\n        ),\n    )\n\n    result = await Runner.run(\n        agent, input=\"What's the weather in Tokyo?\", run_config=config\n    )\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-t\",\n        \"--tool-use-behavior\",\n        type=str,\n        required=True,\n        choices=[\"default\", \"first_tool\", \"custom\"],\n        help=\"The behavior to use for tool use. Default will cause tool outputs to be sent to the model. \"\n        \"first_tool_result will cause the first tool result to be used as the final output. \"\n        \"custom will use a custom tool use behavior function.\",\n    )\n    args = parser.parse_args()\n    asyncio.run(main(args.tool_use_behavior))\n</code></pre>"},{"location":"OpAgentsOlympus/practice/function_tool/","title":"Function Tool","text":"Source code in OpAgentsOlympus/practice/function_tool.py OpAgentsOlympus/practice/function_tool.py<pre><code>from pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    Runner,\n    InputGuardrail,\n    enable_verbose_stdout_logging,\n)\nfrom openai.types.responses import ResponseTextDeltaEvent\nimport asyncio\nfrom config import config\n\nenable_verbose_stdout_logging()\n# dotenv.load_dotenv()\n# set_tracing_disabled(True)\n# GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')\n\n# if not GEMINI_API_KEY:\n#     raise ValueError(\"API key not found!!\")\n\n# client = AsyncOpenAI(\n#     api_key=GEMINI_API_KEY,\n#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n# )\n# model = OpenAIChatCompletionsModel(model='gemini-2.0-flash', openai_client=client)\n# config = RunConfig(model=model, model_provider=client)\n\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n\nmath_tutor_agent = Agent(\n    name=\"Math Tutor\",\n    handoff_description=\"Specialist agent for math questions\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n)\n\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n)\n\n\nasync def homework_guardrail(ctx, agent, input_data):\n    result = await Runner.run(\n        guardrail_agent, input_data, context=ctx.context, run_config=config\n    )\n    final_output = result.final_output_as(HomeworkOutput)\n    print(final_output)\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=not final_output.is_homework,\n    )\n\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"You determine which agent to use based on the user's homework question\",\n    handoffs=[history_tutor_agent, math_tutor_agent],\n    input_guardrails=[\n        InputGuardrail(guardrail_function=homework_guardrail),\n    ],\n)\n\n\nasync def main():\n    # result =  Runner.run_streamed(triage_agent, \"What is 2 + 2?\", run_config=config)\n\n    result = Runner.run_streamed(\n        triage_agent,\n        \"Solve this homework question: What is 2 + 2 = _ \",\n        run_config=config,\n    )\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\" and isinstance(\n            event.data, ResponseTextDeltaEvent\n        ):\n            print(event.data.delta, end=\"\", flush=True)\n    # print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/gemini_with_streaming/","title":"Gemini With Streaming","text":"Source code in OpAgentsOlympus/practice/gemini_with_streaming.py OpAgentsOlympus/practice/gemini_with_streaming.py<pre><code>from agents import (\n    Runner,\n    Agent,\n    OpenAIChatCompletionsModel,\n    AsyncOpenAI,\n    function_tool,\n    RunConfig,\n    RunContextWrapper,\n)\nimport os\nfrom dotenv import load_dotenv\nimport asyncio\nfrom dataclasses import dataclass\n\nload_dotenv()\nGEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\nif not GEMINI_API_KEY:\n    raise ValueError(\"GEMINI_API_KEY is not set in the environment variables.\")\n\n# Set up the API provider\nprovider = AsyncOpenAI(\n    api_key=GEMINI_API_KEY,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\n# Define the AI model\nmodel = OpenAIChatCompletionsModel(model=\"gemini-1.5-flash\", openai_client=provider)\n\n# Configure the run\nconfig = RunConfig(model=model, model_provider=provider, tracing_disabled=True)\n\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    occupation: str\n\n\n@function_tool\ndef info(ctx: RunContextWrapper[Person]) -&gt; str:\n    \"\"\"\n    Get name and age about a person.\n    \"\"\"\n    # This is a placeholder implementation. Replace with actual information fetching logic.\n    return f\"{ctx.context.name} is {ctx.context.age} years old.\"\n\n\n@function_tool\ndef occupation(ctx: RunContextWrapper[Person]) -&gt; str:\n    \"\"\"\n    Get occupation of a person.\n    \"\"\"\n    return f\"{ctx.context.name} is a {ctx.context.occupation}.\"\n\n\nasync def main():\n    person = Person(name=\"Alice\", age=30, occupation=\"Software Engineer\")\n\n    agent = Agent(\n        name=\"info_agent\",\n        instructions=\"You are a helpful assistant that provides details about people using tools.\",\n        tools=[info, occupation],\n    )\n\n    result = await Runner.run(\n        agent, input=\"tell me about the age of  person\", context=person\n    )\n    print(\"\\nFinal Output:\", result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/guardrails_flow/","title":"Guardrails Flow","text":"Source code in OpAgentsOlympus/practice/guardrails_flow.py OpAgentsOlympus/practice/guardrails_flow.py<pre><code>from agents import Agent, InputGuardrail, OutputGuardrail, GuardrailFunctionOutput, Runner  \nimport asyncio  \nfrom open_router_config import config\n\nasync def agent_input_guardrail(ctx, agent, input_data):  \n    print(\"Agent input guardrail...\")  \n    return GuardrailFunctionOutput(  \n        output_info=\"agent_input_guardrail\",  \n        tripwire_triggered=False,\n    )  \n\nasync def agent_output_guardrail(ctx, agent, output):  \n    print(\"Agent output guardrail...\")  \n    return GuardrailFunctionOutput(  \n        output_info='agent_output_guardrail',  \n        tripwire_triggered=False,  \n    )  \n\nasync def runner_input_guardrail(ctx, agent, input_data):  \n    print(\"Runner input guardrail...\")  \n    return GuardrailFunctionOutput(  \n        output_info=\"runner_input_guardrail\",  \n        tripwire_triggered=False,  \n    )  \n\nasync def runner_output_guardrail(ctx, agent, output):  \n    print(\"Runner output guardrail...\")  \n    return GuardrailFunctionOutput(  \n        output_info=\"runner_output_guardrail\",  \n        tripwire_triggered=False,  \n    )  \n\nmath_agent = Agent(  \n    name=\"Math Tutor\",  \n    instructions=\"Help with math problems. Provide step-by-step solutions.\",  \n    input_guardrails=[InputGuardrail(guardrail_function=agent_input_guardrail)],  \n    output_guardrails=[OutputGuardrail(guardrail_function=agent_output_guardrail)],  \n)  \n\nhistory_agent = Agent(  \n    name=\"History Tutor\",   \n    instructions=\"Help with history questions. Provide detailed historical context.\",  \n)  \n\ntriage_agent = Agent(  \n    name=\"Triage Agent\",  \n    instructions=\"Determine which specialist agent to use based on the question.\",  \n    handoffs=[math_agent, history_agent],  \n    input_guardrails=[InputGuardrail(guardrail_function=agent_input_guardrail)],  \n    output_guardrails=[OutputGuardrail(guardrail_function=agent_output_guardrail)], \n)  \n\nasync def main():  \n    config.input_guardrails=[InputGuardrail(guardrail_function=runner_input_guardrail)]\n    config.output_guardrails=[OutputGuardrail(guardrail_function=runner_output_guardrail)]\n    await Runner.run(  \n        triage_agent,   \n        \"What is 2 + 2? Please explain step by step.\",  \n        run_config=config  \n    )  \n\nif __name__ == \"__main__\":  \n    asyncio.run(main())\n\n\n# &lt;== Output ==&gt;\n# Agent input guardrail...\n# Runner input guardrail...\n# Agent output guardrail...\n# Runner output guardrail...\n</code></pre>"},{"location":"OpAgentsOlympus/practice/handoff_and_agent_as_tool_question/","title":"Handoff And Agent As Tool Question","text":"Source code in OpAgentsOlympus/practice/handoff_and_agent_as_tool_question.py OpAgentsOlympus/practice/handoff_and_agent_as_tool_question.py<pre><code>from agents import Agent, Runner\nimport asyncio\nfrom open_router_config import model\n\nsay_hello  = Agent(name=\"say_hello\",  instructions=\"used to say hello.\", model=model)\n\nassistant = Agent(\n    name=\"assistant\",\n    instructions=(\n        \"You are a helpful assistant. You can call tools or handoff\"\n    ),\n    tools=[say_hello.as_tool(tool_name='say_hello', tool_description='used to say hello.')],\n    handoffs=[say_hello],\n    model=model\n)\n\nasync def main():\n    result = await Runner.run(assistant, \"Say hello to daniel!.\")\n    print(result.final_output)\n\nasyncio.run(main())\n\n# What will happen?\n</code></pre>"},{"location":"OpAgentsOlympus/practice/hooks_execution_count/","title":"Hooks Execution Count","text":"Source code in OpAgentsOlympus/practice/hooks_execution_count.py OpAgentsOlympus/practice/hooks_execution_count.py<pre><code>from agents import Agent, Runner, function_tool, AgentHooks, RunHooks, RunContextWrapper\nfrom typing import Any\nfrom config import config\nimport asyncio\n\nclass HookCounter:\n    def __init__(self):\n        self.counts = {\n            \"on_start\": 0,\n            \"on_end\": 0,\n            \"on_agent_start\": 0,\n            \"on_agent_end\": 0,\n            \"on_handoff\": 0,\n            \"on_tool_start\": 0,\n            \"on_tool_end\": 0,\n        }\n\n    def inc(self, key: str):\n        if key in self.counts:\n            self.counts[key] += 1\n\n    def report(self):\n        for k, v in self.counts.items():\n            print(f\"{k}_hook_count: {v}\")\n\nhook_counter = HookCounter()\n\nclass CustomAgentHooks(AgentHooks[Any]):\n    async def on_start(self, context: RunContextWrapper[Any], agent: Any) -&gt; None:\n        hook_counter.inc(\"on_start\")\n\n    async def on_end(self, context: RunContextWrapper[Any], agent: Any, output: Any) -&gt; None:\n        hook_counter.inc(\"on_end\")\n\n    async def on_handoff(self, context: RunContextWrapper[Any], agent: Any, source: Any) -&gt; None:\n        hook_counter.inc(\"on_handoff\")\n\n    async def on_tool_start(self, context: RunContextWrapper[Any], agent: Any, tool: Any) -&gt; None:\n        hook_counter.inc(\"on_tool_start\")\n\n    async def on_tool_end(self, context: RunContextWrapper[Any], agent: Any, tool: Any, result: str) -&gt; None:\n        hook_counter.inc(\"on_tool_end\")\n\nclass CustomRunHooks(RunHooks):\n    async def on_agent_start(self, context: RunContextWrapper[Any], agent: Any) -&gt; None:\n        hook_counter.inc(\"on_agent_start\")\n\n    async def on_agent_end(self, context: RunContextWrapper[Any], agent: Any, output: Any) -&gt; None:\n        hook_counter.inc(\"on_agent_end\")\n\n    async def on_handoff(self, context: RunContextWrapper[Any], from_agent: Any, to_agent: Any) -&gt; None:\n        hook_counter.inc(\"on_handoff\")\n\n    async def on_tool_start(self, context: RunContextWrapper[Any], agent: Any, tool: Any) -&gt; None:\n        hook_counter.inc(\"on_tool_start\")\n\n    async def on_tool_end(self, context: RunContextWrapper[Any], agent: Any, tool: Any, result: str) -&gt; None:\n        hook_counter.inc(\"on_tool_end\")\n\n@function_tool\ndef tool1():\n    \"\"\"MUST call me\"\"\"\n    print(\"tool1 was called!\")\n\n@function_tool\ndef tool2():\n    \"\"\"MUST call me\"\"\"\n    print(\"tool2 was called!\")\n\n@function_tool\ndef tool3():\n    \"\"\"MUST call me\"\"\"\n    print(\"tool3 was called!\")\n\n@function_tool\ndef tool4():\n    \"\"\"MUST call me\"\"\"\n    print(\"tool4 was called!\")\n\nagent_C = Agent(name=\"agent_C\", hooks=CustomAgentHooks())\nagent_B = Agent(\n    name=\"agent_B\",\n    instructions=\"MUST call both tools, MUST handoff to agent_C\",\n    tools=[tool3, tool4],\n    handoffs=[agent_C],\n    hooks=CustomAgentHooks()\n)\nagent_A = Agent(\n    name=\"agent_A\",\n    instructions=\"MUST call both tools, MUST handoff to agent_B\",\n    tools=[tool1, tool2],\n    handoffs=[agent_B],\n    hooks=CustomAgentHooks()\n)\n\nasync def main():\n    await Runner.run(\n        agent_A, input=\"Start the workflow.\", run_config=config, hooks=CustomRunHooks()\n    )\n    hook_counter.report()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# If there are three agents agent_A, agent_B and agent_C,\n# agent_A handoff to agent_B, agent_B handoff to agent_C,\n# Before each handoff two tools will be called and we have configured RunHooks as well &lt;= (means both, RunHooks + AgentHooks)\n# How many time each hook was called?\n\n# Make sure the LLm you are using is capable of executing all tools and handoffs!!\n# &lt;== OUTPUT ==&gt;\n# tool1 was called!\n# tool2 was called!\n# tool3 was called!\n# tool4 was called!\n# on_start_hook_count: 3\n# on_end_hook_count: 1\n# on_agent_start_hook_count: 3\n# on_agent_end_hook_count: 1\n# on_handoff_hook_count: 4\n# on_tool_start_hook_count: 8\n# on_tool_end_hook_count: 8\n</code></pre>"},{"location":"OpAgentsOlympus/practice/image_agent/","title":"Image Agent","text":"Source code in OpAgentsOlympus/practice/image_agent.py OpAgentsOlympus/practice/image_agent.py<pre><code>import asyncio\nimport base64\nimport os\nfrom config import config\nfrom agents import Agent, Runner\n\nFILEPATH = os.path.join(os.path.dirname(__file__), \"image_bison.png\")\n\n\ndef image_to_base64(image_path):\n    with open(image_path, \"rb\") as image_file:\n        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n    return encoded_string\n\n\nasync def main():\n    # Print base64-encoded image\n    b64_image = image_to_base64(FILEPATH)\n\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    result = await Runner.run(\n        agent,\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"input_image\",\n                        \"detail\": \"auto\",\n                        \"image_url\": f\"data:image/jpeg;base64,{b64_image}\",\n                    }\n                ],\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"What do you see in this image?\",\n            },\n        ],\n        run_config=config,\n    )\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/immutable_context_dataclasses/","title":"Immutable Context Dataclasses","text":"Source code in OpAgentsOlympus/practice/immutable_context_dataclasses.py OpAgentsOlympus/practice/immutable_context_dataclasses.py<pre><code>from dataclasses import dataclass  \nfrom agents import Agent, RunContextWrapper, Runner, function_tool  \nfrom config import config\nimport asyncio\n\n@dataclass(frozen=True)  # Makes the dataclass immutable  \nclass ImmutableUserInfo:  \n    name: str  \n    uid: int  \n    age: int = 47  \n\n@function_tool(failure_error_function=None) # Set failure_error_function=None so we can see the error.\nasync def get_user_age(wrapper: RunContextWrapper[ImmutableUserInfo]) -&gt; str:  \n    \"\"\"Get the user's age - cannot modify context.\"\"\"  \n    wrapper.context.name = \"John\" # This would raise an error\n    return f\"The user {wrapper.context.name} is {wrapper.context.age} years old\"  \n\nasync def main():  \n    user_info = ImmutableUserInfo(name=\"Daniel\", uid=123)\n\n    agent = Agent[ImmutableUserInfo](  \n        name=\"Assistant\",  \n        tools=[get_user_age],  \n    )  \n\n    result = await Runner.run(  \n        starting_agent=agent,  \n        input=\"What is the user's age?\",  \n        context=user_info,\n        run_config=config\n    )\n    print(result.final_output)\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/immutable_context_pydantic/","title":"Immutable Context Pydantic","text":"Source code in OpAgentsOlympus/practice/immutable_context_pydantic.py OpAgentsOlympus/practice/immutable_context_pydantic.py<pre><code>from pydantic import BaseModel, ConfigDict  \nfrom agents import Agent, RunContextWrapper, Runner, function_tool  \nfrom config import config\nimport asyncio\n\nclass ImmutableUserInfo(BaseModel):  \n    model_config = ConfigDict(frozen=True)  # Makes Pydantic model immutable  \n\n    name: str  \n    uid: int  \n    age: int = 60 # \ud83d\ude01\n    friends: tuple[str, ...] # Use tuple instead of list for immutability  \n\n@function_tool(failure_error_function=None) # Set failure_error_function=None so we can see the error.\nasync def get_user_age(wrapper: RunContextWrapper[ImmutableUserInfo]) -&gt; str:  \n    \"\"\"Get the user's age - cannot modify context.\"\"\"  \n    # wrapper.context.name = \"John\" # This would raise an error\n    # wrapper.context.friends.append('Ahmad Memon') # This will also raise an error because tuple has no attribute append \ud83e\udd37\u200d\u2642\ufe0f\n    return f\"The user {wrapper.context.name} is {wrapper.context.age} years old\"  \n\nasync def main():  \n    user_info = ImmutableUserInfo(name=\"Daniel\", uid=123, friends=['Junaid', 'Ali'])\n\n    agent = Agent[ImmutableUserInfo](  \n        name=\"Assistant\",  \n        tools=[get_user_age],  \n    )  \n\n    result = await Runner.run(  \n        starting_agent=agent,  \n        input=\"What is the user's age?\",  \n        context=user_info,\n        run_config=config\n    )\n    print(result.final_output)\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/input_filter_precedence/","title":"Input Filter Precedence","text":""},{"location":"OpAgentsOlympus/practice/input_filter_precedence/#input-filter-precedence","title":"Input Filter Precedence","text":"Python<pre><code>input_filter = handoff.input_filter or (\n    run_config.handoff_input_filter if run_config else None\n)\n</code></pre> <p>The precedence logic is implemented in the <code>execute_handoffs</code> method where it first checks for a handoff-specific <code>input_filter</code>, and only falls back to the global <code>run_config.handoff_input_filter</code> if no specific filter is defined</p> <p>This means: 1. If <code>handoff.input_filter</code> is set, it takes precedence and is used 2. If <code>handoff.input_filter</code> is <code>None</code>, then <code>run_config.handoff_input_filter</code> is used as a fallback 3. Only one filter is applied - they are not applied in parallel or combined</p>"},{"location":"OpAgentsOlympus/practice/input_filter_precedence/#notes","title":"Notes","text":"<p>The handoff input filtering system provides a mechanism to transform conversation state data before it's passed to the target agent during handoffs. The HandoffInputData structure is a frozen dataclass that encapsulates three distinct components of the conversation state:</p> <ul> <li> <p><code>input_history</code>: The original input provided to Runner.run() (either a string or tuple of input items)</p> </li> <li> <p><code>pre_handoff_items</code>: Items generated before the current agent turn, that triggered the handoff </p> </li> <li> <p><code>new_items</code>: Items generated during the current turn, including the handoff trigger and handoff output message</p> </li> </ul> <p>The filter function receives this complete HandoffInputData object and must return a modified HandoffInputData object</p> <p>The filtered data then replaces the original conversation state that gets passed to the target agent, The filtering happens during handoff execution in RunImpl.execute_handoffs().</p>"},{"location":"OpAgentsOlympus/practice/input_guardrail_trip/","title":"Input Guardrail Trip","text":"Source code in OpAgentsOlympus/practice/input_guardrail_trip.py OpAgentsOlympus/practice/input_guardrail_trip.py<pre><code>from agents import (\n    Agent,\n    Runner,\n    input_guardrail,\n    RunContextWrapper,\n    GuardrailFunctionOutput,\n)\nfrom pydantic import BaseModel\nfrom open_router_config import config\n# dotenv.load_dotenv()\n# gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\n# if not gemini_api_key:\n#     raise ValueError(\"GEMINI_API_KEY is not set. Please define it in your .env file.\")\n\n# external_client = AsyncOpenAI(\n#     api_key=gemini_api_key,\n#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n# )\n\n# # Preferred Gemini model setup\n# model = OpenAIChatCompletionsModel(\n#     model=\"gemini-2.0-flash\", openai_client=external_client\n# )\n\n# # Runner config (you can export this)\n# config = RunConfig(model=model, model_provider=external_client)\n\n\nclass CheckGuardrailOutput(BaseModel):\n    is_study_related: bool\n    student_input: str\n    reasoning: str\n\n\n@input_guardrail\nasync def check_prompt(ctx: RunContextWrapper, agent: Agent, input: str):\n    check_input_agent = Agent(\n        name=\"check_input_agent\",\n        instructions=\"You check the input of the student, If they are asking study related question or asking something that they should not ask.\",\n        output_type=CheckGuardrailOutput,\n    )\n\n    result = (\n        await Runner.run(\n            check_input_agent, f\"Student Input: {input}\", run_config=config\n        )\n    ).final_output_as(CheckGuardrailOutput)\n\n    return GuardrailFunctionOutput(\n        output_info=f\"Reasoning: {result.reasoning}, Student Input: {result.student_input}\",\n        tripwire_triggered=not result.is_study_related,\n    )\n\n\nteacher = Agent(\n    name=\"teacher\",\n    instructions=\"You are a helpful teacher\",\n    input_guardrails=[check_prompt],\n)\n\nresult = Runner.run_sync(\n    teacher, \"I want to change my class timings \ud83d\ude2d\ud83d\ude2d\", run_config=config\n)\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/input_guardrails/","title":"Input Guardrails","text":"Source code in OpAgentsOlympus/practice/input_guardrails.py OpAgentsOlympus/practice/input_guardrails.py<pre><code>from __future__ import annotations\n\nimport asyncio\n\nfrom pydantic import BaseModel\nfrom config import config\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    InputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    TResponseInputItem,\n    input_guardrail,\n)\n\n\"\"\"\nThis example shows how to use guardrails.\n\nGuardrails are checks that run in parallel to the agent's execution.\nThey can be used to do things like:\n- Check if input messages are off-topic\n- Check that input messages don't violate any policies\n- Take over control of the agent's execution if an unexpected input is detected\n\nIn this example, we'll setup an input guardrail that trips if the user is asking to do math homework.\nIf the guardrail trips, we'll respond with a refusal message.\n\"\"\"\n\n\n### 1. An agent-based guardrail that is triggered if the user is asking to do math homework\nclass MathHomeworkOutput(BaseModel):\n    reasoning: str\n    is_math_homework: bool\n\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking you to do their math homework.\",\n    output_type=MathHomeworkOutput,\n)\n\n\n@input_guardrail\nasync def math_guardrail(\n    context: RunContextWrapper[None],\n    agent: Agent,\n    input: str | list[TResponseInputItem],\n) -&gt; GuardrailFunctionOutput:\n    \"\"\"This is an input guardrail function, which happens to call an agent to check if the input\n    is a math homework question.\n    \"\"\"\n    result = await Runner.run(\n        guardrail_agent, input, context=context.context, run_config=config\n    )\n    final_output = result.final_output_as(MathHomeworkOutput)\n\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=final_output.is_math_homework,\n    )\n\n\n### 2. The run loop\n\n\nasync def main():\n    agent = Agent(\n        name=\"Customer support agent\",\n        instructions=\"You are a customer support agent. You help customers with their questions.\",\n        input_guardrails=[math_guardrail],\n    )\n\n    input_data: list[TResponseInputItem] = []\n\n    while True:\n        user_input = input(\"Enter a message: \")\n        input_data.append(\n            {\n                \"role\": \"user\",\n                \"content\": user_input,\n            }\n        )\n\n        try:\n            result = await Runner.run(agent, input_data, run_config=config)\n            print(result.final_output)\n            # If the guardrail didn't trigger, we use the result as the input for the next run\n            input_data = result.to_input_list()\n        except InputGuardrailTripwireTriggered:\n            # If the guardrail triggered, we instead add a refusal message to the input\n            message = \"Sorry, I can't help you with your math homework.\"\n            print(message)\n            input_data.append(\n                {\n                    \"role\": \"assistant\",\n                    \"content\": message,\n                }\n            )\n\n    # Sample run:\n    # Enter a message: What's the capital of California?\n    # The capital of California is Sacramento.\n    # Enter a message: Can you help me solve for x: 2x + 5 = 11\n    # Sorry, I can't help you with your math homework.\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/is_enabled_real_world_example/","title":"Is Enabled Real World Example","text":"Source code in OpAgentsOlympus/practice/is_enabled_real_world_example.py OpAgentsOlympus/practice/is_enabled_real_world_example.py<pre><code>from agents import (\n    Agent,\n    Runner,\n    function_tool,\n    StopAtTools,\n)\nimport asyncio\nfrom local_config import config\n\n@function_tool(is_enabled=lambda ctx, agent: True if ctx.context['role'] == 'admin' else False)\ndef delete_user(user_id: str) -&gt; str:\n    \"\"\"Deletes a user. This is a final action.\"\"\"\n    return f\"User {user_id} has been deleted.\"\n\nadmin_agent = Agent(\n    name=\"Admin Agent\",\n    instructions=\"Help manage users. First get data, then delete if asked.\",\n    tools=[delete_user],\n    tool_use_behavior=StopAtTools(stop_at_tool_names=[\"delete_user\"]),\n)\n\nasync def main():\n    print(\"--- Running as a regular user ---\")\n    result_user = await Runner.run(\n        admin_agent, \"Please delete user user123.\",\n        context={\"role\": \"user\"},\n        run_config=config\n    )\n    print(f\"Final Output: {result_user.final_output}\")\n\n    print(\"\\n--- Running as an admin ---\")\n    result_admin = await Runner.run(\n        admin_agent,\n        \"Please delete user user123.\",\n        context={\"role\": \"admin\"},\n        run_config=config\n    )\n    print(f\"Final Output: {result_admin.final_output}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/is_run_context_wrapper_just_a_type_hint/","title":"Is Run Context Wrapper Just A Type Hint","text":"Source code in OpAgentsOlympus/practice/is_run_context_wrapper_just_a_type_hint.py OpAgentsOlympus/practice/is_run_context_wrapper_just_a_type_hint.py<pre><code>from agents import Agent, Runner, OpenAIChatCompletionsModel, RunConfig, function_tool, RunContextWrapper, AgentBase\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom typing import Any\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\nAPI_KEY = os.environ.get(\"GEMINI_API_KEY\")\n\nclient = AsyncOpenAI(\n    api_key=API_KEY,\n    base_url='https://generativelanguage.googleapis.com/v1beta/openai'\n)\n\nmodel = OpenAIChatCompletionsModel(\n    model='gemini-1.5-flash',\n    openai_client=client\n)\n\nconfig = RunConfig(model)\n\nclass UserContext(BaseModel):\n    is_user_admin: bool\n\ndef is_user_admin(ctx, agent: AgentBase[Any]): # Specifying type hints for context is only required for tools not for other functions (e.g dynamic instructions, is_enabled, hooks, guardrails etc..)\n    return True if ctx.context.is_user_admin else False\n\n@function_tool(is_enabled=is_user_admin, failure_error_function=None)\ndef weather_tool(ctx: RunContextWrapper[UserContext], city: str) -&gt; str: # Specifying type hint for the context is must for tools, because the SDK performs runtime type hint interception to detect these context types and excludes them from the JSON schema sent to the LLM, RunContextWrapper must be the first parameter.\n    \"\"\"Used to get weather in a city\n    city: str\n    \"\"\"\n    return f\"The weather in {city} is sunny.\" if ctx.context.is_user_admin else 'Weather not found!'\n\nassistant = Agent(\n    name='assistant',\n    instructions='You are a helpful assistant. use `weather_tool` tool to get weather of a city.',\n    tools=[weather_tool]\n)\nuser_data = UserContext(is_user_admin=True)\nresult =  Runner.run_sync(\n    assistant,\n    'What is weather in karachi?',\n    run_config=config,\n    context=user_data\n)\n\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/langsmith_tracing/","title":"Langsmith Tracing","text":"Source code in OpAgentsOlympus/practice/langsmith_tracing.py OpAgentsOlympus/practice/langsmith_tracing.py<pre><code>import asyncio\nfrom agents import Agent, Runner, function_tool\nfrom langsmith import traceable\nfrom open_router_config import config\n\n\n@traceable\nasync def traced_agent_run(agent, question, config):\n    return await Runner.run(agent, question, run_config=config)\n\n\n@function_tool\ndef get_weather(city: str):\n    return f\"The weather in {city} is sunny\"\n\n\nasync def main():\n    agent = Agent(\n        name=\"Assistant\", instructions=\"You are helpful Assistant.\", tools=[get_weather]\n    )\n\n    result = await traced_agent_run(agent, \"What is the weather in karachi?\", config)\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/litellm_config/","title":"Litellm Config","text":"Source code in OpAgentsOlympus/practice/litellm_config.py OpAgentsOlympus/practice/litellm_config.py<pre><code>import os\n\ntry:\n    from dotenv import load_dotenv, find_dotenv\n    from agents.run import RunConfig\n    from agents.extensions.models.litellm_model import LitellmModel\nexcept ImportError:\n    raise ImportError(\n        \"\\nThis package requires 'openai-agents' to be installed.\\n\"\n        \"\\nPlease install it first using pip:\\n\"\n        \"\\npip install openai-agents\\n\"\n        \"\\nFor more information, visit: https://openai.github.io/openai-agents-PyDeepOlympus/quickstart/\\n\"\n    )\n\n# Load environment variables\nload_dotenv(find_dotenv())\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"GEMINI_API_KEY is not set. Please define it in your .env file.\")\n\nmodel = LitellmModel(model=\"gemini/gemini-2.0-flash\", api_key=gemini_api_key)\n\n# Runner config (you can export this)\nconfig = RunConfig(model=model, tracing_disabled=True)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/llm_as_judge_pattern/","title":"Llm As Judge Pattern","text":"Source code in OpAgentsOlympus/practice/llm_as_judge_pattern.py OpAgentsOlympus/practice/llm_as_judge_pattern.py<pre><code>from __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Literal\nfrom rich.markdown import Markdown\nfrom rich.console import Console\nfrom agents import Agent, ItemHelpers, Runner, TResponseInputItem\nfrom config import config\n\n\"\"\"\nThis example shows the LLM as a judge pattern. The first agent generates an outline for a story.\nThe second agent judges the outline and provides feedback. We loop until the judge is satisfied\nwith the outline.\n\"\"\"\n\nconsole = Console()\n\nstory_outline_generator = Agent(\n    name=\"story_outline_generator\",\n    instructions=(\n        \"You generate a very short story outline based on the user's input.\"\n        \"If there is any feedback provided, use it to improve the outline.\"\n    ),\n)\n\n\n@dataclass\nclass EvaluationFeedback:\n    feedback: str\n    score: Literal[\"pass\", \"needs_improvement\", \"fail\"]\n\n\nevaluator = Agent[None](\n    name=\"evaluator\",\n    instructions=(\n        \"You evaluate a story outline and decide if it's good enough.\"\n        \"If it's not good enough, you provide feedback on what needs to be improved.\"\n        \"Never give it a pass on the first try.\"\n    ),\n    output_type=EvaluationFeedback,\n)\n\n\nasync def main() -&gt; None:\n    msg = input(\"What kind of story would you like to hear? \")\n    input_items: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n\n    latest_outline: str | None = None\n\n    # We'll run the entire workflow in a single trace\n    # with trace(\"LLM as a judge\"):\n    while True:\n        story_outline_result = await Runner.run(\n            story_outline_generator, input_items, run_config=config\n        )\n\n        input_items = story_outline_result.to_input_list()\n        latest_outline = ItemHelpers.text_message_outputs(\n            story_outline_result.new_items\n        )\n\n        console.print(Markdown(\"## Story outline generated\\n\\n\"))\n        console.print(Markdown(f\"## Outline: {story_outline_result.final_output}\\n\\n\"))\n\n        evaluator_result = await Runner.run(evaluator, input_items, run_config=config)\n        result: EvaluationFeedback = evaluator_result.final_output\n\n        console.print(Markdown(f\"### Evaluator score: {result.score}\"))\n        console.print(Markdown(f\"### Evaluator feedback: {result.feedback}\"))\n\n        if result.score == \"pass\":\n            console.print(Markdown(\"Story outline is good enough, exiting.\"))\n            break\n\n        console.print(Markdown(\"### Re-running with feedback\\n\\n\"))\n\n        input_items.append({\"content\": f\"Feedback: {result.feedback}\", \"role\": \"user\"})\n\n    console.print(Markdown(f\"## Final story outline: {latest_outline}\"))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/local_config/","title":"Local Config","text":"Source code in OpAgentsOlympus/practice/local_config.py OpAgentsOlympus/practice/local_config.py<pre><code>from agents import AsyncOpenAI, OpenAIChatCompletionsModel\nfrom agents.run import RunConfig\n\nexternal_client = AsyncOpenAI(\n    api_key='1234',\n    base_url=\"http://localhost:8000/v1\",\n)\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemma-3b\", openai_client=external_client\n)\nconfig = RunConfig(model=model, model_provider=external_client, tracing_disabled=True)\n\n# This configuration is for ollama!\n</code></pre>"},{"location":"OpAgentsOlympus/practice/main/","title":"Main","text":"Source code in OpAgentsOlympus/practice/main.py OpAgentsOlympus/practice/main.py<pre><code>from agents import (\n    Agent,\n    Runner,\n    handoff,\n    enable_verbose_stdout_logging,\n    OpenAIChatCompletionsModel,\n    AsyncOpenAI,\n    set_tracing_disabled,\n    function_tool,\n)  # type: ignore\nfrom agents.handoffs import HandoffInputData\nimport os\nimport dotenv\nimport asyncio\n\ndotenv.load_dotenv()\nset_tracing_disabled(True)\napi_key = os.environ.get(\"GEMINI_API_KEY\")\n\nclient = AsyncOpenAI(\n    api_key=api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\nmodel = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client)\nenable_verbose_stdout_logging()\n\n\n# Define some example function tools\n@function_tool\ndef tool_1():\n    \"\"\"First parallel tool\"\"\"\n    return \"Result from tool 1\"\n\n\n@function_tool\ndef tool_2():\n    \"\"\"Second parallel tool\"\"\"\n    return \"Result from tool 2\"\n\n\n@function_tool\ndef tool_3():\n    \"\"\"Third parallel tool\"\"\"\n    return \"Result from tool 3\"\n\n\n# Custom input filter that modifies pre_handoff_items and prints debug info\ndef custom_input_filter(handoff_input_data: HandoffInputData) -&gt; HandoffInputData:\n    print(\"=== BEFORE FILTERING ===\")\n    print(\n        f\"Input history length: {len(handoff_input_data.input_history) if isinstance(handoff_input_data.input_history, tuple) else 'string'}\"\n    )\n    print(f\"Pre-handoff items count: {len(handoff_input_data.pre_handoff_items)}\")\n    print(f\"New items count: {len(handoff_input_data.new_items)}\")\n\n    # Clear pre_handoff_items to demonstrate filtering\n    filtered_data = HandoffInputData(\n        input_history=handoff_input_data.input_history,\n        pre_handoff_items=(),  # Clear pre_handoff_items\n        new_items=handoff_input_data.new_items,\n    )\n\n    print(\"=== AFTER FILTERING ===\")\n    print(\n        f\"Input history length: {len(filtered_data.input_history) if isinstance(filtered_data.input_history, tuple) else 'string'}\"\n    )\n    print(f\"Pre-handoff items count: {len(filtered_data.pre_handoff_items)}\")\n    print(f\"New items count: {len(filtered_data.new_items)}\")\n\n    return filtered_data\n\n\n# Agent B with debug printing\nclass DebugAgent(Agent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    async def _on_agent_start(self, context):\n        print(\"=== AGENT B RECEIVED INPUT ===\")\n        # This would show what Agent B actually receives after filtering\n        print(\"Agent B starting with context\")\n\n\n# Agent B (target of handoff)\nagent_b = DebugAgent(\n    name=\"Agent B\",\n    instructions=\"You are Agent B, handling tasks after handoff.\",\n    model=model,\n)\n\n# Agent A (source agent with tools and handoff)\nagent_a = Agent(\n    name=\"Agent A\",\n    instructions=\"You are Agent A. Use your tools and then handoff to Agent B.\",\n    tools=[tool_1, tool_2, tool_3],\n    model=model,\n    handoffs=[\n        handoff(\n            agent=agent_b,\n            input_filter=custom_input_filter,  # This modifies pre_handoff_items\n        )\n    ],\n)\n\n\n# Run the scenario\nasync def main():\n    result = await Runner.run(\n        agent_a, input=\"Please use all your tools and then handoff to Agent B\"\n    )\n    print(f\"Final result: {result.final_output}\")\n    print(f\"Final agent: {result.last_agent.name}\")\n    print(result)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/max_tokens/","title":"Max Tokens","text":"Source code in OpAgentsOlympus/practice/max_tokens.py OpAgentsOlympus/practice/max_tokens.py<pre><code>from openai import AsyncOpenAI\nfrom agents import (\n    Agent,\n    Runner,\n    OpenAIChatCompletionsModel,\n    RunConfig,\n    ModelSettings\n)\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\n        \"I guess you haven't set API KEY, I'am pretty sure you need to set it dude.\")\n\nclient = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-2.0-flash\", openai_client=client\n)\n\nconfig = RunConfig(model=model)\n\nagent = Agent[None](\n    name=\"assistant\",\n    instructions=\"You are an amazing assistant, You only respond in haikus LOL MUST CALL A TOOL\",\n    model_settings=ModelSettings(\n        max_tokens=10\n    )\n)\n\nresult = Runner.run_sync(\n    agent,\n    \"Am I a CODER?\",\n    run_config=config,\n)\n\nprint(result.final_output)\n\n# max_tokens=1\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python max_tokens.py\n# A\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python max_tokens.py\n# Skills\n\n# max_tokens=10\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python max_tokens.py\n# Query code skill,\n# Tool will check with swift command,\n\n# This behavior is NEVER deterministic so you NEVER know how will be the output.\n# Only LLM calls consume tokens. However, tokens are also consumed by other features included in LLM requests or responses, such as tool schemas or tool calls made by the LLM.\n\n# More about token consumption: https://github.com/DanielHashmi/PyEpicOdyssey/blob/main/OpAgentsOlympus/OpAgentsTokenConsumption.md\n</code></pre>"},{"location":"OpAgentsOlympus/practice/max_turn_example/","title":"Max Turn Example","text":"Source code in OpAgentsOlympus/practice/max_turn_example.py OpAgentsOlympus/practice/max_turn_example.py<pre><code>import asyncio\n\nfrom agents import Agent, ItemHelpers, Runner, function_tool\nfrom open_router_config import config\n\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    return f\"Weather in {city} is sunny.\"\n\n\nassistant = Agent(\n    name=\"assistant\",\n    instructions=(\"You are a helpful assistant.\"),\n    tools=[get_weather],\n    tool_use_behavior=\"stop_on_first_tool\",\n)\n\n\nasync def main():\n    msg = \"What is the weather in karachi?\"\n\n    result = Runner.run_streamed(assistant, msg, run_config=config, max_turns=1)\n\n    async for event in result.stream_events():\n        # We'll ignore the raw responses event deltas\n        if event.type == \"raw_response_event\":\n            continue\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n            continue\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(\n                    f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\"\n                )\n            else:\n                pass  # Ignore other event types\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/mess_with_model_settings_and_tools/","title":"Mess With Model Settings And Tools","text":"Source code in OpAgentsOlympus/practice/mess_with_model_settings_and_tools.py OpAgentsOlympus/practice/mess_with_model_settings_and_tools.py<pre><code>from open_router_config import config\nfrom agents import Agent, Runner, function_tool, ModelSettings, RunContextWrapper, StopAtTools\n\n@function_tool\ndef say_hello() -&gt; str:\n    return 'Hello, Guys!'\n\n@function_tool\ndef say_bye(ctx: RunContextWrapper) -&gt; str:\n    return 'Bye, Guys!'\n\ndef main():\n    assistant = Agent(\n        name=\"assistant\",\n        instructions='You are a helpful assistant.',\n        model_settings=ModelSettings(\n            tool_choice='say_bye',\n            parallel_tool_calls=False,\n        ),\n        reset_tool_choice=False,\n        tool_use_behavior=StopAtTools(stop_at_tool_names=['say_hello']),\n        tools=[say_hello, say_bye]\n    )\n    result = Runner.run_sync(\n        assistant, input='say bye first then hello', run_config=config\n    )\n    print(result.final_output)\n\nif __name__ == \"__main__\":\n    main()\n\n\n# &lt;-- Another Example --&gt;\n# assistant = Agent(\n#     name=\"assistant\",\n#     instructions='You are a helpful assistant.',\n#     model_settings=ModelSettings(\n#         parallel_tool_calls=True,\n#     ),\n#     tool_use_behavior='stop_on_first_tool',\n#     tools=[say_hello, say_bye]\n# )\n# result = Runner.run_sync(\n#     assistant, input='say bye first then hello', run_config=config\n# )\n\n# Question: Why the result of say_bye will be the final_output?\n</code></pre>"},{"location":"OpAgentsOlympus/practice/model_settings/","title":"Model Settings","text":"Source code in OpAgentsOlympus/practice/model_settings.py OpAgentsOlympus/practice/model_settings.py<pre><code>from agents import (\n    Agent,\n    Runner,\n    AsyncOpenAI,\n    OpenAIChatCompletionsModel,\n    RunConfig,\n    ModelSettings\n)\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"GEMINI_API_KEY is not set. Please define it in your .env file.\")\n\nexternal_client = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-2.0-flash\", openai_client=external_client\n)\n\nconfig = RunConfig(model=model, model_provider=external_client)\n\nagent = Agent[None](\n    name=\"assistant\",\n    instructions=\"You are an amazing assistant, You only respond in haikus\",\n    model_settings=ModelSettings(\n        temperature=0.9\n    )\n)\n\nresult = Runner.run_sync(\n    agent,\n    \"What is an apple?\",\n    run_config=config,\n)\n\nprint(result.final_output)\n\n# Temperature = 0.1\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python model_settings.py\n# A fruit, round and red,\n# Grows upon an apple tree,\n# Sweet and crisp to bite.\n\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python model_settings.py\n# A fruit, red and round,\n# Grows upon an apple tree,\n# Sweet and crisp to bite.\n\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python model_settings.py\n# A fruit, red and round,\n# Grows upon an apple tree,\n# Sweet and crisp to bite.\n\n\n# Temperature = 0.9\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python model_settings.py\n# A fruit, round and red,\n# Or green, a sweet, crisp delight,\n# From the apple tree.\n\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python model_settings.py\n# A fruit, red or green,\n# Grows upon an apple tree,\n# Sweet taste, good to eat.\n\n# PyEpicOdyssey\\OpAgentsOlympus\\Practice&gt; python model_settings.py\n# A fruit, red and sweet,\n# Grows upon an apple tree,\n# A healthy snack too.\n\n# Min/Max (0.0 - 2.0)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/multi_tools_agent/","title":"Multi Tools Agent","text":"Source code in OpAgentsOlympus/practice/multi_tools_agent.py OpAgentsOlympus/practice/multi_tools_agent.py<pre><code>from agents import (\n    Agent,\n    Runner,\n    RunContextWrapper,\n    AsyncOpenAI,\n    OpenAIChatCompletionsModel,\n    RunConfig,\n    function_tool,\n)\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"GEMINI_API_KEY is not set. Please define it in your .env file.\")\n\nexternal_client = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\n# Preferred Gemini model setup\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-2.0-flash\", openai_client=external_client\n)\n\n# Runner config (you can export this)\nconfig = RunConfig(model=model, model_provider=external_client)\n\n\n# Tool 1: Get current location\n@function_tool\ndef get_current_location(ctx: RunContextWrapper[None]) -&gt; str:\n    \"\"\"Returns the user's current location.\"\"\"\n    # Dummy location for demonstration\n    return \"New York, USA\"\n\n\n# Tool 2: Get breaking news\n@function_tool\ndef get_breaking_news(ctx: RunContextWrapper[None]) -&gt; list[str]:\n    \"\"\"Returns a list of breaking news headlines.\"\"\"\n    return [\n        \"Global markets rally amid economic optimism.\",\n        \"Major breakthrough in renewable energy announced.\",\n    ]\n\n\n# Tool 3: Explain photosynthesis\n@function_tool\ndef explain_photosynthesis(ctx: RunContextWrapper[None]) -&gt; str:\n    \"\"\"Explains the process of photosynthesis.\"\"\"\n    return \"Photosynthesis is the process by which green plants use sunlight to synthesize foods from carbon dioxide and water.\"\n\n\nagent = Agent[None](\n    name=\"multi_query_agent\",\n    instructions=\"Answer each query using the appropriate tools. MUST call TOOLS\",\n    tools=[get_current_location, get_breaking_news, explain_photosynthesis],\n)\n\nconfig.tracing_disabled = False  # Make sure tracing is not disabled\n\nresult = Runner.run_sync(\n    agent,\n    \"\"\"\n    1. What is my current location?\n    2. Any breaking news?\n    3. What is photosynthesis\n    \"\"\",\n    run_config=config,\n)\n\nprint(\"=\" * 50)\nprint(\"Result: \", result.last_agent.name)\n# print(result.new_items)\nprint(\"Result: \", result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/non_strict_output/","title":"Non Strict Output","text":"Source code in OpAgentsOlympus/practice/non_strict_output.py OpAgentsOlympus/practice/non_strict_output.py<pre><code>import asyncio\nimport json\nfrom dataclasses import dataclass\nfrom typing import Any\nfrom config import config\nfrom agents import Agent, AgentOutputSchema, AgentOutputSchemaBase, Runner\n\n\"\"\"This example demonstrates how to use an output type that is not in strict mode. Strict mode\nallows us to guarantee valid JSON output, but some schemas are not strict-compatible.\n\nIn this example, we define an output type that is not strict-compatible, and then we run the\nagent with strict_json_schema=False.\n\nWe also demonstrate a custom output type.\n\nTo understand which schemas are strict-compatible, see:\nhttps://platform.openai.com/docs/guides/structured-outputs?api-mode=responses#supported-schemas\n\"\"\"\n\n\n@dataclass\nclass OutputType:\n    jokes: dict[str, str]\n    \"\"\"A list of jokes, indexed by joke number.\"\"\"\n\n\nclass CustomOutputSchema(AgentOutputSchemaBase):\n    \"\"\"A demonstration of a custom output schema.\"\"\"\n\n    def is_plain_text(self) -&gt; bool:\n        return False\n\n    def name(self) -&gt; str:\n        return \"CustomOutputSchema\"\n\n    def json_schema(self) -&gt; dict[str, Any]:\n        return {\n            \"type\": \"object\",\n            \"properties\": {\n                \"jokes\": {\"type\": \"object\", \"properties\": {\"joke\": {\"type\": \"string\"}}}\n            },\n        }\n\n    def is_strict_json_schema(self) -&gt; bool:\n        return False\n\n    def validate_json(self, json_str: str) -&gt; Any:\n        json_obj = json.loads(json_str)\n        # Just for demonstration, we'll return a list.\n        return list(json_obj[\"jokes\"].values())\n\n\nasync def main():\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful assistant.\",\n        output_type=OutputType,\n    )\n\n    input = \"Tell me 3 short jokes.\"\n\n    # First, let's try with a strict output type. This should raise an exception.\n    # try:\n    #     result = await Runner.run(agent, input, run_config=config)\n    #     raise AssertionError(\"Should have raised an exception\")\n    # except Exception as e:\n    #     print(f\"Error (expected): {e}\")\n\n    # Now let's try again with a non-strict output type. This should work.\n    # In some cases, it will raise an error - the schema isn't strict, so the model may\n    # produce an invalid JSON object.\n    agent.output_type = AgentOutputSchema(OutputType, strict_json_schema=True)\n    result = await Runner.run(agent, input, run_config=config)\n    print(result.final_output)\n\n    # # Finally, let's try a custom output type.\n    agent.output_type = CustomOutputSchema()\n    result = await Runner.run(agent, input, run_config=config)\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/open_router_config/","title":"Open Router Config","text":"Source code in OpAgentsOlympus/practice/open_router_config.py OpAgentsOlympus/practice/open_router_config.py<pre><code>import os\n\ntry:\n    from dotenv import load_dotenv, find_dotenv\n    from agents import AsyncOpenAI, OpenAIChatCompletionsModel\n    from agents.run import RunConfig\nexcept ImportError:\n    raise ImportError(\n        \"\\nThis package requires 'openai-agents' to be installed.\\n\"\n        \"\\nPlease install it first using pip:\\n\"\n        \"\\npip install openai-agents\\n\"\n        \"\\nFor more information, visit: https://openai.github.io/openai-agents-PyDeepOlympus/quickstart/\\n\"\n    )\n\n# Load environment variables\nload_dotenv(find_dotenv())\n\nAPI_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\nBASE_URL = \"https://openrouter.ai/api/v1\"\nMODEL = \"openai/gpt-4o-mini\"\n\nmodel = OpenAIChatCompletionsModel(\n    model=MODEL, openai_client=AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)\n)\n# Runner config (you can export this)\nconfig = RunConfig(model=model)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/","title":"OpenAI Agents SDK Cheat Sheet","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#1-introduction-to-openai-agents-sdk","title":"1. Introduction to OpenAI Agents SDK","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#what-is-the-openai-agents-sdk","title":"What is the OpenAI Agents SDK?","text":"<p>The OpenAI Agents SDK is an open-source Python library designed to simplify the development of agentic applications powered by OpenAI's LLMs. It offers a modular, composable set of tools and primitives to create deterministic flows, iterative loops, and multi-agent systems with minimal abstractions.</p>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#key-features-and-benefits","title":"Key Features and Benefits","text":"<ul> <li>Lightweight: Minimal overhead for rapid development.</li> <li>Flexible: Supports custom workflows, tools, and agent handoffs.</li> <li>Powerful: Leverages OpenAI's state-of-the-art LLMs.</li> <li>Structured Outputs: Integrates with Pydantic for typed responses.</li> <li>Metrics Tracking: Built-in usage monitoring for optimization.</li> </ul>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#2-core-components","title":"2. Core Components","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#agents","title":"Agents","text":"<p>Agents are the central entities in the SDK, representing an LLM configured with: - Name: A unique identifier. - Instructions: A string defining the agent's behavior and purpose. - Model: The underlying LLM (e.g., <code>gpt-4o</code>, <code>gpt-3.5-turbo</code>). - Tools: Functions or services the agent can utilize.</p>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#tools","title":"Tools","text":"<p>Tools extend agent capabilities by allowing them to interact with external systems or perform specific tasks: - Custom Tools: Defined using the <code>@function_tool</code> decorator. - Hosted Tools: Pre-built tools like <code>FileSearchTool</code>, <code>WebSearchTool</code>, and <code>ComputerTool</code>.</p>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#runners","title":"Runners","text":"<p>Runners manage the execution of agents, handling input processing and output generation: - Asynchronous: For non-blocking execution. - Synchronous: For simpler, blocking workflows. - Streaming: For real-time event handling.</p>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#guardrails","title":"Guardrails","text":"<p>Guardrails ensure data integrity and enforce constraints: - Defined with the <code>@guardrail</code> decorator. - Validate inputs and outputs (e.g., length, format, content).</p>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#handoffs","title":"Handoffs","text":"<p>Handoffs enable multi-agent collaboration by allowing one agent to delegate tasks to another, creating complex workflows.</p>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#3-setting-up-the-sdk","title":"3. Setting Up the SDK","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>OpenAI API key (available from platform.openai.com)</li> </ul>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#installation","title":"Installation","text":"<p>Install the SDK via pip:</p> Bash<pre><code>pip install openai-agents\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#configuration","title":"Configuration","text":"<p>Set your API key in the environment:</p> Python<pre><code>import os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"\n</code></pre> <p>Optionally, configure a custom API base URL:</p> Python<pre><code>os.environ[\"OPENAI_API_BASE\"] = \"https://api.openai.com/v1\"\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#4-building-agents","title":"4. Building Agents","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#defining-an-agent","title":"Defining an Agent","text":"<p>Create an agent with a name, instructions, model, and optional tools:</p> Python<pre><code>from agents import Agent\n\nagent = Agent(\n    name=\"CustomerSupportAgent\",\n    instructions=\"You are a polite and knowledgeable support assistant.\",\n    model=\"gpt-4o\",\n    tools=[],\n)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#configuring-instructions","title":"Configuring Instructions","text":"<p>Instructions should be clear and specific to shape the agent's behavior:</p> Python<pre><code>instructions = \"\"\"\nYou are an expert in Python programming.\nProvide concise, accurate answers and include code examples when possible.\n\"\"\"\nagent = Agent(name=\"PythonExpert\", instructions=instructions, model=\"gpt-4o\")\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#selecting-models","title":"Selecting Models","text":"<p>Supported models include: - <code>gpt-4o</code>: Latest high-performance model. - <code>gpt-3.5-turbo</code>: Cost-effective and fast. - Custom fine-tuned models (if available).</p>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#adding-tools","title":"Adding Tools","text":"<p>Attach tools to an agent for enhanced functionality:</p> Python<pre><code>agent = Agent(\n    name=\"MathAgent\",\n    instructions=\"Solve math problems.\",\n    model=\"gpt-4o\",\n    tools=[add_numbers],  # Defined below\n)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#5-tools-and-decorators","title":"5. Tools and Decorators","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#function_tool","title":"@function_tool","text":"<p>Convert a Python function into an agent-callable tool:</p> Python<pre><code>from agents import function_tool\n\n@function_tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiplies two integers.\"\"\"\n    return a * b\n</code></pre> <ul> <li>Automatically generates a JSON schema for parameters.</li> <li>Supports type hints for validation.</li> </ul>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#guardrail","title":"@guardrail","text":"<p>Define validation logic for inputs or outputs:</p> Python<pre><code>from agents import guardrail\n\n@guardrail\ndef restrict_length(text: str) -&gt; bool:\n    \"\"\"Ensures text is between 1 and 100 characters.\"\"\"\n    return 1 &lt;= len(text) &lt;= 100\n</code></pre> <ul> <li>Returns <code>True</code> if valid, <code>False</code> otherwise.</li> <li>Can be applied to tools or standalone.</li> </ul>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#hosted-tools","title":"Hosted Tools","text":"<p>Pre-built tools provided by the SDK: - <code>FileSearchTool</code>: Search within files. - <code>WebSearchTool</code>: Query the web. - <code>ComputerTool</code>: Execute OS-level commands.</p> <p>Example usage:</p> Python<pre><code>from agents.tools import WebSearchTool\n\nagent = Agent(\n    name=\"ResearchAgent\",\n    instructions=\"Find information online.\",\n    model=\"gpt-4o\",\n    tools=[WebSearchTool()],\n)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#6-running-agents","title":"6. Running Agents","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#asynchronous-execution","title":"Asynchronous Execution","text":"<p>Run an agent asynchronously for non-blocking workflows:</p> Python<pre><code>from agents import Runner\n\nasync def run_agent():\n    result = await Runner.run(agent, \"What is the weather today?\")\n    print(result.final_output)\n\nimport asyncio\nasyncio.run(run_agent())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#synchronous-execution","title":"Synchronous Execution","text":"<p>Run an agent synchronously for simpler use cases:</p> Python<pre><code>result = Runner.run_sync(agent, \"Calculate 5 + 3\")\nprint(result.final_output)  # Output: 8\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#streaming-results","title":"Streaming Results","text":"<p>Stream agent responses in real-time:</p> Python<pre><code>async def stream_agent():\n    async for event in Runner.run_streamed(agent, \"Tell me a story\"):\n        print(event.content, end=\"\")\n\nasyncio.run(stream_agent())\n</code></pre> <ul> <li>Events include intermediate outputs, tool calls, and final results.</li> </ul>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#7-multi-agent-workflows","title":"7. Multi-Agent Workflows","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#handoffs_1","title":"Handoffs","text":"<p>Delegate tasks between agents:</p> Python<pre><code>research_agent = Agent(name=\"Researcher\", instructions=\"Gather data.\")\nwriter_agent = Agent(name=\"Writer\", instructions=\"Write summaries.\")\nrouter = Agent(\n    name=\"Coordinator\",\n    instructions=\"Route tasks to the right agent.\",\n    handoffs=[research_agent, writer_agent],\n)\n\nresult = Runner.run_sync(router, \"Research and summarize AI trends.\")\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#agents-as-tools","title":"Agents as Tools","text":"<p>Use an agent as a callable tool:</p> Python<pre><code>@function_tool\ndef consult_expert(input: str) -&gt; str:\n    expert = Agent(name=\"Expert\", instructions=\"Provide detailed answers.\")\n    return Runner.run_sync(expert, input).final_output\n\nmain_agent = Agent(\n    name=\"MainAgent\",\n    instructions=\"Use the expert when needed.\",\n    tools=[consult_expert],\n)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#8-structured-outputs","title":"8. Structured Outputs","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#using-pydantic-models","title":"Using Pydantic Models","text":"<p>Define structured responses with Pydantic:</p> Python<pre><code>from pydantic import BaseModel\n\nclass MathResult(BaseModel):\n    result: int\n    steps: str\n\nagent = Agent(\n    name=\"MathSolver\",\n    instructions=\"Solve math problems and explain steps.\",\n    model=\"gpt-4o\",\n    output_type=MathResult,\n)\n\nresult = Runner.run_sync(agent, \"What is 7 * 8?\")\nprint(result.final_output.result)  # 56\nprint(result.final_output.steps)   # Explanation\n</code></pre> <ul> <li>Ensures type safety and consistent output formats.</li> </ul>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#9-usage-metrics","title":"9. Usage Metrics","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#tracking-requests-and-tokens","title":"Tracking Requests and Tokens","text":"<p>Monitor API usage via the <code>RunResult</code> object:</p> Python<pre><code>result = Runner.run_sync(agent, \"Hello, world!\")\nprint(f\"Requests: {result.usage.requests}\")\nprint(f\"Input Tokens: {result.usage.input_tokens}\")\nprint(f\"Output Tokens: {result.usage.output_tokens}\")\n</code></pre> <ul> <li>Useful for cost estimation and performance optimization.</li> </ul>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#10-advanced-features","title":"10. Advanced Features","text":""},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#customizing-agent-behavior","title":"Customizing Agent Behavior","text":"<p>Adjust agent settings: - Temperature: Controls randomness (0.0\u20132.0, default 1.0). - Max Tokens: Limits response length.</p> Python<pre><code>agent = Agent(\n    name=\"CreativeAgent\",\n    instructions=\"Write creative stories.\",\n    model=\"gpt-4o\",\n    temperature=1.5,\n    max_tokens=500,\n)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#error-handling","title":"Error Handling","text":"<p>Handle exceptions gracefully:</p> Python<pre><code>try:\n    result = Runner.run_sync(agent, \"Invalid input\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#debugging","title":"Debugging","text":"<p>Enable debug mode for detailed logs:</p> Python<pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\nresult = Runner.run_sync(agent, \"Test run\")\n</code></pre>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#11-best-practices","title":"11. Best Practices","text":"<ul> <li>Clear Instructions: Use precise language to define agent roles.</li> <li>Modular Design: Create reusable tools and agents.</li> <li>Validation: Apply guardrails to enforce constraints.</li> <li>Optimization: Monitor usage metrics to reduce costs.</li> <li>Testing: Simulate edge cases to ensure robustness.</li> <li>Version Control: Track changes to agent configurations.</li> </ul>"},{"location":"OpAgentsOlympus/practice/openai_agents_sdk/#12-additional-resources","title":"12. Additional Resources","text":"<ul> <li>Official Documentation: Full SDK reference.</li> <li>OpenAI Platform Docs: API details and model info.</li> <li>Building Agents Guide: Step-by-step tutorials.</li> <li>GitHub Repository: Source code and issues.</li> <li>YouTube Tutorials: Video guides.</li> </ul> <p>Happy coding Guys!</p>"},{"location":"OpAgentsOlympus/practice/output_guardrails/","title":"Output Guardrails","text":"Source code in OpAgentsOlympus/practice/output_guardrails.py OpAgentsOlympus/practice/output_guardrails.py<pre><code>from __future__ import annotations\n\nimport asyncio\nimport json\n\nfrom pydantic import BaseModel, Field\nfrom config import config\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    OutputGuardrailTripwireTriggered,\n    RunContextWrapper,\n    Runner,\n    output_guardrail,\n)\n\n\"\"\"\nThis example shows how to use output guardrails.\n\nOutput guardrails are checks that run on the final output of an agent.\nThey can be used to do things like:\n- Check if the output contains sensitive data\n- Check if the output is a valid response to the user's message\n\nIn this example, we'll use a (contrived) example where we check if the agent's response contains\na phone number.\n\"\"\"\n\n\n# The agent's output type\nclass MessageOutput(BaseModel):\n    reasoning: str = Field(\n        description=\"Thoughts on how to respond to the user's message\"\n    )\n    response: str = Field(description=\"The response to the user's message\")\n    user_name: str | None = Field(\n        description=\"The name of the user who sent the message, if known\"\n    )\n\n\n@output_guardrail\nasync def sensitive_data_check(\n    context: RunContextWrapper, agent: Agent, output: MessageOutput\n) -&gt; GuardrailFunctionOutput:\n    phone_number_in_response = \"650\" in output.response\n    phone_number_in_reasoning = \"650\" in output.reasoning\n    print(\"Response: \" + output.response)\n    print(\"Reasoning: \" + output.reasoning)\n    print(phone_number_in_reasoning, phone_number_in_response)\n    return GuardrailFunctionOutput(\n        output_info={\n            \"phone_number_in_response\": phone_number_in_response,\n            \"phone_number_in_reasoning\": phone_number_in_reasoning,\n        },\n        tripwire_triggered=phone_number_in_response or phone_number_in_reasoning,\n    )\n\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"You are a helpful assistant.\",\n    output_type=MessageOutput,\n    output_guardrails=[sensitive_data_check],\n)\n\n\nasync def main():\n    # This should be ok\n    await Runner.run(agent, \"What's the capital of California?\", run_config=config)\n    print(\"First message passed\")\n\n    # This should trip the guardrail\n    try:\n        result = await Runner.run(\n            agent, \"say this number back to me 650-123-4567?\", run_config=config\n        )\n        print(\n            f\"Guardrail didn't trip - this is unexpected. Output: {json.dumps(result.final_output.model_dump(), indent=2)}\"\n        )\n\n    except OutputGuardrailTripwireTriggered as e:\n        print(f\"Guardrail tripped. Info: {e.guardrail_result.output.output_info}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/output_guardrails_run_first_or_end_hooks/","title":"Output Guardrails Run First Or End Hooks","text":"Source code in OpAgentsOlympus/practice/output_guardrails_run_first_or_end_hooks.py OpAgentsOlympus/practice/output_guardrails_run_first_or_end_hooks.py<pre><code>from typing import Any  \nfrom agents import (  \n    Agent,   \n    Runner,   \n    RunContextWrapper,   \n    OutputGuardrail,   \n    GuardrailFunctionOutput,  \n    AgentHooks,  \n    RunHooks  \n)  \nfrom config import config\nimport asyncio\n\nexecution_order = []  \n\nclass TestHooks(AgentHooks[Any]):  \n    async def on_end(self, context: RunContextWrapper[Any], agent: Agent[Any], final_output: Any):  \n        execution_order.append(\"on_end_hook\")  \n\nclass TestRunHooks(RunHooks[Any]):  \n    async def on_agent_end(self, context: RunContextWrapper[Any], agent: Agent[Any], final_output: Any):  \n        execution_order.append(\"on_agent_end_hook\")  \n\ndef output_guardrail(context: RunContextWrapper[Any], agent: Agent[Any], agent_output: Any) -&gt; GuardrailFunctionOutput:  \n    execution_order.append(\"output_guardrail\")  \n    return GuardrailFunctionOutput(  \n        output_info=\"validated\",  \n        tripwire_triggered=False  \n    )  \n\nasync def main():  \n    global execution_order  \n    execution_order = []\n\n    agent = Agent(  \n        name=\"test_agent\",  \n        instructions=\"You are a test agent\",  \n        hooks=TestHooks(),  \n        output_guardrails=[OutputGuardrail(guardrail_function=output_guardrail)]  \n    )  \n\n    run_hooks = TestRunHooks()  \n\n    await Runner.run(  \n        agent,  \n        \"Hello, please respond with a simple message\",  \n        hooks=run_hooks,\n        run_config=config\n    ) \n    print(execution_order) # ['on_agent_end_hook', 'on_end_hook', 'output_guardrail']\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/parallel_tool_call/","title":"Parallel Tool Call","text":"Source code in OpAgentsOlympus/practice/parallel_tool_call.py OpAgentsOlympus/practice/parallel_tool_call.py<pre><code>from openai import AsyncOpenAI\nfrom agents import (\n    Agent,\n    Runner,\n    OpenAIChatCompletionsModel,\n    RunConfig,\n    ModelSettings,\n    function_tool\n)\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"I guess you haven't set API KEY, I'am pretty sure you need to set it dude.\")\n\nclient = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-2.5-flash\", openai_client=client\n)\n\nconfig = RunConfig(model=model)\n\n@function_tool\ndef am_i_coder():\n    return 'Maybe \ud83e\udd37\u200d\u2642\ufe0f'\n\n@function_tool\ndef do_i_deserve_it():\n    return 'Maybe \ud83e\udd37\u200d\u2642\ufe0f'\n\nagent = Agent[None](\n    name=\"assistant\",\n    instructions=\"You are an amazing assistant, You only respond in haikus LOL MUST CALL A TOOL\",\n    model_settings=ModelSettings(\n        parallel_tool_calls=False # Setting parallel_tool_calls=False only works with OpenAI Models\n    ),\n    tools=[am_i_coder, do_i_deserve_it]\n)\n\nresult = Runner.run_sync(\n    agent,\n    \"Am I a CODER?\"\n    \"Do i deserve it?\",\n    run_config=config,\n)\n\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/parallelization_pattern/","title":"Parallelization Pattern","text":"Source code in OpAgentsOlympus/practice/parallelization_pattern.py OpAgentsOlympus/practice/parallelization_pattern.py<pre><code>import asyncio\n\nfrom agents import Agent, ItemHelpers, Runner\nfrom config import config\n\n\"\"\"\nThis example shows the parallelization pattern. We run the agent three times in parallel, and pick\nthe best result.\n\"\"\"\n\nspanish_agent = Agent(\n    name=\"spanish_agent\",\n    instructions=\"You translate the user's message to Spanish\",\n)\n\ntranslation_picker = Agent(\n    name=\"translation_picker\",\n    instructions=\"You pick the best Spanish translation from the given options.\",\n)\n\n\nasync def main():\n    msg = input(\"Hi! Enter a message, and we'll translate it to Spanish.\\n\\n\")\n\n    # Ensure the entire workflow is a single trace\n    # with trace(\"Parallel translation\"):\n    res_1, res_2, res_3 = await asyncio.gather(\n        Runner.run(spanish_agent, msg, run_config=config),\n        Runner.run(spanish_agent, msg, run_config=config),\n        Runner.run(spanish_agent, msg, run_config=config),\n    )\n\n    outputs = [\n        ItemHelpers.text_message_outputs(res_1.new_items),\n        ItemHelpers.text_message_outputs(res_2.new_items),\n        ItemHelpers.text_message_outputs(res_3.new_items),\n    ]\n\n    translations = \"\\n\\n|\".join(outputs)\n    print(f\"\\n\\nTranslations:\\n\\n{translations}\")\n\n    best_translation = await Runner.run(\n        translation_picker,\n        f\"Input: {msg}\\n\\nTranslations:\\n{translations}\",\n        run_config=config,\n    )\n\n    print(\"\\n\\n-----\")\n\n    print(f\"Best translation: {best_translation.final_output}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/practice/","title":"Practice","text":"Source code in OpAgentsOlympus/practice/practice.py OpAgentsOlympus/practice/practice.py<pre><code>from open_router_config import config\nfrom agents import HandoffInputData, Agent, handoff, Runner\nimport asyncio\nfrom rich import print\n\n\ndef remove_new_items(handoff_input_data: HandoffInputData) -&gt; HandoffInputData:\n    # print(\"Before:\", handoff_input_data)\n    filtered = HandoffInputData(\n        input_history=handoff_input_data.input_history,\n        pre_handoff_items=handoff_input_data.pre_handoff_items,\n        new_items=(),\n    )\n    # print(\"After:\", filtered)\n    return filtered  # This data will be received to the next handoff agent.\n\n\nsay_bye_agent = Agent(\n    name=\"say_bye_agent\", instructions=\"You only say bye to everyone.\"\n)\n\nsay_hello_agent1 = Agent(\n    name=\"say_hello_agent1\",\n    instructions=\"You only say hello to everyone.\",\n    handoffs=[\n        handoff(\n            agent=say_bye_agent,\n            input_filter=remove_new_items,\n        )\n    ],\n)\n\nsay_hello_agent2 = Agent(\n    name=\"say_hello_agent2\",\n    instructions=\"You only say hello to everyone.\",\n    handoffs=[\n        handoff(\n            agent=say_bye_agent,\n        )\n    ],\n)\n\n\nasync def main():\n    result = Runner.run_streamed(\n        say_hello_agent2, input=\"say hello and bye\", run_config=config\n    )\n    event_count = 0\n    async for event in result.stream_events():\n        event_count += 1\n        print(f\"\\n=== EVENT #{event_count} ===\")\n        print(f\"Type: {event.type}\")\n\n        if event.type == \"raw_response_event\":\n            print(f\"Raw Event Type: {event.data.type}\")\n            print(f\"Raw Event Data: {event.data}\")\n\n        elif event.type == \"run_item_stream_event\":\n            print(f\"Item Name: {event.name}\")\n            print(f\"Item Type: {event.item.type}\")\n            print(f\"Item: {event.item}\")\n\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"New Agent: {event.new_agent.name}\")\n        print(\"Not Filtered:\", result.to_input_list())\n\n    # result = Runner.run_streamed(say_hello_agent1, input=\"say hello and bye\", run_config=config)\n    # async for _ in result.stream_events():\n    #     pass\n    print(\"Filtered:\", result.to_input_list())\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/premium_basic_user_example/","title":"Premium Basic User Example","text":"Source code in OpAgentsOlympus/practice/premium_basic_user_example.py OpAgentsOlympus/practice/premium_basic_user_example.py<pre><code>import asyncio\n\nfrom agents import Agent, ItemHelpers, Runner, RunContextWrapper\nfrom open_router_config import config\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass UserContext:\n    name: str\n    is_premium_user: bool\n    age: int\n\n\ndaniel = UserContext(\"Daniel\", True, 19)\nahmad = UserContext(\"Ahmad\", False, 21)\n\n\ndef dynamic_instructions(\n    ctx: RunContextWrapper[UserContext], _: Agent[UserContext]\n) -&gt; str:\n    if ctx.context.is_premium_user:\n        return \"You are a premium agent, You can help the user with premium features.\"\n    return \"You are a basic agent, You can help the user with basic tasks.\"\n\n\nassistant = Agent(\n    name=\"assistant\",\n    instructions=dynamic_instructions,\n)\n\n\nasync def main():\n    msg = \"Who are you?\"\n\n    result = Runner.run_streamed(assistant, msg, run_config=config, context=daniel)\n\n    async for event in result.stream_events():\n        # We'll ignore the raw responses event deltas\n        if event.type == \"raw_response_event\":\n            continue\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n            continue\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(\n                    f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\"\n                )\n            else:\n                pass  # Ignore other event types\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/previous_response_id/","title":"Previous Response Id","text":"Source code in OpAgentsOlympus/practice/previous_response_id.py OpAgentsOlympus/practice/previous_response_id.py<pre><code>import asyncio\n\nfrom agents import Agent, Runner\nfrom open_router_config import config\n\n\"\"\"This demonstrates usage of the `previous_response_id` parameter to continue a conversation.\nThe second run passes the previous response ID to the model, which allows it to continue the\nconversation without re-sending the previous messages.\n\nNotes:\n1. This only applies to the OpenAI Responses API. Other models will ignore this parameter.\n2. Responses are only stored for 30 days as of this writing, so in production you should\nstore the response ID along with an expiration date; if the response is no longer valid,\nyou'll need to re-send the previous conversation history.\n\"\"\"\n\n\nasync def main():\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful assistant. be VERY concise.\",\n    )\n\n    result = await Runner.run(\n        agent, \"What is the largest country in South America?\", run_config=config\n    )\n    print(result.final_output)\n    # Brazil\n\n    result = await Runner.run(\n        agent,\n        \"What is the capital of that country?\",\n        run_config=config,\n        previous_response_id=result.last_response_id,\n    )\n    print(result.final_output)\n    # Brasilia\n\n\nasync def main_stream():\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful assistant. be VERY concise.\",\n    )\n\n    result = Runner.run_streamed(\n        agent, \"What is the largest country in South America?\", run_config=config\n    )\n\n    async for event in result.stream_events():\n        if (\n            event.type == \"raw_response_event\"\n            and event.data.type == \"response.output_text.delta\"\n        ):\n            print(event.data.delta, end=\"\", flush=True)\n\n    print()\n\n    result = Runner.run_streamed(\n        agent,\n        \"What was my last chat?\",\n        previous_response_id=result.last_response_id,\n        run_config=config,\n    )\n\n    async for event in result.stream_events():\n        if (\n            event.type == \"raw_response_event\"\n            and event.data.type == \"response.output_text.delta\"\n        ):\n            print(event.data.delta, end=\"\", flush=True)\n\n\nif __name__ == \"__main__\":\n    is_stream = input(\"Run in stream mode? (y/n): \")\n    if is_stream == \"y\":\n        asyncio.run(main_stream())\n    else:\n        asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/prompt_template/","title":"Prompt Template","text":"Source code in OpAgentsOlympus/practice/prompt_template.py <p>OpAgentsOlympus/practice/prompt_template.py<pre><code>import argparse\nimport asyncio\nimport random\nfrom open_router_config import config\nfrom agents import Agent, Runner, GenerateDynamicPromptData\n\n\"\"\"\nNOTE: This example will not work out of the box, because the default prompt ID will not be available\nin your project.\n\nTo use it, please:\n1. Go to https://platform.openai.com/playground/prompts\n2. Create a new prompt variable, `poem_style`.\n3. Create a system prompt with the content:\n</code></pre> Write a poem in {{poem_style}} Text Only<pre><code>4. Run the example with the `--prompt-id` flag.\n\"\"\"\n\nDEFAULT_PROMPT_ID = \"pmpt_686a4b884b708193b5e81a4ce03c707f0422d8b0bac332ce\"\n\n\nclass DynamicContext:\n    def __init__(self, prompt_id: str):\n        self.prompt_id = prompt_id\n        self.poem_style = random.choice([\"limerick\", \"haiku\", \"ballad\"])\n        print(f\"[debug] DynamicContext initialized with poem_style: {self.poem_style}\")\n\n\nasync def _get_dynamic_prompt(data: GenerateDynamicPromptData):\n    ctx: DynamicContext = data.context.context\n    return {\n        \"id\": ctx.prompt_id,\n        \"version\": \"1\",\n        \"variables\": {\n            \"poem_style\": ctx.poem_style,\n        },\n    }\n\n\nasync def dynamic_prompt(prompt_id: str):\n    context = DynamicContext(prompt_id)\n\n    agent = Agent(\n        name=\"Assistant\",\n        prompt=_get_dynamic_prompt,\n    )\n\n    result = await Runner.run(\n        agent,\n        \"Tell me about recursion in programming.\",\n        context=context,\n        run_config=config,\n    )\n    print(result.final_output)\n\n\nasync def static_prompt(prompt_id: str):\n    agent = Agent(\n        name=\"Assistant\",\n        prompt={\n            \"id\": prompt_id,\n            \"version\": \"1\",\n            \"variables\": {\n                \"poem_style\": \"limerick\",\n            },\n        },\n    )\n\n    result = await Runner.run(\n        agent, \"Tell me about recursion in programming.\", run_config=config\n    )\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dynamic\", action=\"store_true\")\n    parser.add_argument(\"--prompt-id\", type=str, default=DEFAULT_PROMPT_ID)\n    args = parser.parse_args()\n\n    if args.dynamic:\n        asyncio.run(dynamic_prompt(args.prompt_id))\n    else:\n        asyncio.run(static_prompt(args.prompt_id))\n</code></pre></p>"},{"location":"OpAgentsOlympus/practice/pydantic/","title":"Pydantic","text":"Source code in OpAgentsOlympus/practice/pydantic.py OpAgentsOlympus/practice/pydantic.py<pre><code>from datetime import datetime, date\nfrom decimal import Decimal\nfrom typing import Optional, Union, Annotated, Any\nfrom enum import Enum\n\nfrom pydantic import (\n    BaseModel,\n    Field,\n    ConfigDict,\n    field_validator,\n    model_validator,\n    field_serializer,\n    model_serializer,\n    ValidationError,\n    PositiveInt,\n    EmailStr,\n    HttpUrl,\n    SecretStr\n)\n\nclass UserRole(str, Enum):\n    ADMIN = \"admin\"\n    USER = \"user\"\n    GUEST = \"guest\"\n\nclass Address(BaseModel):\n    model_config = ConfigDict(\n        str_strip_whitespace=True,  # Automatically remove leading/trailing spaces from string inputs\n        validate_assignment=True,   # Validate field values when attributes are set after model creation\n        extra='forbid'              # Raise ValidationError when unknown fields are provided in input data\n    )\n\n    street: str = Field(\n        min_length=1, max_length=100\n    )  # Ensure street is between 1-100 characters\n    city: str = Field(\n        min_length=1, max_length=50\n    )  # Ensure city is between 1-50 characters\n    postal_code: str = Field(\n        pattern=r'^\\d{5}(-\\d{4})?$'\n    )  # US postal code validation (5 digits or 5+4 format)\n    country: str = Field(\n        default=\"USA\", frozen=True\n    )  # Default \"USA\", immutable after creation\n\nclass User(BaseModel):\n    model_config = ConfigDict(\n        validate_assignment=True,      # Validate new values when fields are modified after instantiation\n        validate_default=True,         # Validate default values on every model creation for safety\n        strict=False,                  # Enable lax mode allowing type coercion (string \"123\" -&gt; int 123)\n        coerce_numbers_to_str=True,    # Allow number-to-string conversion (123 -&gt; \"123\")\n        extra='allow',                 # Permit additional fields not defined in model schema\n        str_strip_whitespace=True,     # Automatically trim whitespace from all string inputs\n        str_to_lower=False,            # Preserve original string casing (set True for lowercase conversion)\n        str_max_length=1000,           # Global string length limit for all string fields\n        ser_json_timedelta='iso8601',  # Serialize timedelta objects in ISO8601 format\n        ser_json_bytes='base64',       # Serialize bytes objects as base64 encoded strings\n        title=\"User Model\",            # Human-readable model name in generated schemas\n        use_attribute_docstrings=True, # Include docstrings in generated JSON schema\n        frozen=False,                  # Allow field modification after model creation (True makes immutable)\n        populate_by_name=True,         # Accept both field names and aliases during validation\n        from_attributes=True           # Create models from objects with attributes (replaces orm_mode)\n    )\n\n    id: PositiveInt = Field(\n        description=\"Unique user identifier\",\n        examples=[1, 42, 123],\n        gt=0, le=999999\n    )  # PositiveInt type with constraints\n\n    name: str = Field(\n        min_length=2, max_length=50,\n        description=\"User's full name\",\n        alias=\"full_name\"\n    )  # String field with length constraints and alias\n\n    email: EmailStr = Field(\n        description=\"User's email address\",\n        validation_alias=\"email_address\"\n    )  # EmailStr validation with input-only alias\n\n    password: SecretStr = Field(\n        min_length=8,\n        description=\"User password (will be hidden in output)\"\n    )  # SecretStr field with min_length=8\n\n    role: UserRole = Field(\n        default=UserRole.USER\n    )  # Enum field with default value\n\n    is_active: bool = Field(\n        default=True\n    )  # Boolean field with default=True\n\n    age: Optional[int] = Field(\n        default=None, ge=0, le=150,\n        description=\"User's age in years\"\n    )  # Optional integer with age constraints\n\n    balance: Decimal = Field(\n        default=Decimal('0.00'),\n        max_digits=10, decimal_places=2,\n        description=\"Account balance\"\n    )  # Decimal field with precision constraints\n\n    created_at: datetime = Field(\n        default_factory=datetime.now\n    )  # Datetime field with factory function\n\n    birth_date: Optional[date] = None  # Optional date field for birth date tracking\n\n    website: Optional[HttpUrl] = None  # Optional HttpUrl field for website validation\n\n    address: Optional[Address] = None  # Optional nested model field for address composition\n\n    tags: list[str] = Field(\n        default_factory=list,\n        max_length=10,\n        description=\"User tags\"\n    )  # List field with max_length=10 constraint\n\n    metadata: dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Additional user metadata\"\n    )  # Flexible metadata dict field\n\n    phone: Union[str, int] = Field(\n        description=\"Phone number as string or int\",\n        union_mode='left_to_right'\n    )  # Union field for phone number flexibility\n\n    score: Annotated[float, Field(ge=0.0, le=100.0, multiple_of=0.1)] = 0.0  # Float field with range and precision constraints\n\n    @field_validator('name')\n    @classmethod\n    def validate_name(cls, v: str) -&gt; str:\n        if not v.replace(' ', '').isalpha():\n            raise ValueError('Name must contain only letters and spaces')\n        return v.title()  # Auto-title case\n\n    @field_validator('tags')\n    @classmethod\n    def validate_tags(cls, v: list[str]) -&gt; list[str]:\n        return [tag.lower().strip() for tag in v if tag.strip()]  # Clean and normalize tag list\n\n    @model_validator(mode='after')\n    def validate_age_birth_date(self) -&gt; 'User':\n        if self.age is not None and self.birth_date is not None:\n            calculated_age = (date.today() - self.birth_date).days // 365\n            if abs(calculated_age - self.age) &gt; 1:\n                raise ValueError('Age and birth date do not match')\n        return self  # Cross-field  validation between age and birth_date\n\n    @field_serializer('password')\n    def serialize_password(self, value: SecretStr) -&gt; str:\n        return \"***HIDDEN***\"  # Hide password content in output\n\n    @model_serializer(mode='wrap')\n    def serialize_model(self, serializer, info):\n        data = serializer(self)\n        data['display_name'] = f\"{self.name} ({self.role.value})\"\n        return data  # Inject computed display_name field in output\n\nif __name__ == \"__main__\":\n    user_data = {\n        \"id\": \"123\",  # String to int coercion\n        \"full_name\": \"  john doe  \",  # Whitespace stripping + title casing\n        \"email_address\": \"john@example.com\",  # Email validation\n        \"password\": \"secretpassword123\",  # Secret string handling\n        \"age\": \"25\",  # String to int coercion\n        \"balance\": \"1234.56\",  # String to Decimal coercion\n        \"birth_date\": \"1998-01-01\",  # String to date coercion\n        \"website\": \"https://johndoe.com\",  # String to HttpUrl coercion\n        \"phone\": 1234567890,  # Union type handling\n        \"tags\": [\"  Developer  \", \"Python\", \"  \"],  # List cleaning\n        \"score\": \"85.5\",  # String to float coercion\n        \"metadata\": {\"department\": \"engineering\", \"level\": 3},  # Dict handling\n        \"extra_field\": \"This will be allowed due to extra='allow'\"  # Extra field allowance\n    }\n\n    try:\n        user = User(**user_data)\n        print(\"User created successfully!\")\n        print(f\"Serialized: {user.model_dump()}\")\n        print(f\"JSON: {user.model_dump_json(indent=2)}\")\n\n    except ValidationError as e:\n        print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"OpAgentsOlympus/practice/pydantic_cheatsheet/","title":"Pydantic Cheatsheet","text":""},{"location":"OpAgentsOlympus/practice/pydantic_cheatsheet/#comprehensive-explanation-of-each-setting","title":"Comprehensive Explanation of Each Setting","text":""},{"location":"OpAgentsOlympus/practice/pydantic_cheatsheet/#model-configuration-configdict","title":"Model Configuration (<code>ConfigDict</code>)","text":"<p><code>validate_assignment=True</code> - Validates data when you assign new values to fields after creating the model - Without this, <code>user.age = \"invalid\"</code> would work; with it, it raises a validation error</p> <p><code>validate_default=True</code> - Validates default values every time you create an instance - Normally defaults are trusted for performance; this ensures they're always valid</p> <p><code>strict=False</code> - Enables lax mode where Pydantic tries to coerce compatible types - <code>strict=True</code> would only accept exact type matches</p> <p><code>coerce_numbers_to_str=True</code> - Allows converting numbers to strings (normally disabled) - Example: <code>123</code> \u2192 <code>\"123\"</code> for string fields</p> <p><code>extra='allow'</code> - Allows extra fields not defined in the model - Options: <code>'ignore'</code> (default), <code>'forbid'</code>, <code>'allow'</code></p> <p><code>str_strip_whitespace=True</code> - Automatically removes leading/trailing whitespace from strings - <code>\"  hello  \"</code> becomes <code>\"hello\"</code></p> <p><code>str_to_lower=False</code> - When <code>True</code>, converts all strings to lowercase - Useful for case-insensitive fields</p> <p><code>from_attributes=True</code> - Allows creating models from objects with attributes (like ORM instances) - Replaces V1's <code>orm_mode=True</code></p> <p><code>populate_by_name=False</code> - Allows population of fields using their Python names even if an alias is set</p> <p><code>use_enum_values=False</code> - When <code>True</code>, serializes enum fields using their values instead of names</p> <p><code>json_schema_extra={}</code> - Add extra metadata to the generated JSON schema</p> <p><code>protected_namespaces=('model_',)</code> - Prevents assignment to attributes with these prefixes</p> <p><code>arbitrary_types_allowed=False</code> - When <code>True</code>, allows arbitrary (non-Pydantic) types as fields</p>"},{"location":"OpAgentsOlympus/practice/pydantic_cheatsheet/#field-configuration-field","title":"Field Configuration (<code>Field()</code>)","text":"<p>Validation Constraints: - <code>min_length</code>/<code>max_length</code>: String and collection length limits - <code>gt</code>/<code>ge</code>/<code>lt</code>/<code>le</code>: Numeric comparison constraints - <code>pattern</code>: Regular expression validation - <code>multiple_of</code>: Number must be multiple of specified value</p> <p>Aliases: - <code>alias</code>: Name used for both validation and serialization - <code>validation_alias</code>: Only affects input validation - <code>serialization_alias</code>: Only affects output serialization</p> <p>Metadata: - <code>description</code>: Human-readable field description - <code>examples</code>: Example values for documentation - <code>title</code>: Human-readable title</p> <p>Special Behaviors: - <code>frozen=True</code>: Makes field immutable after creation - <code>exclude=True</code>: Excludes field from serialization - <code>union_mode</code>: Controls how unions are validated - <code>default_factory</code>: Callable to generate default value at runtime</p>"},{"location":"OpAgentsOlympus/practice/pydantic_cheatsheet/#custom-validation","title":"Custom Validation","text":"<p><code>@field_validator</code> - Validates individual fields with custom logic - Replaces V1's <code>@validator</code></p> <p><code>@model_validator</code>  - Validates the entire model after all fields are processed - Can access multiple fields for cross-field validation</p>"},{"location":"OpAgentsOlympus/practice/pydantic_cheatsheet/#custom-serialization","title":"Custom Serialization","text":"<p><code>@field_serializer</code> - Customizes how individual fields are serialized to dict/JSON</p> <p><code>@model_serializer</code> - Customizes how the entire model is serialized - Can add computed fields or modify the output structure</p>"},{"location":"OpAgentsOlympus/practice/pydantic_cheatsheet/#special-types","title":"Special Types","text":"<p><code>PositiveInt</code>: Integer that must be &gt; 0  </p> <p><code>EmailStr</code>: Validates email format (requires <code>email-validator</code>)  </p> <p><code>HttpUrl</code>: Validates and normalizes URLs  </p> <p><code>SecretStr</code>: Hides sensitive data in output  </p> <p><code>StrictInt</code>/<code>StrictStr</code>: Only accepts exact type matches  </p> <p><code>UUID</code>: Validates and parses UUID strings  </p> <p><code>conlist</code>: List with length and item constraints  </p> <p><code>constr</code>: String with length and pattern constraints  </p> <p><code>Json</code>: Field that parses and validates JSON data  </p>"},{"location":"OpAgentsOlympus/practice/quickstart/","title":"Quickstart","text":"Source code in OpAgentsOlympus/practice/quickstart.py OpAgentsOlympus/practice/quickstart.py<pre><code>from pydantic import BaseModel\nfrom agents import (\n    Agent,\n    GuardrailFunctionOutput,\n    Runner,\n    set_tracing_disabled,\n    OpenAIChatCompletionsModel,\n    InputGuardrail,\n    RunConfig,\n)\nimport os\nfrom openai import AsyncOpenAI\nimport dotenv\nimport asyncio\n\ndotenv.load_dotenv()\nset_tracing_disabled(True)\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n\nif not GEMINI_API_KEY:\n    raise ValueError(\"API key not found!!\")\n\nclient = AsyncOpenAI(\n    api_key=GEMINI_API_KEY,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\nmodel = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=client)\nconfig = RunConfig(model=model, model_provider=client)\n\n\nclass HomeworkOutput(BaseModel):\n    is_homework: bool\n    reasoning: str\n\n\nguardrail_agent = Agent(\n    name=\"Guardrail check\",\n    instructions=\"Check if the user is asking about homework.\",\n    output_type=HomeworkOutput,\n)\n\nmath_tutor_agent = Agent(\n    name=\"Math Tutor\",\n    handoff_description=\"Specialist agent for math questions\",\n    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n)\n\nhistory_tutor_agent = Agent(\n    name=\"History Tutor\",\n    handoff_description=\"Specialist agent for historical questions\",\n    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n)\n\n\nasync def homework_guardrail(ctx, agent, input_data):\n    result = await Runner.run(\n        guardrail_agent, input_data, context=ctx.context, run_config=config\n    )\n    final_output = result.final_output_as(HomeworkOutput)\n    print(final_output)\n    return GuardrailFunctionOutput(\n        output_info=final_output,\n        tripwire_triggered=not final_output.is_homework,\n    )\n\n\ntriage_agent = Agent(\n    name=\"Triage Agent\",\n    instructions=\"You determine which agent to use based on the user's homework question\",\n    handoffs=[history_tutor_agent, math_tutor_agent],\n    input_guardrails=[\n        InputGuardrail(guardrail_function=homework_guardrail),\n    ],\n)\n\n\nasync def main():\n    # result =  Runner.run_streamed(triage_agent, \"What is 2 + 2?\", run_config=config)\n    # async for event in result.stream_events():\n    #     if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n    #         print(event.data.delta, end=\"\", flush=True)\n\n    result = await Runner.run(triage_agent, \"what is life\", run_config=config)\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/raise_usererror/","title":"Raise Usererror","text":"Source code in OpAgentsOlympus/practice/raise_usererror.py OpAgentsOlympus/practice/raise_usererror.py<pre><code>from agents import Runner, Agent, function_tool\nimport asyncio\nfrom open_router_config import config\n\n\n@function_tool\ndef say_hello():\n    print(\"say_hello was called...\")\n    return \"Hello!\"\n\n\nasync def main():\n    agent = Agent(\n        name=\"assistant\",\n        instructions=\"You are a helpful assistant. you MUST call say_hello tool\",\n        tools=[say_hello],\n        tool_use_behavior=[\n            \"stop_on_first_tool\"\n        ],  # Passed a List, This will cause a UserError!\n    )\n\n    result = await Runner.run(agent, input=\"Say Hello!\", run_config=config)\n    print(\"\\nFinal Output:\", result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n# _run_impl.py\n# Line 983: raise UserError(f\"Invalid tool_use_behavior: {agent.tool_use_behavior}\")\n</code></pre>"},{"location":"OpAgentsOlympus/practice/remote_image/","title":"Remote Image","text":"Source code in OpAgentsOlympus/practice/remote_image.py OpAgentsOlympus/practice/remote_image.py<pre><code>import asyncio\n\nfrom agents import Agent, Runner\nfrom open_router_config import config\n\nURL = \"https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg\"\n\n\nasync def main():\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    result = await Runner.run(\n        agent,\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"input_image\", \"detail\": \"auto\", \"image_url\": URL}\n                ],\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Which country is this?\",\n            },\n        ],\n        run_config=config,\n    )\n    print(result.final_output)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/routing_pattern/","title":"Routing Pattern","text":"Source code in OpAgentsOlympus/practice/routing_pattern.py OpAgentsOlympus/practice/routing_pattern.py<pre><code>import asyncio\nfrom config import config\n\nfrom agents import (\n    Agent,\n    Runner,\n    TResponseInputItem,\n    handoff,\n    RunContextWrapper,\n    ItemHelpers,\n)\n\n\"\"\"\nThis example shows the handoffs/routing pattern. The triage agent receives the first message, and\nthen hands off to the appropriate agent based on the language of the request. Responses are\nstreamed to the user.\n\"\"\"\n\n\ndef handoff_called(ctx: RunContextWrapper[None]):\n    print(\"Handing off...\")\n\n\ndef handoffs(agent: Agent):\n    return handoff(agent, on_handoff=handoff_called)\n\n\nfrench_agent = Agent(\n    name=\"french_agent\",\n    instructions=\"You only speak French, If user asks some other language, transfer to triage agent.\",\n)\n\nspanish_agent = Agent(\n    name=\"spanish_agent\",\n    instructions=\"You only speak Spanish, If user asks some other language, transfer to triage agent.\",\n)\n\nenglish_agent = Agent(\n    name=\"english_agent\",\n    instructions=\"You only speak English, If user asks some other language, transfer to triage agent.\",\n)\n\ntriage_agent = Agent(\n    name=\"triage_agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request. You NEVER answer to USER yourself! ALWAYS delegate to specialized agents\",\n    handoffs=[handoffs(french_agent), handoffs(spanish_agent), handoffs(english_agent)],\n    handoff_description=\"Triage agent who decide which language agent to delegate the task to.\",\n)\n\nfrench_agent.handoffs.append(handoffs(triage_agent))\nspanish_agent.handoffs.append(handoffs(triage_agent))\nenglish_agent.handoffs.append(handoffs(triage_agent))\n\n\nasync def main():\n    # We'll create an ID for this conversation, so we can link each trace\n    # conversation_id = str(uuid.uuid4().hex[:16])\n\n    msg = input(\"Hi! We speak French, Spanish and English. How can I help? \")\n    agent = triage_agent\n    inputs: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n\n    while True:\n        # Each conversation turn is a single trace. Normally, each input from the user would be an\n        # API request to your app, and you can wrap the request in a trace()\n        # with trace(\"Routing example\", group_id=conversation_id):\n        result = Runner.run_streamed(agent, input=inputs, run_config=config)\n        async for event in result.stream_events():\n            # We'll ignore the raw responses event deltas\n            if event.type == \"raw_response_event\":\n                continue\n            elif event.type == \"agent_updated_stream_event\":\n                print(f\"Agent updated: {event.new_agent.name}\")\n                continue\n            elif event.type == \"run_item_stream_event\":\n                if event.item.type == \"tool_call_item\":\n                    print(\"-- Tool was called\")\n                elif event.item.type == \"tool_call_output_item\":\n                    print(f\"-- Tool output: {event.item.output}\")\n                elif event.item.type == \"message_output_item\":\n                    print(\n                        f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\"\n                    )\n                else:\n                    pass  # Ignore other event types\n\n        inputs = result.to_input_list()\n        print(\"\\n\")\n\n        user_msg = input(\"Enter a message: \")\n        inputs.append({\"content\": user_msg, \"role\": \"user\"})\n        agent = result.current_agent\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/run_hooks/","title":"Run Hooks","text":"Source code in OpAgentsOlympus/practice/run_hooks.py OpAgentsOlympus/practice/run_hooks.py<pre><code>from agents import Agent, Runner, function_tool, RunContextWrapper, RunHooks, TContext, TResponseInputItem, ModelResponse\nfrom typing import Optional, Any\nfrom open_router_config import config\n\nclass HelloRunHooks(RunHooks):\n\n    async def on_llm_start(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        system_prompt: Optional[str],\n        input_items: list[TResponseInputItem],\n    ) -&gt; None:\n        \"\"\"Called just before invoking the LLM for this agent.\"\"\"\n        print('on_llm_start')\n\n    async def on_llm_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        response: ModelResponse,\n    ) -&gt; None:\n        \"\"\"Called immediately after the LLM call returns for this agent.\"\"\"\n        print('on_llm_end')\n\n    async def on_agent_start(self, context: RunContextWrapper[TContext], agent: Any) -&gt; None:\n        \"\"\"Called before the agent is invoked. Called each time the current agent changes.\"\"\"\n        print('on_agent_start')\n\n    async def on_agent_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Any,\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Called when the agent produces a final output.\"\"\"\n        print('on_agent_end')\n\n    async def on_handoff(\n        self,\n        context: RunContextWrapper[TContext],\n        from_agent: Any,\n        to_agent: Any,\n    ) -&gt; None:\n        \"\"\"Called when a handoff occurs.\"\"\"\n        print('on_handoff')\n\n    async def on_tool_start(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Any,\n        tool: Any,\n    ) -&gt; None:\n        \"\"\"Called concurrently with tool invocation.\"\"\"\n        print('on_tool_start')\n\n    async def on_tool_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Any,\n        tool: Any,\n        result: str,\n    ) -&gt; None:\n        \"\"\"Called after a tool is invoked.\"\"\"\n        print('on_tool_tool')\n\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"A simple function to get the weather for a user.\"\"\"\n    return f\"The weather for {city} is sunny.\"\n\nnews_agent: Agent = Agent(\n    name=\"NewsAgent\",\n    instructions=\"You are a helpful news assistant.\",\n)\n\nbase_agent: Agent = Agent(\n    name=\"WeatherAgent\",\n    instructions=\"You are a helpful assistant. Talk about weather and let news_agent handle the news things\",\n    tools=[get_weather],\n    handoffs=[news_agent]\n)\n\nres = Runner.run_sync(\n    starting_agent=base_agent, \n    input=\"What's the latest news about Qwen Code - seems like it can give though time to claude code.\",\n    hooks=HelloRunHooks(),\n    run_config=config\n)\n\nprint(res.last_agent.name)\nprint(res.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/run_lifecycle/","title":"Run Lifecycle","text":"Source code in OpAgentsOlympus/practice/run_lifecycle.py OpAgentsOlympus/practice/run_lifecycle.py<pre><code>import asyncio\nimport random\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom agents import (\n    Agent,\n    RunContextWrapper,\n    RunHooks,\n    Runner,\n    Tool,\n    Usage,\n    function_tool,\n)\n\nfrom open_router_config import config\n\n\nclass ExampleHooks(RunHooks):\n    def __init__(self):\n        self.event_counter = 0\n\n    def _usage_to_str(self, usage: Usage) -&gt; str:\n        return f\"{usage.requests} requests, {usage.input_tokens} input tokens, {usage.output_tokens} output tokens, {usage.total_tokens} total tokens\"\n\n    async def on_agent_start(self, context: RunContextWrapper, agent: Agent) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### {self.event_counter}: Agent {agent.name} started. Usage: {self._usage_to_str(context.usage)}\"\n        )\n\n    async def on_agent_end(\n        self, context: RunContextWrapper, agent: Agent, output: Any\n    ) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### {self.event_counter}: Agent {agent.name} ended with output {output}. Usage: {self._usage_to_str(context.usage)}\"\n        )\n\n    async def on_tool_start(\n        self, context: RunContextWrapper, agent: Agent, tool: Tool\n    ) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### {self.event_counter}: Tool {tool.name} started. Usage: {self._usage_to_str(context.usage)}\"\n        )\n\n    async def on_tool_end(\n        self, context: RunContextWrapper, agent: Agent, tool: Tool, result: str\n    ) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### {self.event_counter}: Tool {tool.name} ended with result {result}. Usage: {self._usage_to_str(context.usage)}\"\n        )\n\n    async def on_handoff(\n        self, context: RunContextWrapper, from_agent: Agent, to_agent: Agent\n    ) -&gt; None:\n        self.event_counter += 1\n        print(\n            f\"### {self.event_counter}: Handoff from {from_agent.name} to {to_agent.name}. Usage: {self._usage_to_str(context.usage)}\"\n        )\n\n\nhooks = ExampleHooks()\n\n###\n\n\n@function_tool\ndef random_number(max: int) -&gt; int:\n    \"\"\"Generate a random number up to the provided max.\"\"\"\n    return random.randint(0, max)\n\n\n@function_tool\ndef multiply_by_two(x: int) -&gt; int:\n    \"\"\"Return x times two.\"\"\"\n    return x * 2\n\n\nclass FinalResult(BaseModel):\n    number: int\n\n\nmultiply_agent = Agent(\n    name=\"Multiply Agent\",\n    instructions=\"Multiply the number by 2 and then return the final result.\",\n    tools=[multiply_by_two],\n    output_type=FinalResult,\n)\n\nstart_agent = Agent(\n    name=\"Start Agent\",\n    instructions=\"Generate a random number. If it's even, stop. If it's odd, hand off to the multiplier agent.\",\n    tools=[random_number],\n    output_type=FinalResult,\n    handoffs=[multiply_agent],\n)\n\n\nasync def main() -&gt; None:\n    user_input = input(\"Enter a max number: \")\n    await Runner.run(\n        start_agent,\n        hooks=hooks,\n        input=f\"Generate a random number between 0 and {user_input}.\",\n        run_config=config,\n    )\n\n    print(\"Done!\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\"\"\"\n$ python examples/basic/lifecycle_example.py\n\nEnter a max number: 250\n### 1: Agent Start Agent started. Usage: 0 requests, 0 input tokens, 0 output tokens, 0 total tokens\n### 2: Tool random_number started. Usage: 1 requests, 148 input tokens, 15 output tokens, 163 total tokens\n### 3: Tool random_number ended with result 101. Usage: 1 requests, 148 input tokens, 15 output tokens, 163 total tokens\n### 4: Handoff from Start Agent to Multiply Agent. Usage: 2 requests, 323 input tokens, 30 output tokens, 353 total tokens\n### 5: Agent Multiply Agent started. Usage: 2 requests, 323 input tokens, 30 output tokens, 353 total tokens\n### 6: Tool multiply_by_two started. Usage: 3 requests, 504 input tokens, 46 output tokens, 550 total tokens\n### 7: Tool multiply_by_two ended with result 202. Usage: 3 requests, 504 input tokens, 46 output tokens, 550 total tokens\n### 8: Agent Multiply Agent ended with output number=202. Usage: 4 requests, 714 input tokens, 63 output tokens, 777 total tokens\nDone!\n\n\"\"\"\n</code></pre>"},{"location":"OpAgentsOlympus/practice/simple_agent/","title":"Simple Agent","text":"Source code in OpAgentsOlympus/practice/simple_agent.py OpAgentsOlympus/practice/simple_agent.py<pre><code>from agents import Agent, Runner\nfrom local_config import config\n\ndef main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"You are a helpful assistant.\",\n    )\n    result = Runner.run_sync(\n        agent, input='hello', run_config=config\n    )\n    print(result.final_output)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"OpAgentsOlympus/practice/stream_items/","title":"Stream Items","text":"Source code in OpAgentsOlympus/practice/stream_items.py OpAgentsOlympus/practice/stream_items.py<pre><code>import asyncio\nimport random\n\nfrom agents import Agent, ItemHelpers, Runner, function_tool\nfrom config import config\n\n\n@function_tool\ndef random_number() -&gt; int:\n    print(\"I was called\")\n    return random.randint(1, 4)\n\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"First call the `random_number` tool which will give you a number, then tell that many jokes.\",\n        tools=[random_number],\n    )\n\n    result = Runner.run_streamed(agent, input=\"tell me jokes\", run_config=config)\n    print(\"=== Run starting ===\")\n    async for event in result.stream_events():\n        # We'll ignore the raw responses event deltas\n        if event.type == \"raw_response_event\":\n            continue\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n            continue\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(\n                    f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\"\n                )\n            else:\n                pass  # Ignore other event types\n\n    print(\"=== Run complete ===\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n    # === Run starting ===\n    # Agent updated: Joker\n    # -- Tool was called\n    # -- Tool output: 4\n    # -- Message output:\n    #  Sure, here are four jokes for you:\n\n    # 1. **Why don't skeletons fight each other?**\n    #    They don't have the guts!\n\n    # 2. **What do you call fake spaghetti?**\n    #    An impasta!\n\n    # 3. **Why did the scarecrow win an award?**\n    #    Because he was outstanding in his field!\n\n    # 4. **Why did the bicycle fall over?**\n    #    Because it was two-tired!\n    # === Run complete ===\n</code></pre>"},{"location":"OpAgentsOlympus/practice/stream_text/","title":"Stream Text","text":"Source code in OpAgentsOlympus/practice/stream_text.py OpAgentsOlympus/practice/stream_text.py<pre><code>import asyncio\n\nfrom openai.types.responses import ResponseTextDeltaEvent\n\nfrom agents import Agent, Runner\nfrom config import config\n\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    result = Runner.run_streamed(\n        agent, input=\"Please tell me 5 jokes.\", run_config=config\n    )\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\" and isinstance(\n            event.data, ResponseTextDeltaEvent\n        ):\n            print(event.data.delta, end=\"\", flush=True)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/subscription_management_application/","title":"Subscription Management Application","text":"Source code in OpAgentsOlympus/practice/subscription_management_application.py OpAgentsOlympus/practice/subscription_management_application.py<pre><code>import os  \nimport asyncio  \nimport asyncpg  \nfrom datetime import datetime  \nfrom dataclasses import dataclass  \nfrom pydantic import BaseModel, Field  \nfrom open_router_config import config\nfrom agents import Agent, Runner, function_tool, SQLiteSession, RunContextWrapper  \n\nconfig.tracing_disabled = False\n# Pydantic models for structured output  \nclass UserEvent(BaseModel):  \n    uid: str  \n    plan: str  \n    timestamp: datetime = Field(default_factory=datetime.now)  \n    action: str  \n\nclass PlanFeatures(BaseModel):  \n    plan_name: str  \n    api_calls_limit: int  2\n    support_level: str  \n    features: list[str]  \n\nclass UsageInfo(BaseModel):  \n    current_usage: int  \n    limit: int  \n    percentage_used: float  \n\n@dataclass  \nclass UserContext:  \n    uid: str  \n    db_pool: asyncpg.Pool  \n    session_id: str  \n\n    async def get_plan(self) -&gt; str:  \n        \"\"\"Get user's subscription plan from database\"\"\"  \n        try:  \n            async with self.db_pool.acquire() as conn:  \n                result = await conn.fetchrow(  \n                    \"SELECT plan_type FROM user_subscriptions WHERE user_id = $1 AND active = true\",   \n                    self.uid  \n                )  \n                return result['plan_type'] if result else \"No active plan\"  \n        except Exception as e:  \n            print(f\"Database error getting plan for user {self.uid}: {e}\")  \n            return \"Error retrieving plan\"  \n\n    async def get_usage(self) -&gt; dict:  \n        \"\"\"Get user's current API usage\"\"\"  \n        try:  \n            async with self.db_pool.acquire() as conn:  \n                result = await conn.fetchrow(  \n                    \"SELECT current_usage, monthly_limit FROM user_usage WHERE user_id = $1\",   \n                    self.uid  \n                )  \n                if result:  \n                    return {  \n                        'current_usage': result['current_usage'],  \n                        'limit': result['monthly_limit'],  \n                        'percentage_used': (result['current_usage'] / result['monthly_limit']) * 100  \n                    }  \n                return {'current_usage': 0, 'limit': 0, 'percentage_used': 0}  \n        except Exception as e:  \n            print(f\"Database error getting usage for user {self.uid}: {e}\")  \n            return {'current_usage': 0, 'limit': 0, 'percentage_used': 0}  \n\n# Function tools with proper error handling  \n@function_tool  \nasync def show_user_plan(context: RunContextWrapper[UserContext]) -&gt; str:  \n    \"\"\"  \n    Get the user's current subscription plan.  \n\n    This tool checks the user's unique ID and returns their current plan from the database.  \n    Always call this function when the response needs to be personalized based on the user's plan.  \n\n    Args:  \n        context: Automatically injected context with user data and database connection.  \n\n    Returns:  \n        A string indicating the user's current plan level.  \n    \"\"\"  \n    plan = await context.context.get_plan()  \n    print(f\"Retrieved plan '{plan}' for user {context.context.uid}\")  \n    return f\"Your current subscription plan is: {plan}\"  \n\n@function_tool  \nasync def get_plan_features(context: RunContextWrapper[UserContext]) -&gt; str:\n    \"\"\"  \n    Get detailed features available for the user's current plan.  \n\n    Returns comprehensive information about what the user can access with their plan.  \n\n    Args:  \n        context: Automatically injected context with user data.  \n\n    Returns:  \n        Detailed feature information for the user's plan.  \n    \"\"\"  \n    plan = await context.context.get_plan()  \n\n    features_map = {  \n        \"Enterprise\": {  \n            \"api_calls\": \"Unlimited\",  \n            \"support\": \"24/7 Priority Support\",  \n            \"features\": [\"Custom integrations\", \"Advanced analytics\", \"Dedicated account manager\", \"SLA guarantee\"]  \n        },  \n        \"Pro\": {  \n            \"api_calls\": \"50,000/month\",  \n            \"support\": \"Email support (24h response)\",  \n            \"features\": [\"Advanced analytics\", \"API access\", \"Custom webhooks\", \"Priority processing\"]  \n        },  \n        \"Basic\": {  \n            \"api_calls\": \"5,000/month\",  \n            \"support\": \"Community support\",  \n            \"features\": [\"Basic analytics\", \"Standard API access\", \"Email notifications\"]  \n        }  \n    }  \n\n    if plan in features_map:  \n        features = features_map[plan]  \n        return f\"\"\"  \nPlan: {plan}  \nAPI Calls: {features['api_calls']}  \nSupport: {features['support']}  \nFeatures: {', '.join(features['features'])}  \n        \"\"\".strip()  \n    else:  \n        return f\"No feature information available for plan: {plan}\"  \n\n@function_tool  \nasync def check_usage_limits(context: RunContextWrapper[UserContext]) -&gt; str:  \n    \"\"\"  \n    Check current usage against plan limits.  \n\n    Provides detailed information about the user's API usage and remaining quota.  \n\n    Args:  \n        context: Automatically injected context with user data.  \n\n    Returns:  \n        Current usage statistics and remaining quota information.  \n    \"\"\"  \n    usage_data = await context.context.get_usage()  \n\n    if usage_data['limit'] == 0:  \n        return \"No usage data available for your account.\"  \n\n    percentage = usage_data['percentage_used']  \n    status = \"Good\" if percentage &lt; 80 else \"Warning\" if percentage &lt; 95 else \"Critical\"  \n\n    return f\"\"\"  \nCurrent Usage: {usage_data['current_usage']:,} API calls  \nMonthly Limit: {usage_data['limit']:,} API calls  \nUsage: {percentage:.1f}% ({status})  \nRemaining: {usage_data['limit'] - usage_data['current_usage']:,} API calls  \n    \"\"\".strip()  \n\n@function_tool  \nasync def upgrade_plan_info(context: RunContextWrapper[UserContext]) -&gt; str:  \n    \"\"\"  \n    Provide information about plan upgrades.  \n\n    Shows available upgrade options and benefits for the user's current plan.  \n\n    Args:  \n        context: Automatically injected context with user data.  \n\n    Returns:  \n        Information about available plan upgrades.  \n    \"\"\"  \n    current_plan = await context.context.get_plan()  \n\n    upgrade_info = {  \n        \"Basic\": \"Upgrade to Pro for 10x more API calls and email support, or Enterprise for unlimited usage.\",  \n        \"Pro\": \"Upgrade to Enterprise for unlimited API calls and 24/7 priority support.\",  \n        \"Enterprise\": \"You're already on our highest tier plan!\",  \n        \"No active plan\": \"Choose from Basic ($9/month), Pro ($49/month), or Enterprise ($199/month).\"  \n    }  \n\n    return upgrade_info.get(current_plan, \"Contact support for upgrade options.\")  \n\n# Database initialization  \nasync def init_database(db_pool: asyncpg.Pool):  \n    \"\"\"Initialize database tables if they don't exist\"\"\"  \n    async with db_pool.acquire() as conn:  \n        await conn.execute(\"\"\"  \n            CREATE TABLE IF NOT EXISTS user_subscriptions (  \n                user_id TEXT PRIMARY KEY,  \n                plan_type TEXT NOT NULL,  \n                active BOOLEAN DEFAULT true,  \n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP  \n            )  \n        \"\"\")  \n\n        await conn.execute(\"\"\"  \n            CREATE TABLE IF NOT EXISTS user_usage (  \n                user_id TEXT PRIMARY KEY,  \n                current_usage INTEGER DEFAULT 0,  \n                monthly_limit INTEGER NOT NULL,  \n                last_reset TIMESTAMP DEFAULT CURRENT_TIMESTAMP  \n            )  \n        \"\"\")  \n\n        # Insert sample data  \n        await conn.execute(\"\"\"  \n            INSERT INTO user_subscriptions (user_id, plan_type)   \n            VALUES ('1', 'Enterprise'), ('2', 'Pro'), ('3', 'Basic')  \n            ON CONFLICT (user_id) DO NOTHING  \n        \"\"\")  \n\n        await conn.execute(\"\"\"  \n            INSERT INTO user_usage (user_id, current_usage, monthly_limit)  \n            VALUES ('1', 15000, 999999), ('2', 8500, 50000), ('3', 2100, 5000)  \n            ON CONFLICT (user_id) DO NOTHING  \n        \"\"\")  \n\n# Authentication and validation  \nasync def authenticate_user(uid: str, db_pool: asyncpg.Pool) -&gt; bool:  \n    \"\"\"Authenticate user exists in database\"\"\"  \n    try:  \n        async with db_pool.acquire() as conn:  \n            result = await conn.fetchrow(  \n                \"SELECT user_id FROM user_subscriptions WHERE user_id = $1\", uid  \n            )  \n            return result is not None  \n    except Exception as e:  \n        print(f\"Authentication error for user {uid}: {e}\")  \n        return False  \n\ndef validate_input(query: str) -&gt; bool:  \n    \"\"\"Validate user input\"\"\"  \n    return len(query.strip()) &gt; 0 and len(query) &lt; 1000  \n\n# Main application class  \nclass SubscriptionAssistantApp:  \n    def __init__(self):  \n        self.db_pool = None  \n        self.config = config\n\n        # Create agent with all tools  \n        self.agent = Agent[UserContext](  \n            name=\"Subscription Assistant\",  \n            instructions=\"\"\"You are a helpful subscription management assistant.   \n            Always use the available tools to get accurate, up-to-date information about the user's account.  \n            Be friendly, informative, and proactive in helping users understand their subscription status.  \n            If users ask about upgrades, provide clear information about benefits.\"\"\",  \n            tools=[show_user_plan, get_plan_features, check_usage_limits, upgrade_plan_info],  \n            # output_type=UserEvent  \n        )  \n\n    async def setup_database(self):  \n        \"\"\"Setup database connection and initialize tables\"\"\"  \n        database_url = os.getenv(\"DATABASE_URL\", \"postgresql://localhost/subscription_app\")  \n        try:  \n            self.db_pool = await asyncpg.create_pool(database_url, min_size=1, max_size=10)  \n            await init_database(self.db_pool)  \n            print(\"Database initialized successfully\")  \n        except Exception as e:  \n            print(f\"Database setup failed: {e}\")  \n            # Fallback to SQLite for demo  \n            print(\"Falling back to in-memory demo mode\")  \n            self.db_pool = None  \n\n    async def run_conversation(self, uid: str, query: str, session: SQLiteSession) -&gt; str:  \n        \"\"\"Run a single conversation turn\"\"\"  \n        if not validate_input(query):  \n            return \"Invalid input. Please provide a valid query.\"  \n\n        # For demo purposes, create a mock pool if database setup failed  \n        if self.db_pool is None:  \n            # Create mock context for demo  \n            user_context = UserContext(uid=uid, db_pool=None, session_id=session.session_id)  \n            # Override get_plan method for demo  \n            async def mock_get_plan():  \n                plans = {'1': 'Enterprise', '2': 'Pro', '3': 'Basic'}  \n                return plans.get(uid, 'No active plan')  \n            user_context.get_plan = mock_get_plan  \n\n            async def mock_get_usage():  \n                usage_data = {  \n                    '1': {'current_usage': 15000, 'limit': 999999, 'percentage_used': 1.5},  \n                    '2': {'current_usage': 8500, 'limit': 50000, 'percentage_used': 17.0},  \n                    '3': {'current_usage': 2100, 'limit': 5000, 'percentage_used': 42.0}  \n                }  \n                return usage_data.get(uid, {'current_usage': 0, 'limit': 0, 'percentage_used': 0})  \n            user_context.get_usage = mock_get_usage  \n        else:  \n            # Authenticate user  \n            if not await authenticate_user(uid, self.db_pool):  \n                return \"User authentication failed. Please check your user ID.\"  \n\n            user_context = UserContext(uid=uid, db_pool=self.db_pool, session_id=session.session_id)  \n\n        try:  \n            result = await Runner.run(  \n                self.agent,  \n                input=query,  \n                context=user_context,  \n                run_config=self.config,  \n                session=session,  \n                max_turns=5  \n            )  \n            return result.final_output  \n        except Exception as e:  \n            print(f\"Agent execution failed: {e}\")  \n            return f\"Sorry, I encountered an error: {str(e)}\"  \n\n    async def start_interactive_session(self):  \n        \"\"\"Start interactive command-line session\"\"\"  \n        await self.setup_database()  \n\n        print(\"\ud83d\ude80 Subscription Assistant Started!\")  \n        print(\"Type 'quit' to exit, 'help' for commands\")  \n        print(\"-\" * 50)  \n\n        current_uid = None  \n        session = None  \n\n        try:  \n            while True:  \n                if current_uid is None:  \n                    uid = input(\"\\n\ud83d\udc64 Enter your User ID (1, 2, or 3 for demo): \").strip()  \n                    if uid.lower() == 'quit':  \n                        break  \n                    if uid in ['1', '2', '3']:  \n                        current_uid = uid  \n                        session = SQLiteSession(f\"user_{uid}_session\")  \n                        print(f\"\u2705 Logged in as User {uid}\")  \n                        continue  \n                    else:  \n                        print(\"\u274c Invalid User ID. Use 1, 2, or 3 for demo.\")  \n                        continue  \n\n                query = input(f\"\\n\ud83d\udcac User {current_uid}: \").strip()  \n\n                if query.lower() == 'quit':  \n                    break  \n                elif query.lower() == 'help':  \n                    print(\"\"\"  \nAvailable commands:  \n- Ask about your subscription plan  \n- Check usage limits    \n- Get plan features  \n- Ask about upgrades  \n- 'logout' to switch users  \n- 'quit' to exit  \n                    \"\"\")  \n                    continue  \n                elif query.lower() == 'logout':  \n                    current_uid = None  \n                    session = None  \n                    print(\"\ud83d\udc4b Logged out\")  \n                    continue  \n                elif not query:  \n                    continue  \n\n                print(\"\ud83e\udd16 Assistant: \", end=\"\")  \n                response = await self.run_conversation(current_uid, query, session)  \n                print(response)  \n\n        except KeyboardInterrupt:  \n            print(\"\\n\ud83d\udc4b Goodbye!\")  \n        finally:  \n            if self.db_pool:  \n                await self.db_pool.close()  \n\n# Entry point  \nasync def main():  \n    \"\"\"Main application entry point\"\"\"  \n    app = SubscriptionAssistantApp()  \n    await app.start_interactive_session()  \n\nif __name__ == '__main__':      \n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/test_run_config_inheritance/","title":"Test Run Config Inheritance","text":"Source code in OpAgentsOlympus/practice/test_run_config_inheritance.py OpAgentsOlympus/practice/test_run_config_inheritance.py<pre><code>from __future__ import annotations\n\nimport pytest\nfrom typing import cast\n\nfrom agents import Agent, RunConfig, Runner\nfrom agents.tool import function_tool\nfrom agents.run import (\n    get_current_run_config,\n    set_current_run_config,\n    reset_current_run_config,\n)\n\nfrom .fake_model import FakeModel\nfrom .test_responses import get_function_tool_call, get_text_message\n\n\n@pytest.mark.asyncio\nasync def test_run_config_inheritance_enabled():\n    \"\"\"Test that run_config is inherited when pass_run_config_to_sub_agents=True\"\"\"\n    inherited_configs = []\n\n    @function_tool\n    async def config_capture_tool() -&gt; str:\n        \"\"\"Tool that captures the current run config\"\"\"\n        current_config = get_current_run_config()\n        inherited_configs.append(current_config)\n        return \"config_captured\"\n\n    sub_agent = Agent(\n        name=\"SubAgent\",\n        instructions=\"You are a sub agent\",\n        model=FakeModel(),\n        tools=[config_capture_tool],\n    )\n\n    sub_fake_model = cast(FakeModel, sub_agent.model)\n    sub_fake_model.add_multiple_turn_outputs(\n        [\n            [get_function_tool_call(\"config_capture_tool\", \"{}\")],\n            [get_text_message(\"sub_agent_response\")],\n        ]\n    )\n\n    parent_agent = Agent(\n        name=\"ParentAgent\",\n        instructions=\"You are a parent agent\",\n        model=FakeModel(),\n        tools=[\n            sub_agent.as_tool(\n                tool_name=\"sub_agent_tool\", tool_description=\"Call the sub agent\"\n            )\n        ],\n    )\n\n    parent_fake_model = cast(FakeModel, parent_agent.model)\n    parent_fake_model.add_multiple_turn_outputs(\n        [\n            [get_function_tool_call(\"sub_agent_tool\", '{\"input\": \"test\"}')],\n            [get_text_message(\"parent_response\")],\n        ]\n    )\n\n    run_config = RunConfig(pass_run_config_to_sub_agents=True)\n\n    assert get_current_run_config() is None\n\n    await Runner.run(\n        starting_agent=parent_agent,\n        input=\"Use the sub agent tool\",\n        run_config=run_config,\n    )\n\n    assert get_current_run_config() is None\n    assert len(inherited_configs) == 1\n    assert inherited_configs[0] is run_config\n    assert inherited_configs[0].pass_run_config_to_sub_agents is True\n\n\n@pytest.mark.asyncio\nasync def test_run_config_inheritance_disabled():\n    \"\"\"Test that run_config is not inherited when pass_run_config_to_sub_agents=False\"\"\"\n    inherited_configs = []\n\n    @function_tool\n    async def config_capture_tool() -&gt; str:\n        \"\"\"Tool that captures the current run config\"\"\"\n        current_config = get_current_run_config()\n        inherited_configs.append(current_config)\n        return \"config_captured\"\n\n    sub_agent = Agent(\n        name=\"SubAgent\",\n        instructions=\"You are a sub agent\",\n        model=FakeModel(),\n        tools=[config_capture_tool],\n    )\n\n    sub_fake_model = cast(FakeModel, sub_agent.model)\n    sub_fake_model.add_multiple_turn_outputs(\n        [\n            [get_function_tool_call(\"config_capture_tool\", \"{}\")],\n            [get_text_message(\"sub_agent_response\")],\n        ]\n    )\n\n    parent_agent = Agent(\n        name=\"ParentAgent\",\n        instructions=\"You are a parent agent\",\n        model=FakeModel(),\n        tools=[\n            sub_agent.as_tool(\n                tool_name=\"sub_agent_tool\", tool_description=\"Call the sub agent\"\n            )\n        ],\n    )\n\n    parent_fake_model = cast(FakeModel, parent_agent.model)\n    parent_fake_model.add_multiple_turn_outputs(\n        [\n            [get_function_tool_call(\"sub_agent_tool\", '{\"input\": \"test\"}')],\n            [get_text_message(\"parent_response\")],\n        ]\n    )\n\n    run_config = RunConfig()\n\n    await Runner.run(\n        starting_agent=parent_agent,\n        input=\"Use the sub agent tool\",\n        run_config=run_config,\n    )\n\n    assert get_current_run_config() is None\n    assert len(inherited_configs) == 1\n    assert inherited_configs[0] is None\n\n\n@pytest.mark.asyncio\nasync def test_context_variable_cleanup_on_error():\n    \"\"\"Test that context variable is cleaned up even when errors occur\"\"\"\n    failing_model = FakeModel()\n    failing_model.set_next_output(RuntimeError(\"Intentional test failure\"))\n\n    failing_agent = Agent(\n        name=\"FailingAgent\",\n        instructions=\"Fail\",\n        model=failing_model,\n    )\n\n    run_config = RunConfig(pass_run_config_to_sub_agents=True)\n\n    assert get_current_run_config() is None\n\n    with pytest.raises(RuntimeError, match=\"Intentional test failure\"):\n        await Runner.run(\n            starting_agent=failing_agent,\n            input=\"This should fail\",\n            run_config=run_config,\n        )\n\n    assert get_current_run_config() is None\n\n\n@pytest.mark.asyncio\nasync def test_scope_methods_directly():\n    \"\"\"Test the Scope class methods directly for RunConfig management\"\"\"\n    run_config = RunConfig(pass_run_config_to_sub_agents=True)\n\n    assert get_current_run_config() is None\n\n    token = set_current_run_config(run_config)\n    assert get_current_run_config() is run_config\n\n    reset_current_run_config(token)\n    assert get_current_run_config() is None\n\n    token = set_current_run_config(None)\n    assert get_current_run_config() is None\n    reset_current_run_config(token)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/testing/","title":"Testing","text":"Source code in OpAgentsOlympus/practice/testing.py OpAgentsOlympus/practice/testing.py<pre><code>from agents import (\n    Agent,\n    Runner,\n    set_tracing_disabled,\n    OpenAIChatCompletionsModel,\n    function_tool,\n    RunHooks,\n    AgentHooks,\n    RunContextWrapper,\n    TContext,\n    Tool,\n    RunConfig,\n)\nfrom typing import Any\nimport os\nfrom openai import AsyncOpenAI\nimport dotenv\nimport asyncio\n\ndotenv.load_dotenv()\nset_tracing_disabled(True)\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n\nif not GEMINI_API_KEY:\n    raise ValueError(\"API key not found!!\")\nclient = AsyncOpenAI(\n    api_key=GEMINI_API_KEY,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\nmodel = OpenAIChatCompletionsModel(model=\"gemini-1.5-pro\", openai_client=client)\n\n\nclass Run_Hooks(RunHooks):\n    \"\"\"A class that receives callbacks on various lifecycle events in an agent run. Subclass and\n    override the methods you need.\n    \"\"\"\n\n    async def on_agent_start(\n        self, context: RunContextWrapper[TContext], agent: Agent[TContext]\n    ) -&gt; None:\n        \"\"\"Called before the agent is invoked. Called each time the current agent changes.\"\"\"\n        print(\"Parent Runner: on_agent_start called...\")\n\n    async def on_agent_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Called when the agent produces a final output.\"\"\"\n        print(\"Parent Runner: on_agent_end called...\")\n\n    async def on_handoff(\n        self,\n        context: RunContextWrapper[TContext],\n        from_agent: Agent[TContext],\n        to_agent: Agent[TContext],\n    ) -&gt; None:\n        \"\"\"Called when a handoff occurs.\"\"\"\n        print(\"Parent Runner: on_handoff called...\")\n\n    async def on_tool_start(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        tool: Tool,\n    ) -&gt; None:\n        \"\"\"Called before a tool is invoked.\"\"\"\n        print(\"Parent Runner: on_tool_start called...\")\n\n    async def on_tool_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        tool: Tool,\n        result: str,\n    ) -&gt; None:\n        \"\"\"Called after a tool is invoked.\"\"\"\n        print(\"Parent Runner: on_tool_end called...\")\n\n\nclass Agent_Hooks(AgentHooks):\n    \"\"\"A class that receives callbacks on various lifecycle events for a specific agent. You can\n    set this on `agent.hooks` to receive events for that specific agent.\n\n    Subclass and override the methods you need.\n    \"\"\"\n\n    async def on_start(\n        self, context: RunContextWrapper[TContext], agent: Agent[TContext]\n    ) -&gt; None:\n        \"\"\"Called before the agent is invoked. Called each time the running agent is changed to this\n        agent.\"\"\"\n        print(\"Parent Agent: on_start called...\")\n\n    async def on_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Called when the agent produces a final output.\"\"\"\n        print(\"Parent Agent: on_end called...\")\n\n    async def on_handoff(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        source: Agent[TContext],\n    ) -&gt; None:\n        \"\"\"Called when the agent is being handed off to. The `source` is the agent that is handing\n        off to this agent.\"\"\"\n        print(\"Parent Agent: on_handoff called...\")\n\n    async def on_tool_start(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        tool: Tool,\n    ) -&gt; None:\n        \"\"\"Called before a tool is invoked.\"\"\"\n        print(\"Parent Agent: on_tool_start called...\")\n\n    async def on_tool_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        tool: Tool,\n        result: str,\n    ) -&gt; None:\n        \"\"\"Called after a tool is invoked.\"\"\"\n        print(\"Parent Agent: on_tool_end called...\")\n\n\nclass Child_Agent_Hooks(AgentHooks):\n    \"\"\"A class that receives callbacks on various lifecycle events for a specific agent. You can\n    set this on `agent.hooks` to receive events for that specific agent.\n\n    Subclass and override the methods you need.\n    \"\"\"\n\n    async def on_start(\n        self, context: RunContextWrapper[TContext], agent: Agent[TContext]\n    ) -&gt; None:\n        \"\"\"Called before the agent is invoked. Called each time the running agent is changed to this\n        agent.\"\"\"\n        print(\"Child Agent: on_start called...\")\n\n    async def on_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        output: Any,\n    ) -&gt; None:\n        \"\"\"Called when the agent produces a final output.\"\"\"\n        print(\"Child Agent: on_end called...\")\n\n    async def on_handoff(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        source: Agent[TContext],\n    ) -&gt; None:\n        \"\"\"Called when the agent is being handed off to. The `source` is the agent that is handing\n        off to this agent.\"\"\"\n        print(\"Child Agent: on_handoff called...\")\n\n    async def on_tool_start(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        tool: Tool,\n    ) -&gt; None:\n        \"\"\"Called before a tool is invoked.\"\"\"\n        print(\"Child Agent: on_tool_start called...\")\n\n    async def on_tool_end(\n        self,\n        context: RunContextWrapper[TContext],\n        agent: Agent[TContext],\n        tool: Tool,\n        result: str,\n    ) -&gt; None:\n        \"\"\"Called after a tool is invoked.\"\"\"\n        print(\"Child Agent: on_tool_end called...\")\n\n\nconfig = RunConfig(model=model, model_provider=client)\n\n\n@function_tool\ndef hello():\n    return \"Hello! from Say Hello Agent/Tool\"\n\n\nsay_hello_agent = Agent(\n    name=\"say_hello_agent\",\n    instructions=\"You are a say hello agent, You use tools to respond.\",\n    tools=[hello],\n    # model=model,\n    hooks=Child_Agent_Hooks(),\n)\n\nagent = Agent(\n    name=\"agent\",\n    instructions=\"You are a friendly assistant, You use tools to respond.\",\n    tools=[say_hello_agent.as_tool(\"say_hello_tool\", \"A say hello tool\")],\n    # model=model,\n    hooks=Agent_Hooks(),\n)\n\n\nasync def main():\n    result = await Runner.run(agent, \"Say Hello!\", hooks=Run_Hooks(), run_config=config)\n    print(result.final_output)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/testing2222/","title":"Testing2222","text":"Source code in OpAgentsOlympus/practice/testing2222.py OpAgentsOlympus/practice/testing2222.py<pre><code>from dotenv import load_dotenv\nfrom agents import (\n    Agent,\n    Runner,\n    function_tool,\n    RunConfig,\n    AsyncOpenAI,\n    OpenAIChatCompletionsModel,\n)\n\nload_dotenv()\nimport os\n\n# Load environment variables\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"GEMINI_API_KEY is not set. Please define it in your .env file.\")\n\n# Setup Gemini client\nexternal_client = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\n# Preferred Gemini model setup\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-2.0-flash\", openai_client=external_client\n)\n\n# Runner config (you can export this)\nconfig = RunConfig(\n    model=model,\n    model_provider=external_client,\n    # tracing_disabled=True\n)\n\n\n# --------------------------------------------------------\n# Enhanced Tool Functions\n@function_tool\ndef get_weather_info(city: str = \"Karachi\") -&gt; str:\n    \"\"\"Get detailed weather information for a specific city\"\"\"\n    weather_data = {\n        \"Karachi\": \"Temperature: 40\u00b0C, Condition: Sunny, Humidity: 65%, Wind: 15 km/h\",\n        \"Lahore\": \"Temperature: 35\u00b0C, Condition: Partly Cloudy, Humidity: 70%, Wind: 10 km/h\",\n        \"Islamabad\": \"Temperature: 28\u00b0C, Condition: Clear, Humidity: 55%, Wind: 8 km/h\",\n    }\n    return weather_data.get(city, f\"Weather data not available for {city}\")\n\n\n@function_tool\ndef get_location_info() -&gt; str:\n    \"\"\"Get current location and timezone information\"\"\"\n    return \"Location: Karachi, Pakistan | Timezone: PKT (UTC+5) | Coordinates: 24.8607\u00b0 N, 67.0011\u00b0 E\"\n\n\n@function_tool\ndef get_medical_info(topic: str) -&gt; str:\n    \"\"\"Provide general medical information about a specific topic (educational purposes only)\"\"\"\n    medical_info = {\n        \"diabetes\": \"Diabetes is a chronic condition affecting blood sugar levels. Common symptoms include increased thirst, frequent urination, and fatigue. Always consult a healthcare provider for diagnosis and treatment.\",\n        \"hypertension\": \"Hypertension (high blood pressure) is a common condition that can lead to heart disease. Symptoms may include headaches, shortness of breath, and nosebleeds. Regular check-ups are important.\",\n        \"asthma\": \"Asthma is a respiratory condition causing airway inflammation. Symptoms include wheezing, coughing, and chest tightness. Avoid triggers and follow prescribed treatment plans.\",\n        \"headache\": \"Headaches can have various causes including stress, dehydration, or underlying conditions. Rest, hydration, and pain relievers may help. Seek medical attention for severe or persistent headaches.\",\n    }\n    return medical_info.get(\n        topic.lower(),\n        f\"General information about {topic}: This is for educational purposes only. Please consult a healthcare professional for medical advice.\",\n    )\n\n\n@function_tool\ndef get_general_knowledge(topic: str) -&gt; str:\n    \"\"\"Provide general knowledge information about various topics\"\"\"\n    knowledge_base = {\n        \"photosynthesis\": \"Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into glucose and oxygen. This process is essential for life on Earth as it produces oxygen and serves as the foundation of the food chain.\",\n        \"gravity\": \"Gravity is a fundamental force that attracts objects with mass toward each other. On Earth, it's approximately 9.8 m/s\u00b2 and keeps us grounded to the planet's surface.\",\n        \"solar_system\": \"The Solar System consists of the Sun and the objects that orbit it, including 8 planets, dwarf planets, moons, asteroids, and comets. Earth is the third planet from the Sun.\",\n        \"internet\": \"The Internet is a global network of connected computers that allows information sharing and communication worldwide. It was developed from ARPANET in the 1960s and has revolutionized modern communication.\",\n    }\n    return knowledge_base.get(\n        topic.lower(),\n        f\"General information about {topic}: This is a broad topic that covers various aspects. For specific information, please provide more details.\",\n    )\n\n\n# ---------------------------------------------------------------\n# Specialized Agents\nmedicine_agent = Agent(\n    model=config.model,\n    name=\"medical-info-agent\",\n    instructions=(\n        \"You are a medical information assistant. \"\n        \"You can provide general information about health topics, but do not give medical advice. \"\n        \"Always recommend consulting healthcare professionals for medical decisions. \"\n        \"Use the get_medical_info tool to provide educational information about health topics.\"\n    ),\n    tools=[get_medical_info],\n)\n\nweather_agent = Agent(\n    model=config.model,\n    name=\"weather-agent\",\n    instructions=(\n        \"You are a weather information assistant. \"\n        \"Use the get_weather_info and get_location_info tools to provide comprehensive weather information. \"\n        \"Always provide current weather data and location context when available.\"\n    ),\n    tools=[get_weather_info, get_location_info],\n)\n\ngeneral_knowledge_agent = Agent(\n    model=config.model,\n    name=\"general-knowledge-agent\",\n    instructions=(\n        \"You are a general knowledge assistant. \"\n        \"You can answer questions about various topics including science, history, geography, and general facts. \"\n        \"Use the get_general_knowledge tool to provide accurate information.\"\n    ),\n    tools=[get_general_knowledge],\n)\n\n# --------------------------------------------------------\n# Main Coordinator Agent\ncoordinator_agent = Agent(\n    model=config.model,\n    name=\"coordinator-agent\",\n    instructions=(\n        \"You are a smart coordinator that routes user queries to the most appropriate specialized agent. \"\n        \"Your job is to analyze the user's query and determine which agent should handle it:\\n\"\n        \"- For weather-related questions (weather, temperature, climate, location), route to weather_agent\\n\"\n        \"- For medical/health questions (symptoms, diseases, medications, health advice), route to medicine_agent\\n\"\n        \"- For general knowledge questions (science, history, facts, explanations), route to general_knowledge_agent\\n\"\n        \"- For other queries, answer directly if you can, or route to the most appropriate agent\\n\"\n        \"Always explain which agent you're routing to and why.\"\n    ),\n    handoffs=[weather_agent, medicine_agent, general_knowledge_agent],\n)\n\n# -----------------------------------------------------------------------\n# Interactive Query System\nquery = input(\"Enter your query: \")\n\nresult = Runner.run_sync(\n    coordinator_agent,\n    query,\n    # run_config=config\n)\nprint(f\"\ud83c\udfaf Agent Used: {result.last_agent.name}\")\nprint(f\"\ud83d\udcdd Response: {result.final_output}\")\nprint(\"=\" * 60)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/tool_call_error/","title":"Tool Call Error","text":"Source code in OpAgentsOlympus/practice/tool_call_error.py OpAgentsOlympus/practice/tool_call_error.py<pre><code>from agents import RunContextWrapper, Agent, Runner, function_tool\nfrom typing import Any\nfrom config import config\n\n\ndef custom_error_handler(ctx: RunContextWrapper[Any], error: Exception) -&gt; str:\n    print(\"Error occurred\")\n    return f\"Error: Tool failed with {error.__class__.__name__}: {str(error)}\"\n\n\n@function_tool(failure_error_function=custom_error_handler)\ndef multiply_by_2(x: int) -&gt; int:\n    raise ValueError()\n    print(f\"Multiplying {x} by 2\")\n    return x * 2\n\n\nassistant = Agent(\n    name=\"assistant\",\n    instructions=\"You are a friendly assistant.\",\n    tools=[multiply_by_2],\n)\n\nresult = Runner.run_sync(assistant, \"multiply 4 with 2\", run_config=config)\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/tool_calling_error/","title":"Tool Calling Error","text":"Source code in OpAgentsOlympus/practice/tool_calling_error.py OpAgentsOlympus/practice/tool_calling_error.py<pre><code>from agents import AsyncOpenAI, OpenAIChatCompletionsModel, Agent, Runner, function_tool, set_tracing_disabled, ModelSettings\nimport os\nimport dotenv\n\nset_tracing_disabled(True)\ndotenv.load_dotenv()\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\nclient = AsyncOpenAI(\n    api_key=GEMINI_API_KEY,\n    base_url='https://generativelanguage.googleapis.com/v1beta/openai',\n)\n\nmodel = OpenAIChatCompletionsModel(\n    'gemini-2.0-flash',\n    openai_client=client\n)\n\n@function_tool(name_override='add_tool')\ndef add(a: int, b: int) -&gt; int:\n    print(\"Adding...\")\n    return a + b\n\n@function_tool\ndef subtract(a: int, b: int) -&gt; int:\n    print(\"Subtracting...\")\n    return a - b\n\nassistant = Agent(\n    name='assistant',\n    instructions='You are a helpful assistant.',\n    tools=[add, subtract],\n    model=model,\n    model_settings=ModelSettings(\n        tool_choice='subtract'\n    )\n)\n\nresult = Runner.run_sync(assistant, 'Hello')\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/tool_choice/","title":"Tool Choice","text":"Source code in OpAgentsOlympus/practice/tool_choice.py OpAgentsOlympus/practice/tool_choice.py<pre><code>from openai import AsyncOpenAI\nfrom agents import (\n    Agent,\n    Runner,\n    OpenAIChatCompletionsModel,\n    RunConfig,\n    ModelSettings,\n    function_tool\n)\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"I guess you haven't set API KEY, I'am pretty sure you need to set it dude.\")\n\nclient = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-2.0-flash\", openai_client=client\n)\n\nconfig = RunConfig(model=model)\n\n@function_tool\ndef am_i_coder():\n    return 'Maybe \ud83e\udd37\u200d\u2642\ufe0f'\n\n@function_tool\ndef do_i_deserve_it():\n    return 'Maybe \ud83e\udd37\u200d\u2642\ufe0f'\n\nagent = Agent[None](\n    name=\"assistant\",\n    instructions=\"You are an amazing assistant, You only respond in haikus LOL MUST CALL A TOOL\",\n    model_settings=ModelSettings(\n        tool_choice='do_i_deserve_it'\n    ),\n    tools=[am_i_coder, do_i_deserve_it]\n)\n\nresult = Runner.run_sync(\n    agent,\n    \"Am I a CODER?\",\n    run_config=config,\n)\n\nprint(result.final_output)\n\n# tool_choice = \"none\"  &lt;-- The tools will be hidden from the LLM.\n# Lines of code you write,\n# Logic flows from your own mind,\n# Coder, you may be. &lt;----------- Don't be confused with this \"may be\", Trust me the LLM didn't even saw the tools.\n\n# tool_choice = None  &lt;-- The behavior depends on the LLM provider's defaults.\n# Perhaps you code well,\n# Or maybe just a little, &lt;------- Tool was called!\n# The tool is unsure.\n\n# Difference between None and \"none\"\n# tool_choice=None \u2192 returns NOT_GIVEN\n# tool_choice='none' \u2192 returns \"none\"\n\n# Difference between None and \"auto\"\n# tool_choice=None \u2192 Uses whatever default the LLM provider has configured\n# tool_choice=\"auto\" \u2192 Explicitly tells the LLM it can decide whether to use tools or not (Default)\n\n# tool_choice='required' \u2192 Must call a tool\n# tool_choice='tool_name' \u2192 Must call that tool\n</code></pre>"},{"location":"OpAgentsOlympus/practice/tool_choice_with_tool_name_set/","title":"Tool Choice With Tool Name Set","text":"Source code in OpAgentsOlympus/practice/tool_choice_with_tool_name_set.py OpAgentsOlympus/practice/tool_choice_with_tool_name_set.py<pre><code>from config import config\nfrom agents import Agent, Runner, function_tool, ModelSettings, RunContextWrapper\n\n\n@function_tool\ndef say_hello() -&gt; int:\n    return 'Hello, Guys!'\n\n@function_tool\ndef say_bye(ctx: RunContextWrapper) -&gt; int:\n    return 'Bye, Guys!'\n\ndef main():\n    assistant = Agent(\n        name=\"assistant\",\n        instructions='123',\n        model_settings=ModelSettings(\n            tool_choice='say_bye'\n        ),\n        tools=[say_bye, say_hello]\n    )\n    result = Runner.run_sync(\n        assistant, input='hello', run_config=config\n    )\n    print(result.final_output)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"OpAgentsOlympus/practice/tool_use_behavior_set_to_none/","title":"Tool Use Behavior Set To None","text":"Source code in OpAgentsOlympus/practice/tool_use_behavior_set_to_none.py OpAgentsOlympus/practice/tool_use_behavior_set_to_none.py<pre><code>from agents import Agent, Runner\nfrom config import config\n\ndef main():\n    agent = Agent(\n        name=\"assistant\",\n        instructions=\"You are a helpful assistant.\",\n        tool_use_behavior=None # TypeError: Agent tool_use_behavior must be 'run_llm_again', 'stop_on_first_tool', StopAtTools dict, or callable, got NoneType\n    )\n    result = Runner.run_sync(\n        agent, input='hello', run_config=config\n    )\n    print(result.final_output)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"OpAgentsOlympus/practice/tools/","title":"Tools","text":"Source code in OpAgentsOlympus/practice/tools.py OpAgentsOlympus/practice/tools.py<pre><code>import asyncio\n\nfrom pydantic import BaseModel\n\nfrom agents import Agent, Runner, function_tool\nfrom config import config\n\n\nclass Weather(BaseModel):\n    city: str\n    temperature_range: str\n    conditions: str\n\n\n@function_tool\ndef get_weather(city: str) -&gt; Weather:\n    print(\"[debug] get_weather called\")\n    return Weather(city=city, temperature_range=\"14-20C\", conditions=\"Sunny with wind.\")\n\n\nAssistant = Agent(\n    name=\"Assistant\",\n    instructions=\"You are a helpful Assistant.\",\n    tools=[get_weather],\n)\n\n\nasync def main():\n    result = await Runner.run(\n        Assistant, input=\"What's the weather in Tokyo?\", run_config=config\n    )\n    print(result.final_output)\n    # The weather in Tokyo is sunny.\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/top_p/","title":"Top P","text":"Source code in OpAgentsOlympus/practice/top_p.py OpAgentsOlympus/practice/top_p.py<pre><code>import os\nimport dotenv\nfrom openai import AsyncOpenAI\nfrom agents import (\n    Agent,\n    Runner,\n    OpenAIChatCompletionsModel,\n    RunConfig,\n    ModelSettings,\n)\n\ndotenv.load_dotenv()\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"I guess you haven't set API KEY, I'am pretty sure you need to set it dude.\")\n\nclient = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-2.0-flash\", openai_client=client\n)\n\nconfig = RunConfig(model=model)\n\nagent = Agent[None](\n    name=\"assistant\",\n    instructions=\"You are a bro agent\",\n    model_settings=ModelSettings(\n        top_p=0.1,\n        max_tokens=50,\n    ),\n)\n\nresult = Runner.run_sync(\n    agent,\n    \"What is an apple?\",\n    run_config=config,\n)\n\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/tracing_in_console/","title":"Tracing In Console","text":"Source code in OpAgentsOlympus/practice/tracing_in_console.py OpAgentsOlympus/practice/tracing_in_console.py<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom rich.console import Console\nfrom rich.tree import Tree\nfrom rich.panel import Panel\n\nfrom openai.types.responses import ResponseTextDeltaEvent\nfrom agents import Agent, Runner\nfrom config import config\nfrom agents.tracing import set_trace_processors\nfrom agents.tracing.processors import BatchTraceProcessor, TracingExporter\nfrom agents.tracing.spans import Span\nfrom agents.tracing.traces import Trace\nfrom typing import Any\n\n\nclass ComprehensiveRichConsoleSpanExporter(TracingExporter):\n    \"\"\"Prints traces and spans to console with rich formatting, showing all data like the OpenAI dashboard.\"\"\"\n\n    def __init__(self):\n        self.console = Console()\n        self.span_hierarchy = {}  # Track parent-child relationships\n\n    def export(self, items: list[Trace | Span[Any]]) -&gt; None:\n        for item in items:\n            if isinstance(item, Trace):\n                self._export_trace(item)\n            else:\n                self._export_span(item)\n\n    def _export_trace(self, trace: Trace):\n        trace_data = trace.export()\n        if not trace_data:\n            return\n\n        # Format trace information like the dashboard\n        trace_info = []\n        trace_info.append(\n            f\"[bold blue]Trace ID:[/bold blue] {trace_data.get('id', 'N/A')}\"\n        )\n        trace_info.append(\n            f\"[bold blue]Name:[/bold blue] {trace_data.get('workflow_name', 'N/A')}\"\n        )\n\n        if trace_data.get(\"group_id\"):\n            trace_info.append(\n                f\"[bold blue]Group ID:[/bold blue] {trace_data['group_id']}\"\n            )\n\n        if trace_data.get(\"metadata\"):\n            trace_info.append(\n                f\"[bold blue]Metadata:[/bold blue] {json.dumps(trace_data['metadata'], indent=2)}\"\n            )\n\n        panel = Panel(\n            \"\\n\".join(trace_info), title=\"\ud83d\udd0d Trace Started\", border_style=\"blue\"\n        )\n        self.console.print(panel)\n\n    def _export_span(self, span: Span[Any]):\n        span_data = span.export()\n        if not span_data:\n            return\n\n        # Extract all span information\n        span_id = span_data.get(\"id\", \"N/A\")\n        parent_id = span_data.get(\"parent_id\")\n        trace_id = span_data.get(\"trace_id\", \"N/A\")\n        started_at = span_data.get(\"started_at\")\n        ended_at = span_data.get(\"ended_at\")\n\n        # Calculate duration\n        duration = None\n        if started_at and ended_at:\n            try:\n                start_time = datetime.fromisoformat(started_at.replace(\"Z\", \"+00:00\"))\n                end_time = datetime.fromisoformat(ended_at.replace(\"Z\", \"+00:00\"))\n                duration = (end_time - start_time).total_seconds()\n            except:\n                duration = None\n\n        # Get span type and data\n        span_details = span_data.get(\"span_data\", {})\n        span_type = span_details.get(\"type\", \"unknown\")\n\n        # Create main tree with span type\n        tree = Tree(f\"[bold green]Span:[/bold green] {span_type}\")\n\n        # Add core span metadata\n        tree.add(f\"[cyan]Span ID:[/cyan] {span_id}\")\n        tree.add(f\"[cyan]Trace ID:[/cyan] {trace_id}\")\n        if parent_id:\n            tree.add(f\"[cyan]Parent ID:[/cyan] {parent_id}\")\n\n        # Add timing information\n        if started_at:\n            tree.add(f\"[cyan]Started At:[/cyan] {started_at}\")\n        if ended_at:\n            tree.add(f\"[cyan]Ended At:[/cyan] {ended_at}\")\n        if duration is not None:\n            tree.add(f\"[cyan]Duration:[/cyan] {duration:.3f}s\")\n\n        # Add error information if present\n        if span_data.get(\"error\"):\n            error_info = span_data[\"error\"]\n            error_tree = tree.add(\"[red]Error:[/red]\")\n            error_tree.add(f\"[red]Message:[/red] {error_info.get('message', 'N/A')}\")\n            if error_info.get(\"data\"):\n                error_tree.add(\n                    f\"[red]Data:[/red] {json.dumps(error_info['data'], indent=2)}\"\n                )\n\n        # Add span-specific data based on type\n        self._add_span_type_data(tree, span_type, span_details)\n\n        self.console.print(tree)\n        self.console.print()  # Add spacing between spans\n\n    def _add_span_type_data(self, tree: Tree, span_type: str, span_details: dict):\n        \"\"\"Add type-specific span data like the OpenAI dashboard shows.\"\"\"\n\n        if span_type == \"generation\":\n            self._add_generation_data(tree, span_details)\n        elif span_type == \"agent\":\n            self._add_agent_data(tree, span_details)\n        elif span_type == \"function\":\n            self._add_function_data(tree, span_details)\n        elif span_type == \"handoff\":\n            self._add_handoff_data(tree, span_details)\n        elif span_type == \"guardrail\":\n            self._add_guardrail_data(tree, span_details)\n        elif span_type == \"transcription\":\n            self._add_transcription_data(tree, span_details)\n        elif span_type == \"speech\":\n            self._add_speech_data(tree, span_details)\n        elif span_type == \"custom\":\n            self._add_custom_data(tree, span_details)\n        else:\n            # Generic data display for unknown types\n            for key, value in span_details.items():\n                if key != \"type\":\n                    tree.add(f\"[yellow]{key}:[/yellow] {self._format_value(value)}\")\n\n    def _add_generation_data(self, tree: Tree, data: dict):\n        \"\"\"Add generation span data.\"\"\"\n        if data.get(\"model\"):\n            tree.add(f\"[yellow]Model:[/yellow] {data['model']}\")\n\n        if data.get(\"input\"):\n            input_tree = tree.add(\"[yellow]Input Messages:[/yellow]\")\n            for i, msg in enumerate(data[\"input\"]):\n                msg_tree = input_tree.add(f\"Message {i + 1}\")\n                msg_tree.add(f\"Role: {msg.get('role', 'N/A')}\")\n                content = msg.get(\"content\", \"\")\n                if len(content) &gt; 100:\n                    content = content[:100] + \"...\"\n                msg_tree.add(f\"Content: {content}\")\n\n        if data.get(\"output\"):\n            output_tree = tree.add(\"[yellow]Output Messages:[/yellow]\")\n            for i, msg in enumerate(data[\"output\"]):\n                msg_tree = output_tree.add(f\"Message {i + 1}\")\n                if isinstance(msg, dict):\n                    for key, value in msg.items():\n                        if key == \"content\" and len(str(value)) &gt; 100:\n                            value = str(value)[:100] + \"...\"\n                        msg_tree.add(f\"{key}: {value}\")\n\n        if data.get(\"model_config\"):\n            config_tree = tree.add(\"[yellow]Model Config:[/yellow]\")\n            for key, value in data[\"model_config\"].items():\n                config_tree.add(f\"{key}: {value}\")\n\n        if data.get(\"usage\"):\n            usage_tree = tree.add(\"[yellow]Usage:[/yellow]\")\n            for key, value in data[\"usage\"].items():\n                usage_tree.add(f\"{key}: {value}\")\n\n    def _add_agent_data(self, tree: Tree, data: dict):\n        \"\"\"Add agent span data.\"\"\"\n        if data.get(\"name\"):\n            tree.add(f\"[yellow]Agent Name:[/yellow] {data['name']}\")\n\n        if data.get(\"handoffs\"):\n            handoffs_tree = tree.add(\"[yellow]Available Handoffs:[/yellow]\")\n            for handoff in data[\"handoffs\"]:\n                handoffs_tree.add(f\"\u2022 {handoff}\")\n\n        if data.get(\"tools\"):\n            tools_tree = tree.add(\"[yellow]Available Tools:[/yellow]\")\n            for tool in data[\"tools\"]:\n                tools_tree.add(f\"\u2022 {tool}\")\n\n        if data.get(\"output_type\"):\n            tree.add(f\"[yellow]Output Type:[/yellow] {data['output_type']}\")\n\n    def _add_function_data(self, tree: Tree, data: dict):\n        \"\"\"Add function span data.\"\"\"\n        if data.get(\"name\"):\n            tree.add(f\"[yellow]Function Name:[/yellow] {data['name']}\")\n\n        if data.get(\"input\"):\n            tree.add(f\"[yellow]Input:[/yellow] {self._format_value(data['input'])}\")\n\n        if data.get(\"output\"):\n            tree.add(f\"[yellow]Output:[/yellow] {self._format_value(data['output'])}\")\n\n        if data.get(\"mcp_data\"):\n            mcp_tree = tree.add(\"[yellow]MCP Data:[/yellow]\")\n            for key, value in data[\"mcp_data\"].items():\n                mcp_tree.add(f\"{key}: {self._format_value(value)}\")\n\n    def _add_handoff_data(self, tree: Tree, data: dict):\n        \"\"\"Add handoff span data.\"\"\"\n        if data.get(\"from_agent\"):\n            tree.add(f\"[yellow]From Agent:[/yellow] {data['from_agent']}\")\n        if data.get(\"to_agent\"):\n            tree.add(f\"[yellow]To Agent:[/yellow] {data['to_agent']}\")\n\n    def _add_guardrail_data(self, tree: Tree, data: dict):\n        \"\"\"Add guardrail span data.\"\"\"\n        if data.get(\"name\"):\n            tree.add(f\"[yellow]Guardrail Name:[/yellow] {data['name']}\")\n        if \"triggered\" in data:\n            status = \"\ud83d\udea8 TRIGGERED\" if data[\"triggered\"] else \"\u2705 PASSED\"\n            tree.add(f\"[yellow]Status:[/yellow] {status}\")\n\n    def _add_transcription_data(self, tree: Tree, data: dict):\n        \"\"\"Add transcription span data.\"\"\"\n        if data.get(\"model\"):\n            tree.add(f\"[yellow]Model:[/yellow] {data['model']}\")\n\n        if data.get(\"input\"):\n            input_info = data[\"input\"]\n            if isinstance(input_info, dict):\n                tree.add(\n                    f\"[yellow]Input Format:[/yellow] {input_info.get('format', 'N/A')}\"\n                )\n                if input_info.get(\"data\"):\n                    tree.add(\"[yellow]Input Data:[/yellow] [Audio data present]\")\n\n        if data.get(\"output\"):\n            tree.add(f\"[yellow]Transcription:[/yellow] {data['output']}\")\n\n    def _add_speech_data(self, tree: Tree, data: dict):\n        \"\"\"Add speech span data.\"\"\"\n        if data.get(\"model\"):\n            tree.add(f\"[yellow]Model:[/yellow] {data['model']}\")\n\n        if data.get(\"input\"):\n            tree.add(f\"[yellow]Text Input:[/yellow] {data['input']}\")\n\n        if data.get(\"output\"):\n            output_info = data[\"output\"]\n            if isinstance(output_info, dict):\n                tree.add(\n                    f\"[yellow]Output Format:[/yellow] {output_info.get('format', 'N/A')}\"\n                )\n                if output_info.get(\"data\"):\n                    tree.add(\"[yellow]Audio Output:[/yellow] [Audio data present]\")\n\n        if data.get(\"first_content_at\"):\n            tree.add(f\"[yellow]First Content At:[/yellow] {data['first_content_at']}\")\n\n    def _add_custom_data(self, tree: Tree, data: dict):\n        \"\"\"Add custom span data.\"\"\"\n        if data.get(\"name\"):\n            tree.add(f\"[yellow]Custom Span Name:[/yellow] {data['name']}\")\n\n        if data.get(\"data\"):\n            custom_tree = tree.add(\"[yellow]Custom Data:[/yellow]\")\n            for key, value in data[\"data\"].items():\n                custom_tree.add(f\"{key}: {self._format_value(value)}\")\n\n    def _format_value(self, value: Any) -&gt; str:\n        \"\"\"Format a value for display, truncating if too long.\"\"\"\n        if value is None:\n            return \"None\"\n\n        str_value = str(value)\n        if len(str_value) &gt; 200:\n            return str_value[:200] + \"...\"\n        return str_value\n\n\n# Use the comprehensive rich exporter\ncomprehensive_processor = BatchTraceProcessor(ComprehensiveRichConsoleSpanExporter())\nset_trace_processors([comprehensive_processor])\n\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    config.tracing_disabled = False\n\n    result = Runner.run_streamed(\n        agent, input=\"Please tell me 5 jokes.\", run_config=config\n    )\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\" and isinstance(\n            event.data, ResponseTextDeltaEvent\n        ):\n            print(event.data.delta, end=\"\", flush=True)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/turn_count_per_llm_invocation/","title":"Turn Count Per Llm Invocation","text":"Source code in OpAgentsOlympus/practice/turn_count_per_llm_invocation.py OpAgentsOlympus/practice/turn_count_per_llm_invocation.py<pre><code>from agents import Agent, Runner, function_tool, AgentHooks, RunContextWrapper, ModelResponse, TResponseInputItem\nfrom typing import Optional, Any\nfrom local_config import config\n\n\nclass LLMCallTracker(AgentHooks):\n    def __init__(self):\n        self.llm_call_count = 0\n\n    async def on_llm_start(\n        self,\n        context: RunContextWrapper,\n        agent: Agent,\n        system_prompt: Optional[str],\n        input_items: list[TResponseInputItem],\n    ) -&gt; None:\n        self.llm_call_count += 1\n        print(f\"LLM Call #{self.llm_call_count}\")\n\n    async def on_llm_end(\n        self,\n        context: RunContextWrapper,\n        agent: Agent,\n        response: ModelResponse,\n    ) -&gt; None:\n        print(f\"LLM Call #{self.llm_call_count} completed\")\n\n    async def on_tool_start(\n        self,\n        context: RunContextWrapper[Any],\n        agent: Any,\n        tool: Any,\n    ) -&gt; None:\n        \"\"\"Called concurrently with tool invocation.\"\"\"\n        print('Tool started!')\n\n    async def on_tool_end(\n        self,\n        context: RunContextWrapper[Any],\n        agent: Any,\n        tool: Any,\n        result: str,\n    ) -&gt; None:\n        \"\"\"Called after a tool is invoked.\"\"\"\n        print('Tool ended!')\n\n\n@function_tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get weather information for a city.\"\"\"\n    return f\"The weather in {city} is sunny and 75\u00b0F\"\n\n\nasync def main():\n    tracker = LLMCallTracker()\n\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"Help users with weather information. Use the appropriate tools.\",\n        tools=[get_weather],\n        hooks=tracker\n    )\n\n    print(\"Starting workflow demonstrations...\")\n    print(\"=\" * 60)\n\n    print(\"First Runner.run() call:\")\n\n    result1 = await Runner.run(\n        agent,\n        input=\"What's the weather in Tokyo and what time is it there?\",\n        run_config=config\n    )\n\n    print(\"Workflow completed!\")\n    print(f\"Final output: {result1.final_output}\")\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n\n# Add \"print(f\"Turn: {current_turn}#\")\" under \"current_turn += 1\" inside Runner.run's while loop to actually see how many times did the turn counter increased for each LLM call.\n\n\n# &lt; =======Output======= &gt;\n# Starting workflow demonstrations...\n# ============================================================\n# First Runner.run() call:\n# Turn: 1#\n# LLM Call #1\n# LLM Call #1 completed\n# Tool started!\n# Tool ended!\n# Turn: 2#\n# LLM Call #2\n# LLM Call #2 completed\n# Workflow completed!\n# Final output: &lt;think&gt;\n# Okay, the user asked for the weather in Tokyo and the time there. I called the get_weather function and got back that the weather is sunny and 75\u00b0F. Now I need to answer the user's question based on this response. The user also asked about the time, but the tool doesn't provide time information. So I should mention the weather details and clarify that the time isn't available with the current tools. Let me make sure the answer is clear and helpful.\n# &lt;/think&gt;\n\n# The weather in Tokyo is currently sunny with a temperature of 75\u00b0F. Note that the exact time in Tokyo isn't provided by the available tools. Let me know if you need further weather details!\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/","title":"OpenAI Agents SDK Quizzes","text":""},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/#intermediate-level-quiz","title":"\ud83c\udf89 Intermediate Level Quiz","text":"<p>50 completely new MCQs testing practical OpenAI Agents SDK knowledge.</p> <p>Quiz Details:</p> <ul> <li> <p>Time Limit: 60 minutes</p> </li> <li> <p>Difficulty Level: Intermediate</p> </li> <li> <p>Focus: Memorization and attention to detail</p> </li> <li> <p>Success Target: 90%+ score</p> </li> </ul> <p>Take the Intermediate Quiz</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/#advance-level-quiz","title":"\ud83d\ude80 Advance Level Quiz","text":"<p>100 questions covering core to advanced features, including code snippets and new topics like tracing &amp; hooks (20 specialized questions).</p> <p>Difficulty Warning \u26a0\ufe0f Brace yourself \u2013 these are intense! Each question combines multiple SDK systems and requires deep reasoning.</p> <p>What You're Facing:</p> <ul> <li> <p>90%+ SDK Coverage: agents, tools, handoffs, guardrails, streaming, models, hooks, tracing, workflows</p> </li> <li> <p>High-Complexity Integration: execution flows, exceptions, custom behaviors, edge cases</p> </li> </ul> <p>Success Metrics:</p> <ul> <li> <p>75%+ Score: Solid grasp of SDK fundamentals</p> </li> <li> <p>90%+ Score: Expert-level understanding for building complex systems</p> </li> <li> <p>Perfect for: Senior engineers, framework contributors, production agent system builders</p> </li> </ul> <p>Time Allocation:</p> <ul> <li> <p>150 minutes total</p> </li> <li> <p>1.5 minutes per question (some may require 5\u201310 minutes of deep reasoning)</p> </li> </ul> <p>Dive in and level up your SDK expertise! Every wrong answer is a learning opportunity. \ud83e\udde0\ud83d\udd25</p> <p>Take the Advanced Quiz</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%231.%20strict_json_schema/","title":"#1. Strict Json Schema","text":"Source code in OpAgentsOlympus/practice/#1. strict_json_schema.py OpAgentsOlympus/practice/#1. strict_json_schema.py<pre><code>from agents import AgentOutputSchema\nfrom pydantic import BaseModel\nfrom typing import Any\n\n\nclass Human(BaseModel):\n    name: str\n    age: int\n    friends: list[str]\n\n\n# Strict JSON Schema False\noutput_wrapper = AgentOutputSchema(output_type=Human, strict_json_schema=False)\n\nprint(output_wrapper, \"\\n\\n\")\n\nassert (\n    not output_wrapper.is_strict_json_schema()\n)  # This will return False, But not will make it True again, so this will pass\nassert (\n    output_wrapper.json_schema() == Human.model_json_schema()\n)  # Both are equal since strict_json_schema=False (No Changes)\n\nprint(output_wrapper.json_schema(), \"\\n\\n\")\n\nprint(Human.model_json_schema(), \"\\n\\n\")\n\nprint(\"All Tests Passed!\\n\\n\")\n\n# Strict JSON Schema True\noutput_wrapper = AgentOutputSchema(output_type=Human, strict_json_schema=True)\n\nprint(output_wrapper, \"\\n\\n\")\n\nassert output_wrapper.is_strict_json_schema()\n# assert output_wrapper.json_schema() == Human.model_json_schema() # This will fail, Because we have an extra additionalProperties key inside output_wrapper.json_schema()\n\nprint(\"Before\", output_wrapper.json_schema(), \"\\n\\n\")\ndel output_wrapper.json_schema()[\"additionalProperties\"]\nprint(\"After\", output_wrapper.json_schema(), \"\\n\\n\")\n\nassert (\n    output_wrapper.json_schema() == Human.model_json_schema()\n)  # Now ths will pass, Because we have deleted the additionalProperties key\n\nprint(Human.model_json_schema(), \"\\n\\n\")\n\nprint(\"All Tests Passed!\\n\\n\")\n\n# Manually forcing both to be Equal\nstrict_false = AgentOutputSchema(output_type=Human, strict_json_schema=False)\nstrict_true = AgentOutputSchema(output_type=Human, strict_json_schema=True)\n\nprint(\"Strict False\", strict_false, \"\\n\")\nprint(\"Strict True\", strict_true, \"\\n\")\n\nprint(\"Are Both Equal?\", strict_false == strict_true, \"\\n\")  # Are Both Equal? False\n\n\n# Force Modifying\ndef __eq__(self: AgentOutputSchema, other: AgentOutputSchema | Any):\n    if not isinstance(other, AgentOutputSchema):\n        return False\n    return (\n        self.output_type == other.output_type\n        and self._strict_json_schema == other._strict_json_schema\n        and self._is_wrapped == other._is_wrapped\n        and self._output_schema == other._output_schema\n    )\n\n\nAgentOutputSchema.__eq__ = __eq__  # Custom __eq__ method to check for equality\n\ndel strict_true.json_schema()[\"additionalProperties\"]  # Delete additionalProperties\nstrict_true._strict_json_schema = (\n    False  # Assign False to _strict_json_schema private attribute\n)\n\nprint(\n    \"Are Both Equal Now?\", strict_false == strict_true, \"\\n\"\n)  # Are Both Equal Now? True\n\n# When strict_json_schema=True (the default), the schema is processed through ensure_strict_json_schema() to make it compliant with OpenAI's structured output requirements.\n# When set to False, the schema uses the original Pydantic model schema without strict constraints.\n# The AgentRunner._get_output_schema() method creates an AgentOutputSchema instance for agents with non-string output types.\n\n# Answer\n# B. Whether to use strict or non-strict JSON schemas for output validation is what AgentOutputSchema allows you to customize.\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2311.%20sequence_of_events/","title":"#11. Sequence Of Events","text":"Source code in OpAgentsOlympus/practice/#11. sequence_of_events.py OpAgentsOlympus/practice/#11. sequence_of_events.py<pre><code>import asyncio\nfrom agents import Agent, Runner\n\n\nasync def print_all_streaming_events():\n    agent = Agent(\n        name=\"Test Agent\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    result = Runner.run_streamed(\n        agent, input=\"Hello, tell me a joke and use any tools if needed.\"\n    )\n\n    event_count = 0\n    async for event in result.stream_events():\n        event_count += 1\n        print(f\"\\n=== EVENT #{event_count} ===\")\n        print(f\"Type: {event.type}\")\n\n        if event.type == \"raw_response_event\":\n            print(f\"Raw Event Type: {event.data.type}\")\n            print(f\"Raw Event Data: {event.data}\")\n\n        elif event.type == \"run_item_stream_event\":\n            print(f\"Item Name: {event.name}\")\n            print(f\"Item Type: {event.item.type}\")\n            print(f\"Item: {event.item}\")\n\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"New Agent: {event.new_agent.name}\")\n\n        print(\"=\" * 50)\n\n    print(f\"\\nTotal events processed: {event_count}\")\n    print(f\"Final output: {result.final_output}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(print_all_streaming_events())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%232.%20handoff_input_filter/","title":"#2. Handoff Input Filter","text":"Source code in OpAgentsOlympus/practice/#2. handoff_input_filter.py OpAgentsOlympus/practice/#2. handoff_input_filter.py<pre><code>from open_router_config import config\nfrom agents import HandoffInputData, Agent, handoff, Runner\nimport asyncio\nfrom rich import print\n\nconfig.tracing_disabled = False\n\n\ndef remove_new_items(handoff_input_data: HandoffInputData) -&gt; HandoffInputData:\n    # print(\"Before:\", handoff_input_data)\n    filtered = HandoffInputData(\n        input_history=handoff_input_data.input_history,\n        pre_handoff_items=handoff_input_data.pre_handoff_items,\n        new_items=(),\n    )\n    # print(\"After:\", filtered)\n    return filtered  # This data will be received to the next handoff agent.\n\n\nsay_bye_agent = Agent(\n    name=\"say_bye_agent\", instructions=\"You only say bye to everyone.\"\n)\n\nsay_hello_agent1 = Agent(\n    name=\"say_hello_agent1\",\n    instructions=\"You only say hello to everyone.\",\n    handoffs=[\n        handoff(\n            agent=say_bye_agent,\n            input_filter=remove_new_items,\n        )\n    ],\n)\n\nsay_hello_agent2 = Agent(\n    name=\"say_hello_agent2\",\n    instructions=\"You only say hello to everyone.\",\n    handoffs=[\n        handoff(\n            agent=say_bye_agent,\n        )\n    ],\n)\n\n\nasync def main():\n    result = Runner.run_streamed(\n        say_hello_agent2, input=\"say hello and bye\", run_config=config\n    )\n    async for _ in result.stream_events():\n        pass\n    print(\"Not Filtered:\", result.to_input_list())\n\n    result = Runner.run_streamed(\n        say_hello_agent1, input=\"say hello and bye\", run_config=config\n    )\n    async for _ in result.stream_events():\n        pass\n    print(\"Filtered:\", result.to_input_list())\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2321.%20stop_on_first_tool_with_none/","title":"#21. Stop On First Tool With None","text":"Source code in OpAgentsOlympus/practice/#21. stop_on_first_tool_with_none.py OpAgentsOlympus/practice/#21. stop_on_first_tool_with_none.py<pre><code>from open_router_config import config\nfrom agents import Agent, Runner, function_tool, RunContextWrapper\nimport asyncio\nfrom rich import print\nfrom pydantic import BaseModel\nfrom typing import Any\n\n\nclass UserData(BaseModel):\n    name: str\n    age: int\n\n\n@function_tool\ndef hello(ctx: RunContextWrapper[Any]) -&gt; str:\n    \"\"\"Used to say hello\"\"\"\n    try:\n        print(f\"Name: {ctx.context.name} Age: {ctx.context.age}\")\n    except Exception:\n        print(\"Context Unavailable!\")\n    return \"None\"  # Return None\n\n\nclass Great(BaseModel):\n    great: str\n\n\nuserdata = UserData(name=\"Daniel\", age=18)\n\nassistant = Agent(\n    name=\"assistant\",\n    instructions=\"You are a helpful assistant.\",\n    tools=[hello],\n    # output_type=Great,\n    # tool_use_behavior='stop_on_first_tool' # This means no further LLM processing, The output of the first tool directly become the final_output\n)\n\n\nasync def main():\n    result = await Runner.run(\n        assistant, input=\"say hello\", run_config=config, context=userdata\n    )\n    print(result.final_output)\n    print(type(result.final_output))\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2324.%20tool_choice_required_with_reset_false/","title":"#24. Tool Choice Required With Reset False","text":"Source code in OpAgentsOlympus/practice/#24. tool_choice_required_with_reset_false.py OpAgentsOlympus/practice/#24. tool_choice_required_with_reset_false.py<pre><code># The answer is A: \"The tool choice remains \"required\" causing an infinite loop\"\n\n# When reset_tool_choice=False, the framework does not reset the tool_choice after a tool call\n# The tool_choice remains \"required\"\n# After the first tool call, the results are sent back to the LLM\n# Since tool_choice is still \"required\", the LLM must make another tool call\n# This cycle continues indefinitely, creating an infinite loop\n# This is exactly why the default behavior (reset_tool_choice=True) exists - to prevent these infinite loops by automatically resetting tool_choice to \"auto\" after tool calls.\n\nfrom __future__ import annotations\nimport asyncio\nfrom open_router_config import config\nfrom agents import Agent, Runner, function_tool, ModelSettings\nfrom agents.exceptions import MaxTurnsExceeded\n\n\n@function_tool\ndef simple_tool(message: str):\n    \"\"\"A simple tool that echoes the message back.\"\"\"\n    print(f\"Tool called with message: {message}\")\n    return f\"Processed: {message}\"\n\n\nagent = Agent(\n    name=\"TestAgent\",\n    instructions=\"You are a helpful assistant. Use the simple_tool whenever possible.\",\n    tools=[simple_tool],\n    reset_tool_choice=False,  # Key setting we're testing\n)\n\n\nasync def main():\n    config.model_settings = ModelSettings(\n        tool_choice=\"required\"\n    )  # Key setting we're testing\n\n    try:\n        result = await Runner.run(agent, \"Hello\", run_config=config)\n        print(\"Final output:\", result.final_output)\n    except MaxTurnsExceeded as e:\n        print(f\"Max turns exceeded: {e}\")\n        print(\n            \"This confirms the agent entered an infinite loop because tool_choice remained 'required'\"\n        )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# &lt;=== Output ===&gt;\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Tool called with message: Hello!\n# Max turns exceeded: Max turns (10) exceeded\n# This confirms the agent entered an infinite loop because tool_choice remained 'required'\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2325.%20stop_on_tool_name/","title":"#25. Stop On Tool Name","text":"Source code in OpAgentsOlympus/practice/#25. stop_on_tool_name.py OpAgentsOlympus/practice/#25. stop_on_tool_name.py<pre><code>from open_router_config import config\nfrom agents import Agent, Runner, function_tool\nimport asyncio\n\n\n@function_tool\ndef hello() -&gt; str:\n    \"\"\"Used to say hello\"\"\"\n    print(\"hello was called\")\n    return \"hello\"\n\n\n@function_tool\ndef bye() -&gt; str:\n    \"\"\"Used to say bye\"\"\"\n    print(\"bye was called\")\n    return \"bye\"\n\n\nassistant = Agent(\n    name=\"assistant\",\n    instructions=\"You are a helpful assistant.\",\n    tools=[hello, bye],\n    tool_use_behavior={\"stop_at_tool_names\": [\"hello\"]},  # stop on hello\n)\n\n\nasync def main():\n    result = await Runner.run(assistant, input=\"say hello and bye\", run_config=config)\n    print(result.final_output)\n\n\nasyncio.run(main())\n\n# &lt;== Output ==&gt;\n# hello was called\n# bye was called\n# hello\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2334.%20consume_remain_events/","title":"#34. Consume Remain Events","text":"Source code in OpAgentsOlympus/practice/#34. consume_remain_events.py OpAgentsOlympus/practice/#34. consume_remain_events.py<pre><code>from agents import Agent, Runner, function_tool\nimport asyncio\nfrom open_router_config import config\n\n\n@function_tool\nasync def long_running_task(query: str) -&gt; str:\n    await asyncio.sleep(5)  # Long operation\n    return f\"Completed {query}\"\n\n\n@function_tool\ndef quick_task(data: str) -&gt; str:\n    return f\"Quick: {data}\"\n\n\nagent = Agent(\n    name=\"Worker\",\n    tools=[long_running_task, quick_task],\n)\n\n\nasync def main():\n    result = Runner.run_streamed(agent, \"Do both tasks\", run_config=config)\n\n    # Consume first batch of events\n    events = []\n    async for event in result.stream_events():\n        events.append(event.type)\n        print(f\"Event: {event.type}\")\n        if len(events) == 3:\n            break\n\n    print(\"Stream Complete:\", result.is_complete)\n    print(f\"First batch events: {events}, Length: {len(events)}\")\n\n    # Resume consuming events\n    async for event in result.stream_events():\n        events.append(event.type)\n        print(f\"Resumed event: {event.type}\")\n        # if len(events) == 10:\n        #     break\n    print(\"Stream Complete:\", result.is_complete)\n    print(f\"Second batch events: {events}, Length: {len(events)}\")\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2342.%20custom_tool_use_behavior/","title":"#42. Custom Tool Use Behavior","text":"Source code in OpAgentsOlympus/practice/#42. custom_tool_use_behavior.py OpAgentsOlympus/practice/#42. custom_tool_use_behavior.py<pre><code>from agents import Agent, ModelSettings, function_tool, ToolsToFinalOutputResult, Runner\nfrom open_router_config import config\n\n\n@function_tool\ndef fetch_data(query: str) -&gt; str:\n    return f\"Data for {query}\"\n\n\n@function_tool\ndef process_data(data: str) -&gt; str:\n    return f\"Processed: {data}\"\n\n\ndef custom_tool_handler(ctx, tool_results):\n    print(len(tool_results))  # Output: 2\n\n    if len(tool_results) &gt; 1:\n        return ToolsToFinalOutputResult(\n            is_final_output=True,\n            final_output=tool_results[-1].output,  # return only the last tool's result\n        )\n    return ToolsToFinalOutputResult(is_final_output=False)\n\n\nagent = Agent(\n    name=\"DataProcessor\",\n    tools=[fetch_data, process_data],\n    model_settings=ModelSettings(tool_choice=\"required\"),\n    tool_use_behavior=custom_tool_handler,  # custom tool use behavior\n    reset_tool_choice=False,\n)\n\nresult = Runner.run_sync(agent, \"call all tools\", run_config=config)\n\n\nprint(result.final_output)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2343.%20event_sequence/","title":"#43. Event Sequence","text":"Source code in OpAgentsOlympus/practice/#43. event_sequence.py OpAgentsOlympus/practice/#43. event_sequence.py<pre><code>from agents import (\n    Agent,\n    Runner,\n    input_guardrail,\n    output_guardrail,\n    GuardrailFunctionOutput,\n)\nimport asyncio\nfrom config import config\n\n\n@input_guardrail\nasync def slow_input_check(ctx, agent, input_data):\n    await asyncio.sleep(2)  # Simulates slow guardrail\n    return GuardrailFunctionOutput(output_info=\"Checked\", tripwire_triggered=False)\n\n\n@output_guardrail\nasync def fast_output_check(ctx, agent, output):\n    return GuardrailFunctionOutput(output_info=\"Fast check\", tripwire_triggered=False)\n\n\nspecialist = Agent(name=\"Specialist\", output_guardrails=[fast_output_check])\n\nmain_agent = Agent(\n    name=\"Main\", input_guardrails=[slow_input_check], handoffs=[specialist]\n)\n\n\nasync def main():\n    result = Runner.run_streamed(main_agent, \"Process this request\", run_config=config)\n    events = []\n    async for event in result.stream_events():\n        if event.type not in events:\n            events.append(event.type)\n    print(events)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2344.%20handoff_chain_with_input_filter/","title":"#44. Handoff Chain With Input Filter","text":"Source code in OpAgentsOlympus/practice/#44. handoff_chain_with_input_filter.py OpAgentsOlympus/practice/#44. handoff_chain_with_input_filter.py<pre><code>from agents import Agent, handoff, HandoffInputData, Runner\nfrom open_router_config import config\nfrom rich import print\n\n\ndef remove_transfer_data(data: HandoffInputData) -&gt; HandoffInputData:\n    if isinstance(data.input_history, str):\n        return HandoffInputData(\n            input_history=data.input_history,\n            pre_handoff_items=data.pre_handoff_items,\n            new_items=(),\n        )\n\n    filtered_history = tuple(\n        item for item in data.input_history if item.get(\"name\") != \"transfer_to_b\"\n    )\n\n    return HandoffInputData(\n        input_history=filtered_history,\n        pre_handoff_items=data.pre_handoff_items,\n        new_items=(),\n    )\n\n\ndef add_context(data: HandoffInputData) -&gt; HandoffInputData:\n    context_item = {\"role\": \"assistant\", \"content\": \"You are now the specialist agent\"}\n    if isinstance(data.input_history, str):\n        return HandoffInputData(\n            input_history=(\n                context_item,\n                {\"role\": \"user\", \"content\": data.input_history},\n            ),\n            pre_handoff_items=data.pre_handoff_items,\n            new_items=data.new_items,\n        )\n\n    return HandoffInputData(\n        input_history=(context_item,) + data.input_history,\n        pre_handoff_items=data.pre_handoff_items,\n        new_items=data.new_items,\n    )\n\n\nagent_a = Agent(name=\"A\")\nagent_b = Agent(name=\"B\", handoffs=[handoff(agent_a, input_filter=add_context)])\nagent_c = Agent(\n    name=\"C\", handoffs=[handoff(agent_b, input_filter=remove_transfer_data)]\n)\n\nresult = Runner.run_sync(agent_c, \"keep handing off\", run_config=config)\nprint(result.to_input_list())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2345.%20model_precedense/","title":"#45. Model Precedense","text":"Source code in OpAgentsOlympus/practice/#45. model_precedense.py OpAgentsOlympus/practice/#45. model_precedense.py<pre><code>from agents import (\n    Agent,\n    Runner,\n    RunConfig,\n    ModelProvider,\n    OpenAIChatCompletionsModel,\n    AsyncOpenAI,\n)\nfrom agents.tracing import set_trace_processors\nfrom agents.tracing.processors import BatchTraceProcessor, TracingExporter\nfrom rich.console import Console\nfrom rich.tree import Tree\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n\nclient = AsyncOpenAI(\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai\",\n    api_key=GEMINI_API_KEY,\n)\n\n\nclass CustomConsoleSpanExporter(TracingExporter):\n    def __init__(self):\n        self.console = Console()\n\n    def export(self, items):\n        for item in items:\n            if (\n                hasattr(item, \"export\")\n                and item.export().get(\"span_data\", {}).get(\"type\", \"unknown\")\n                == \"generation\"\n            ):\n                self._export_span(item)\n\n    def _export_span(self, span):\n        span_data = span.export()\n        if not span_data:\n            return\n\n        span_details = span_data.get(\"span_data\", {})\n        span_type = span_details.get(\"type\", \"unknown\")\n\n        tree = Tree(f\"[bold green]Span:[/bold green] {span_type}\")\n\n        if span_type == \"generation\":\n            self._add_generation_data(tree, span_details)\n\n        self.console.print(tree)\n\n    def _add_generation_data(self, tree, data):\n        if data.get(\"model\"):\n            tree.add(\n                f\"[yellow]\ud83c\udfaf ACTUAL MODEL USED:[/yellow] [bold red]{data['model']}[/bold red]\"\n            )\n\n\ncomprehensive_processor = BatchTraceProcessor(CustomConsoleSpanExporter())\nset_trace_processors([comprehensive_processor])\n\nmodel1 = OpenAIChatCompletionsModel(\"gemini-1.5-flash\", client)\nmodel2 = OpenAIChatCompletionsModel(\"gemini-2.0-flash\", client)\n\n\nclass CustomProvider(ModelProvider):\n    def get_model(self, model_name: str):\n        if model_name == \"custom-model\":\n            return model2\n        print(f\"\u274c CustomProvider could not resolve: {model_name}\")\n        return None\n\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=\"You are a helpful assistant\",\n    model=model1,  # This will be overridden\n)\n\nrun_config = RunConfig(\n    model=\"custom-model\",\n    model_provider=CustomProvider(),\n    tracing_disabled=False,  # Enable tracing so that the CustomConsoleSpanExporter be able to log in terminal\n)\n\n# This will use gemini-2.0-flash, not gemini-1.5-flash\nresult = Runner.run_sync(agent, \"Hello\", run_config=run_config)\nprint(f\"\u2728 Final output: {result.final_output}\")\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2346.%20nested_agent_context_propagation/","title":"#46. Nested Agent Context Propagation","text":"Source code in OpAgentsOlympus/practice/#46. nested_agent_context_propagation.py OpAgentsOlympus/practice/#46. nested_agent_context_propagation.py<pre><code>     from agents import Agent, function_tool, Runner\n\n     @function_tool\n     def get_user_data(ctx, user_id: str) -&gt; str:\n          # Access context from parent agent\n          return f\"User {user_id} data from {ctx.context.database_name}\"\n\n     class DatabaseContext:\n          def __init__(self, db_name: str):\n                self.database_name = db_name\n\n     inner_agent = Agent(\n          name=\"DataAgent\",\n          tools=[get_user_data],\n          instructions=\"Fetch user data from the database\"\n     )\n\n     outer_agent = Agent(\n          name=\"MainAgent\",\n          tools=[inner_agent.as_tool(tool_name=\"fetch_user_data\")],\n          instructions=\"Use the data agent to get user information\"\n     )\n\n     context = DatabaseContext(\"production_db\")\n     result = await Runner.run(outer_agent, \"Get data for user 123\", context=context)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2347.%20cancel_while_streaming/","title":"#47. Cancel While Streaming","text":"Source code in OpAgentsOlympus/practice/#47. cancel_while_streaming.py OpAgentsOlympus/practice/#47. cancel_while_streaming.py<pre><code>from agents import Agent, Runner, function_tool\nimport asyncio\nfrom open_router_config import config\n\n\n@function_tool\nasync def long_running_task(query: str) -&gt; str:\n    await asyncio.sleep(5)  # Long operation\n    return f\"Completed {query}\"\n\n\n@function_tool\ndef quick_task(data: str) -&gt; str:\n    return f\"Quick: {data}\"\n\n\nagent = Agent(\n    name=\"Worker\",\n    tools=[long_running_task, quick_task],\n)\n\n\nasync def test_cancellation():\n    result = Runner.run_streamed(agent, \"Do both tasks\", run_config=config)\n\n    # Cancel after 2 seconds\n    await asyncio.sleep(2)\n    result.cancel()\n\n    # Try to consume remaining events\n    events = []\n    async for event in result.stream_events():\n        events.append(event.type)\n\n    print(result.final_output, len(events))\n\n\nasyncio.run(test_cancellation())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2348.%20json.with_string_age/","title":"#48. Json.with String Age","text":"Source code in OpAgentsOlympus/practice/#48. json.with_string_age.py OpAgentsOlympus/practice/#48. json.with_string_age.py<pre><code>     from agents import Agent, AgentOutputSchema\n     from typing import Dict, Any\n\n     class UserProfile(BaseModel):\n          name: str\n          age: int\n          active: bool = True\n\n     agent = Agent(\n          name=\"ProfileAgent\",\n          instructions=\"Return user profile data\",\n          output_type=AgentOutputSchema(UserProfile, strict_json_schema=False)\n     )\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2354.%20session/","title":"#54. Session","text":"Source code in OpAgentsOlympus/practice/#54. session.py OpAgentsOlympus/practice/#54. session.py<pre><code>from agents import Agent, Runner, SQLiteSession\nfrom open_router_config import config\nimport asyncio\n\nsession = SQLiteSession(\"test.db\")\nagent = Agent(name=\"assistant\", instructions=\"You are a helpful assistant\")\n\n\nasync def main():\n    # First run adds to session\n    result1 = await Runner.run(\n        agent,\n        \"my name is daniel and my height is 5.11\",\n        session=session,\n        run_config=config,\n    )\n    print(result1.final_output)\n\n    # Second run with same session\n    result2 = await Runner.run(\n        agent, \"what is my name?\", session=session, run_config=config\n    )\n    print(result2.final_output)\n\n    result2 = await Runner.run(\n        agent, \"what is my height?\", session=session, run_config=config\n    )\n    print(result2.final_output)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2359.%20consume_only_first_stream/","title":"#59. Consume Only First Stream","text":"Source code in OpAgentsOlympus/practice/#59. consume_only_first_stream.py OpAgentsOlympus/practice/#59. consume_only_first_stream.py<pre><code>from agents import Agent, Runner, input_guardrail, GuardrailFunctionOutput\nfrom open_router_config import config\nimport asyncio\n\n\n@input_guardrail\nasync def async_validation(ctx, agent, input_data):\n    await asyncio.sleep(1)  # Simulates async validation\n    return GuardrailFunctionOutput(output_info=\"Validated\", tripwire_triggered=False)\n\n\nagent = Agent(name=\"Validator\", input_guardrails=[async_validation])\n\n\nasync def test_streaming():\n    result = Runner.run_streamed(agent, \"Test input\", run_config=config)\n\n    # Consume only the first event\n    first_event = await result.stream_events().__anext__()\n\n    # Check completion status immediately\n    print(result.is_complete, first_event.type)\n\n    # Check completion regardless of consuming events\n    # while True:\n    #     # print('Ongoing')\n    #     if result.is_complete:\n    #         print('Finished')\n    #         break\n    #     await asyncio.sleep(0.1)\n\n\nasyncio.run(test_streaming())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2360.%20session_with_input_list/","title":"#60. Session With Input List","text":"Source code in OpAgentsOlympus/practice/#60. session_with_input_list.py OpAgentsOlympus/practice/#60. session_with_input_list.py<pre><code>from agents import Agent, Runner, SQLiteSession\nfrom open_router_config import config\nimport asyncio\n\nsession = SQLiteSession(\"workflow.db\")\nagent = Agent(name=\"Stateful\", instructions=\"Remember our conversation\")\n\n\nasync def main():\n    # First execution\n    await Runner.run(agent, \"My name is Alice\", session=session, run_config=config)\n\n    # Second execution with different input format\n    await Runner.run(\n        agent, [{\"role\": \"user\", \"content\": \"What's my name?\"}], session=session\n    )\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2366.%20tool_return_type/","title":"#66. Tool Return Type","text":"Source code in OpAgentsOlympus/practice/#66. tool_return_type.py OpAgentsOlympus/practice/#66. tool_return_type.py<pre><code>from open_router_config import config\nfrom agents import Agent, Runner, function_tool\nimport asyncio\nfrom pydantic import BaseModel\n\n\n@function_tool\ndef hello() -&gt; str:\n    \"\"\"Used to say hello\"\"\"\n    return None  # Return None\n\n\nclass Great(BaseModel):\n    great: str\n\n\nassistant = Agent(\n    name=\"assistant\",\n    instructions=\"You are a helpful assistant.\",\n    tools=[hello],\n    output_type=Great,\n    tool_use_behavior=\"stop_on_first_tool\",\n)\n\n\nasync def main():\n    result = await Runner.run(assistant, input=\"say hello\", run_config=config)\n    print(type(result.final_output))  # What is the output_type?\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2372.%20turn_count/","title":"#72. Turn Count","text":"Source code in OpAgentsOlympus/practice/#72. turn_count.py OpAgentsOlympus/practice/#72. turn_count.py<pre><code>from agents import Agent, Runner, function_tool\n\n\n@function_tool\ndef delegate_work(task: str) -&gt; str:\n    # This tool internally uses another agent\n    specialist = Agent(name=\"Specialist\", instructions=\"Handle specialized tasks\")\n    result = Runner.run_sync(specialist, f\"Process: {task}\")\n    return result.final_output\n\n\nmain_agent = Agent(\n    name=\"MainAgent\", tools=[delegate_work], tool_use_behavior=\"run_llm_again\"\n)\n\nresult = await Runner.run(main_agent, \"Complete complex task\", max_turns=3)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2379.%20tool_execution_flow/","title":"#79. Tool Execution Flow","text":"Source code in OpAgentsOlympus/practice/#79. tool_execution_flow.py OpAgentsOlympus/practice/#79. tool_execution_flow.py<pre><code>from agents import Agent, function_tool, WebSearchTool, ToolsToFinalOutputResult, Runner\nfrom open_router_config import config\n\n\n@function_tool\ndef analyzer(data: str) -&gt; str:\n    return f\"Analyzed: {data}\"\n\n\n@function_tool\ndef processor(data: str) -&gt; str:\n    return f\"Processed: {data}\"\n\n\ndef selective_handler(ctx, tool_results):\n    function_results = [\n        r for r in tool_results if r.tool_name in [\"analyzer\", \"processor\"]\n    ]\n    if len(function_results) &gt;= 2:\n        return ToolsToFinalOutputResult(\n            is_final_output=True,\n            final_output=f\"Combined: {', '.join(r.output for r in function_results)}\",\n        )\n    return ToolsToFinalOutputResult(is_final_output=False)\n\n\nagent = Agent(\n    name=\"MixedToolAgent\",\n    tools=[WebSearchTool(), analyzer, processor],\n    tool_use_behavior=selective_handler,\n)\n\nresult = Runner.run_sync(agent, \"Use all tools\", run_config=config)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/","title":"Why Answer A is Correct: ToolsToFinalOutputFunction Loop Termination","text":""},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#the-question","title":"The Question","text":"<p>When using <code>ToolsToFinalOutputFunction</code> with an agent that has a custom <code>output_type</code>, what determines whether the agent loop continues or terminates?</p> <p>A. Whether the function returns <code>is_final_output=True</code> regardless of the output type match \u2705 B. Whether the final_output matches the agent's output_type AND is_final_output=True \u274c</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#code-analysis","title":"Code Analysis","text":""},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#1-toolstofinaloutputresult-dataclass","title":"1. <code>ToolsToFinalOutputResult</code> Dataclass","text":"Python<pre><code>@dataclass\nclass ToolsToFinalOutputResult:\n    is_final_output: bool\n    \"\"\"Whether this is the final output. If False, the LLM will run again and receive the tool call\n    output.\n    \"\"\"\n\n    final_output: Any | None = None\n    \"\"\"The final output. Can be None if `is_final_output` is False, otherwise must match the\n    `output_type` of the agent.\n    \"\"\"\n</code></pre> <p>Key Point: The docstring says <code>final_output</code> must match the <code>output_type</code> of the agent but this is a documentation requirement, not an enforced runtime check.</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#2-toolstofinaloutputfunction-typealias","title":"2. <code>ToolsToFinalOutputFunction</code> TypeAlias","text":"Python<pre><code>ToolsToFinalOutputFunction: TypeAlias = Callable[\n    [RunContextWrapper[TContext], list[FunctionToolResult]],\n    MaybeAwaitable[ToolsToFinalOutputResult],\n]\n\"\"\"A function that takes a run context and a list of tool results, and returns a\n`ToolsToFinalOutputResult`.\n\"\"\"\n</code></pre> <p>This defines the signature for custom tool use behavior functions that return <code>ToolsToFinalOutputResult</code>.</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#3-critical-implementation-logic","title":"3. Critical Implementation Logic","text":"Python<pre><code># Next, we'll check if the tool use should result in a final output\ncheck_tool_use = await cls._check_for_final_output_from_tools(\n    agent=agent,\n    tool_results=function_results,\n    context_wrapper=context_wrapper,\n    config=run_config,\n)\n\nif check_tool_use.is_final_output:\n    # If the output type is str, then let's just stringify it\n    if not agent.output_type or agent.output_type is str:\n        check_tool_use.final_output = str(check_tool_use.final_output)\n\n    if check_tool_use.final_output is None:\n        logger.error(\n            \"Model returned a final output of None. Not raising an error because we assume\"\n            \"you know what you're doing.\"\n        )\n\n    return await cls.execute_final_output(\n        agent=agent,\n        original_input=original_input,\n        new_response=new_response,\n        pre_step_items=pre_step_items,\n        new_step_items=new_step_items,\n        final_output=check_tool_use.final_output,\n        hooks=hooks,\n        context_wrapper=context_wrapper,\n    )\n</code></pre> <p>Analysis: 1. The loop terminates only if <code>check_tool_use.is_final_output</code> is <code>True</code>. 2. If there's no <code>output_type</code> or it's <code>str</code>, the system stringifies the output. 3. If <code>final_output</code> is <code>None</code>, it logs an error but still terminates. 4. The loop ends by calling <code>execute_final_output()</code>.</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#4-custom-tool-use-behavior-execution","title":"4. Custom Tool Use Behavior Execution","text":"Python<pre><code>elif callable(agent.tool_use_behavior):\n    if inspect.iscoroutinefunction(agent.tool_use_behavior):\n        return await cast(\n            Awaitable[ToolsToFinalOutputResult],\n            agent.tool_use_behavior(context_wrapper, tool_results),\n        )\n    else:\n        return cast(\n            ToolsToFinalOutputResult, agent.tool_use_behavior(context_wrapper, tool_results)\n        )\n</code></pre> <p>Custom <code>ToolsToFinalOutputFunction</code> returns a <code>ToolsToFinalOutputResult</code> directly, with no additional validation.</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#why-answer-a-is-correct","title":"Why Answer A Is Correct","text":"<ul> <li>No Type Validation: There is no runtime type check on <code>final_output</code> from a <code>ToolsToFinalOutputFunction</code>.</li> <li>Single Condition: The system only checks <code>is_final_output</code> to terminate.</li> <li>Documentation vs Implementation: Type matching is suggested in docs, but not enforced.</li> <li>Error Handling: Even if <code>final_output</code> is <code>None</code> or mismatched, the loop still terminates (just logs an error).</li> </ul>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#conclusion","title":"Conclusion","text":"<p>The agent loop termination is determined solely by <code>is_final_output=True</code>, with no runtime validation of <code>final_output</code> against the agent's <code>output_type</code>.</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%238.%20tools_to_final_output_result/#notes","title":"Notes","text":"<p>Type validation logic applies only to model responses, not tool function results. This happens in separate code paths when <code>is_final_output</code> is <code>False</code>.</p>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2380.%20value_error_in_input_guardrail/","title":"#80. Value Error In Input Guardrail","text":"Source code in OpAgentsOlympus/practice/#80. value_error_in_input_guardrail.py OpAgentsOlympus/practice/#80. value_error_in_input_guardrail.py<pre><code>import asyncio\nfrom agents import (\n    Agent,\n    Runner,\n    function_tool,\n    input_guardrail,\n    GuardrailFunctionOutput,\n)\n\n\n@input_guardrail\nasync def failing_guardrail(ctx, agent, input_data):\n    if \"cascade\" in str(input_data).lower():\n        raise ValueError(\"Cascading validation failure\")\n    return GuardrailFunctionOutput(output_info=\"Valid\", tripwire_triggered=False)\n\n\n@function_tool\nasync def nested_operation(query: str) -&gt; str:\n    if \"nested_fail\" in query:\n        raise RuntimeError(\"Nested operation failed\")\n    return f\"Success: {query}\"\n\n\nasync def test_valueerror_no_context():\n    agent = Agent(\n        name=\"TestAgent\",\n        tools=[nested_operation],\n        input_guardrails=[failing_guardrail],\n        tool_use_behavior=\"run_llm_again\",\n    )\n\n    result = Runner.run_streamed(agent, \"cascade nested_fail test\")\n\n    try:\n        async for event in result.stream_events():\n            pass\n    except Exception as e:\n        exception_type = type(e).__name__\n        has_run_data = hasattr(e, \"run_data\")\n        guardrail_results = e.run_data.input_guardrail_results if has_run_data else None\n\n        print(f\"Exception: {exception_type}\")\n        print(f\"Has run_data: {has_run_data}\")\n        print(f\"Guardrail results: {guardrail_results}\")\n\n        # This validates our expectation: ValueError with no context\n        assert exception_type == \"ValueError\"\n        assert not has_run_data\n        assert guardrail_results is None\n\n        return exception_type, has_run_data, guardrail_results\n\n\n# Run the test\nif __name__ == \"__main__\":\n    result = asyncio.run(test_valueerror_no_context())\n    print(f\"Result: {result}\")  # Expected: ('ValueError', False, None)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/%2390.%20span_hierarchy_preserved/","title":"#90. Span Hierarchy Preserved","text":"Source code in OpAgentsOlympus/practice/#90. span_hierarchy_preserved.py OpAgentsOlympus/practice/#90. span_hierarchy_preserved.py<pre><code>import asyncio  \nfrom agents import trace  \nfrom agents.tracing import agent_span, function_span, generation_span  \nfrom agents.tracing.scope import Scope\nimport dotenv\n\ndotenv.load_dotenv()\n\nasync def demonstrate_span_hierarchy():  \n    print(\"=== Demonstrating Span Hierarchy ===\")  \n\n    # This will create the main trace that Runner uses  \n    with trace(\"span_hierarchy_demo\") as main_trace:  \n        print(f\"Main trace started: {main_trace.trace_id}\")  \n\n        # Create spans manually within the Runner context  \n        parent_span = agent_span(\"parent_agent\")  \n        parent_span.start(mark_as_current=True)  \n        print(f\"Parent span started: {parent_span.span_id}\")  \n        print(f\"Current span after parent start: {Scope.get_current_span()}\")  \n\n        child_span = function_span(\"tool_call\")  \n        child_span.start(mark_as_current=True)  \n        print(f\"Child span started: {child_span.span_id}\")  \n        print(f\"Current span after child start: {Scope.get_current_span()}\")  \n\n        # Create another span that becomes current  \n        other_span = generation_span(  \n            input=[{\"role\": \"user\", \"content\": \"manual test\"}],  \n            model=\"gpt-4o-mini\"  \n        )\n\n        other_span.start(mark_as_current=True)  \n        print(f\"Other span started: {other_span.span_id}\")  \n        print(f\"Current span after other start: {Scope.get_current_span()}\")  \n\n        # Finish child_span while other_span is current  \n        print(f\"About to finish child_span while other_span is current\")  \n        child_span.finish()  \n        print(f\"Child span finished. Current span: {Scope.get_current_span()}\")  \n\n        # Verify the hierarchy is preserved  \n        print(f\"Child span parent_id: {child_span.parent_id}\")  \n        print(f\"Parent span span_id: {parent_span.span_id}\")  \n        print(f\"Other span parent_id: {other_span.parent_id}\")  \n\n        # Clean up remaining spans  \n        other_span.finish()  \n        parent_span.finish()  \n\n# Run the demonstration  \nasyncio.run(demonstrate_span_hierarchy())\n\n# Output:\n# === Demonstrating Span Hierarchy ===\n# Main trace started: trace_9244edff272d4532ad02c1ccf777ac29\n# Parent span started: span_810c48d032ee4fcab115520a\n# Current span after parent start: &lt;agents.tracing.spans.SpanImpl object at 0x0000029C9E592DD0&gt;\n# Child span started: span_0ff1add6b7e043f899883cac\n# Current span after child start: &lt;agents.tracing.spans.SpanImpl object at 0x0000029C9E592E50&gt;\n# Other span started: span_6e3091714f894fb38a34b1e7\n# Current span after other start: &lt;agents.tracing.spans.SpanImpl object at 0x0000029C9E592ED0&gt;\n# About to finish child_span while other_span is current\n# Child span finished. Current span: &lt;agents.tracing.spans.SpanImpl object at 0x0000029C9E592ED0&gt;\n# Child span parent_id: span_810c48d032ee4fcab115520a\n# Parent span span_id: span_810c48d032ee4fcab115520a\n# Other span parent_id: span_0ff1add6b7e043f899883cac\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/config/","title":"Config","text":"Source code in OpAgentsOlympus/practice/config.py OpAgentsOlympus/practice/config.py<pre><code>import os\n\ntry:\n    from dotenv import load_dotenv, find_dotenv\n    from agents import AsyncOpenAI, OpenAIChatCompletionsModel\n    from agents.run import RunConfig\nexcept ImportError:\n    raise ImportError(\n        \"\\nThis package requires 'openai-agents' to be installed.\\n\"\n        \"\\nPlease install it first using pip:\\n\"\n        \"\\npip install openai-agents\\n\"\n        \"\\nFor more information, visit: https://openai.github.io/openai-agents-PyDeepOlympus/quickstart/\\n\"\n    )\n\n# Load environment variables\nload_dotenv(find_dotenv())\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif not gemini_api_key:\n    raise ValueError(\"GEMINI_API_KEY is not set. Please define it in your .env file.\")\n\n# Setup Gemini client\nexternal_client = AsyncOpenAI(\n    api_key=gemini_api_key,\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n)\n\n# Preferred Gemini model setup\nmodel = OpenAIChatCompletionsModel(\n    model=\"gemini-1.5-flash\", openai_client=external_client\n)\n\n# Runner config (you can export this)\nconfig = RunConfig(model=model, model_provider=external_client)\n</code></pre>"},{"location":"OpAgentsOlympus/practice/100-mcqs-answer/open_router_config/","title":"Open Router Config","text":"Source code in OpAgentsOlympus/practice/open_router_config.py OpAgentsOlympus/practice/open_router_config.py<pre><code>import os\n\ntry:\n    from dotenv import load_dotenv, find_dotenv\n    from agents import AsyncOpenAI, OpenAIChatCompletionsModel\n    from agents.run import RunConfig\nexcept ImportError:\n    raise ImportError(\n        \"\\nThis package requires 'openai-agents' to be installed.\\n\"\n        \"\\nPlease install it first using pip:\\n\"\n        \"\\npip install openai-agents\\n\"\n        \"\\nFor more information, visit: https://openai.github.io/openai-agents-PyDeepOlympus/quickstart/\\n\"\n    )\n\n# Load environment variables\nload_dotenv(find_dotenv())\n\nAPI_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\nBASE_URL = \"https://openrouter.ai/api/v1\"\nMODEL = \"openai/gpt-4o-mini\"\n\nmodel = OpenAIChatCompletionsModel(\n    model=MODEL, openai_client=AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)\n)\n# Runner config (you can export this)\nconfig = RunConfig(model=model)\n</code></pre>"},{"location":"PyDeepOlympus/DeepStringInterning/","title":"String Interning","text":""},{"location":"PyDeepOlympus/DeepStringInterning/#question","title":"Question:","text":"<p>What is the output of the following code?</p> Python<pre><code>a = 'hello' * 2  \nb = 'hello' + 'hello'  \nprint(a is b)\n</code></pre>"},{"location":"PyDeepOlympus/DeepStringInterning/#answer-choices","title":"Answer Choices:","text":"<ul> <li>True  </li> <li>False  </li> <li>It Depends on the Python Version  </li> <li>It Depends on the Environment  </li> </ul>"},{"location":"PyDeepOlympus/DeepStringInterning/#my-answer","title":"My Answer:","text":"<p>The question doesn\u2019t mention a specific Python version or execution environment, so we can\u2019t assume either. That rules out both <code>True</code> and <code>False</code> as definitive answers.</p> <p>The option \"It Depends on the Python Version\" is partially accurate, but it\u2019s too narrow. Version alone doesn\u2019t determine the result \u2014 the execution environment and CPython\u2019s optimizations also play a major role.</p> <p>\u27a1\ufe0f Most accurate answer: \u2705 It Depends on the Environment</p>"},{"location":"PyDeepOlympus/DeepStringInterning/#if-youre-thinking-like-a-genius-the-real-answer-is","title":"If You\u2019re Thinking Like a Genius, the Real Answer Is:","text":"<p>\ud83d\udca1 It depends on the Python Environment, Version, and Interning Behavior.</p>"},{"location":"PyDeepOlympus/DeepStringInterning/#why-lets-break-it-down","title":"\ud83e\udde0 Why? Let\u2019s Break It Down:","text":""},{"location":"PyDeepOlympus/DeepStringInterning/#environment","title":"\ud83d\udd39 Environment:","text":"<ul> <li> <p>Script file (<code>.py</code>): Python will often fold constants (like <code>'hello'*2</code> and <code>'hello'+'hello'</code>) during compilation, especially in Python 3.7+. This leads to interned strings, and <code>a is b</code> could be <code>True</code>.</p> </li> <li> <p>REPL (e.g., Jupyter, Python shell, Google Colab): Expressions are evaluated at runtime, so <code>'hello'*2</code> and <code>'hello'+'hello'</code> may create two different string objects, making <code>a is b</code> return <code>False</code>.</p> </li> </ul> <p>\u26a0\ufe0f Even in REPL, identifier-like strings (like <code>'hellohello'</code>) might be interned, leading to <code>True</code> occasionally.</p>"},{"location":"PyDeepOlympus/DeepStringInterning/#version","title":"\ud83d\udd39 Version:","text":"<ul> <li> <p>Before Python 3.7: The peephole optimizer didn\u2019t aggressively fold strings, so <code>a is b</code> was more likely <code>False</code>.</p> </li> <li> <p>Python 3.7 and beyond: Introduction of the AST optimizer increased chances of folding expressions like <code>'hello' * 2</code> and <code>'hello' + 'hello'</code> into the same object \u2014 especially in scripts \u2014 making <code>True</code> more likely.</p> </li> </ul>"},{"location":"PyDeepOlympus/DeepStringInterning/#interning-behavior","title":"\ud83d\udd39 Interning Behavior:","text":"<ul> <li> <p>Python automatically interns certain strings \u2014 usually identifier-like ones (alphanumeric, no spaces or special chars).</p> </li> <li> <p>Non-identifier strings (e.g., <code>'hello world'</code>) are less likely to be interned, especially in dynamic environments like Colab or Jupyter.</p> </li> </ul> <p>Different Python implementations (like PyPy, IronPython) may handle string interning differently \u2014 this behavior is a CPython-specific optimization, not a language guarantee.</p>"},{"location":"PyDeepOlympus/PyBeTheInterpreter/","title":"Bytecode Interpreter","text":""},{"location":"PyDeepOlympus/PyBeTheInterpreter/#deliberate-mental-simulation-of-code-execution-be-the-python-interpreter","title":"Deliberate Mental Simulation of Code Execution: Be The Python Interpreter!","text":"<p>Your brain becomes the interpreter itself. You don\u2019t just read what <code>a += 2</code> means, You mentally execute it. You start questioning every single detail, not just accepting what\u2019s written but breaking it down like the compiler would.</p> <p>Even for a small line like:</p> Text Only<pre><code>a += 2\n</code></pre> <p>You naturally start asking yourself:</p> <ul> <li>what if? <code>a + = 2</code></li> <li>what if? <code>a =+ 2</code></li> <li>what if? <code>a = + 2</code></li> <li>what if? <code>2 += a</code></li> <li>what if? <code>2 + = a</code></li> <li>what if? <code>2 = + a</code></li> <li>what if? <code>2 =+ a</code></li> <li>why not? <code>2 + a</code></li> <li>why not? <code>a = 2</code></li> <li>why not? <code>a + 2</code></li> </ul> <p>You\u2019re no longer depending on practice to tell you what\u2019s right or wrong. You mentally simulate syntax, behavior, side effects, and errors just like the Python runtime would. When you see a line of code, you parse it, and predict the output.</p> <p>If you reach this level, then reading theory becomes as effective as coding practically.</p> <p>You\u2019re not passively reading anymore you\u2019re actively interpreting.</p> <p>You don\u2019t need to try it in an editor to be sure. You already know what works, what fails, and why.</p> <p>That\u2019s when theory stops being theory, It becomes internalized execution logic.</p> <p>And you stop needing the keyboard to prove yourself right.</p>"},{"location":"PyDeepOlympus/PyDiffAbs%26Encps/","title":"Abstraction & Encapsulation","text":""},{"location":"PyDeepOlympus/PyDiffAbs%26Encps/#the-main-difference-between-encapsulation-and-abstraction","title":"The main difference between Encapsulation and Abstraction!","text":""},{"location":"PyDeepOlympus/PyDiffAbs%26Encps/#confusion","title":"Confusion:","text":"<ul> <li>Conflicting Behavior like both HIDES!!</li> </ul>"},{"location":"PyDeepOlympus/PyDiffAbs%26Encps/#bundling-data-and-methods-into-a-single-private-unit-this-is-encapsulation","title":"Bundling data and methods into a single private unit: This is encapsulation.","text":""},{"location":"PyDeepOlympus/PyDiffAbs%26Encps/#concealing-the-inner-workings-and-exposing-only-what-is-necessary-this-is-abstraction","title":"Concealing the inner workings and exposing only what is necessary: This is abstraction.","text":"<p>In the two statements above i didn't used word \"HIDE\", But if you understand the context and meaning, Then i have used word \"HIDE\" implicitly in both statement's!!</p> <p>Encapsulation does involve hiding internal details (e,g Bundling/Hiding/Packing data &amp; methods), its primary purpose is to bundle and protect the data and methods, not necessarily to simplify the interface for the user. Abstraction, however, is all about simplifying the interaction by hiding complexity and showing only the essential parts.</p> Aspect Encapsulation Abstraction Purpose Protect/bundle data and methods as a single unit Simplify by exposing only necessary information Hides Internal state, implementation details Complexity, irrelevant details"},{"location":"PyDeepOlympus/PyIsSuperClass/","title":"Super Class","text":""},{"location":"PyDeepOlympus/PyIsSuperClass/#most-tutorials-call-super-a-function-but-its-actually-a-class","title":"Most Tutorials Call <code>super()</code> a Function But It\u2019s Actually a Class","text":"<p>Many tutorials explain how to use <code>super()</code> and highlight its benefits, but they often use imprecise terminology. For example:</p> <ul> <li>\ud83d\udd39 DigitalOcean refers to it as the <code>super()</code> function</li> <li>\ud83d\udd39 GeeksforGeeks also calls it a super() function</li> <li>\ud83d\udd39 Real Python casually uses the same term in its explanations</li> </ul> <p>These descriptions aren't technically wrong in terms of simplicity, but they can be misleading.</p>"},{"location":"PyDeepOlympus/PyIsSuperClass/#what-does-python-itself-say","title":"What Does Python Itself Say?","text":"<p>The official Python documentation and the source code of CPython treat <code>super</code> as a built-in class.</p> <p><code>super()</code> acts like a function call, but behind the scenes, you are instantiating the super class that returns a proxy object</p>"},{"location":"PyDeepOlympus/PyIsSuperClass/#proof-super-is-a-class-cpython-source","title":"Proof: <code>super</code> is a Class (CPython Source)","text":"<p>Below is the actual implementation of <code>super</code> in CPython, written in C. This makes it crystal clear: <code>super</code> is defined as a <code>PyTypeObject</code>, which is how Python internally represents classes.</p> C<pre><code>PyTypeObject PySuper_Type = {\n    PyVarObject_HEAD_INIT(&amp;PyType_Type, 0)\n    \"super\",                                    /* tp_name */\n    sizeof(superobject),                        /* tp_basicsize */\n    0,                                          /* tp_itemsize */\n    /* methods */\n    super_dealloc,                              /* tp_dealloc */\n    0,                                          /* tp_vectorcall_offset */\n    0,                                          /* tp_getattr */\n    0,                                          /* tp_setattr */\n    0,                                          /* tp_as_async */\n    super_repr,                                 /* tp_repr */\n    0,                                          /* tp_as_number */\n    0,                                          /* tp_as_sequence */\n    0,                                          /* tp_as_mapping */\n    0,                                          /* tp_hash */\n    0,                                          /* tp_call */\n    0,                                          /* tp_str */\n    super_getattro,                             /* tp_getattro */\n    0,                                          /* tp_setattro */\n    0,                                          /* tp_as_buffer */\n    Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HAVE_GC |\n        Py_TPFLAGS_BASETYPE,                    /* tp_flags */\n    super_doc,                                  /* tp_doc */\n    super_traverse,                             /* tp_traverse */\n    0,                                          /* tp_clear */\n    0,                                          /* tp_richcompare */\n    0,                                          /* tp_weaklistoffset */\n    0,                                          /* tp_iter */\n    0,                                          /* tp_iternext */\n    0,                                          /* tp_methods */\n    super_members,                              /* tp_members */\n    0,                                          /* tp_getset */\n    0,                                          /* tp_base */\n    0,                                          /* tp_dict */\n    super_descr_get,                            /* tp_descr_get */\n    0,                                          /* tp_descr_set */\n    0,                                          /* tp_dictoffset */\n    super_init,                                 /* tp_init */\n    PyType_GenericAlloc,                        /* tp_alloc */\n    PyType_GenericNew,                          /* tp_new */\n    PyObject_GC_Del,                            /* tp_free */\n    .tp_vectorcall = super_vectorcall,\n};\n</code></pre>"},{"location":"PyDeepOlympus/PyIsSuperClass/#sources-i-believe","title":"Sources I Believe!","text":"<ul> <li>CPython Source: https://github.com/PyDeepOlympus/cPyDeepOlympus/blob/main/Objects/typeobject.c</li> <li>Official Documentation: https://docs.python.org/3/library/functions.html#super</li> </ul>"},{"location":"PyDeepOlympus/PyMROC3Algo/","title":"MRO C3 Algorithm","text":""},{"location":"PyDeepOlympus/PyMROC3Algo/#disclaimer-this-is-going-to-be-complicated","title":"Disclaimer: This is going to be Complicated!","text":"<p>Let\u2019s figure out why this Python code raises a TypeError:</p>"},{"location":"PyDeepOlympus/PyMROC3Algo/#python-1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18class-a-def-processself-printfrom-a-class-ba-def-processself-printfrom-b-superprocess-class-ca-def-processself-printfrom-c-superprocess-class-firstb-c-pass-class-secondc-b-pass-class-combinedfirst-second-pass-typeerror","title":"Python<pre><code>class A:\n    def process(self):\n        print(\"From: A\")\n\nclass B(A):\n    def process(self):\n        print(\"From: B\")\n        super().process()\n\nclass C(A):\n    def process(self):\n        print(\"From: C\")\n        super().process()\n\nclass First(B, C): pass\nclass Second(C, B): pass\n\nclass Combined(First, Second): pass  # TypeError\n</code></pre>","text":""},{"location":"PyDeepOlympus/PyMROC3Algo/#1-what-is-mro-and-c3-linearization","title":"1. What is MRO and C3 Linearization?","text":"<ul> <li>MRO stands for Method Resolution Order. It\u2019s the sequence Python uses to look for methods or attributes in a class and its parents. For example, if <code>Combined</code> needs a method, Python follows the MRO to decide where to look first, second, and so on.</li> <li>When a class inherits from multiple parents (like <code>First</code> and <code>Second</code> here), Python uses an algorithm called C3 linearization to build this sequence. It has to be consistent and make sense.</li> <li>In C3, we merge lists (the MROs of the parent classes plus the list of parents) into one order. Each list has a head (the first item) and a tail (everything after the head). The rule is: Pick a head only if it\u2019s not in any other list\u2019s tail. If we can\u2019t pick anything, the merge fails, and we get a <code>TypeError</code>.</li> </ul>"},{"location":"PyDeepOlympus/PyMROC3Algo/#2-what-are-the-mros-of-first-and-second","title":"2. What Are the MROs of <code>First</code> and <code>Second</code>?","text":"<p>let\u2019s figure out the MROs for its parents: <code>First</code> and <code>Second</code>.</p> <ul> <li><code>First</code> is <code>class First(B, C)</code>:</li> <li>It inherits from <code>B</code> and <code>C</code>, in that order.</li> <li><code>B</code> inherits from <code>A</code>.</li> <li><code>C</code> inherits from <code>A</code>.</li> <li>Python starts with <code>First</code>, then follows the order of parents: <code>B</code>, then <code>C</code>, then <code>A</code> (since both <code>B</code> and <code>C</code> lead to <code>A</code>), and finally <code>object</code> (the ultimate base class in Python).</li> <li> <p>So, the MRO is: <code>[First, B, C, A, object]</code>.</p> </li> <li> <p><code>Second</code> is <code>class Second(C, B)</code>:</p> </li> <li>It inherits from <code>C</code> and <code>B</code>, in that order.</li> <li><code>C</code> inherits from <code>A</code>.</li> <li><code>B</code> inherits from <code>A</code>.</li> <li>Starting with <code>Second</code>, it goes <code>C</code>, then <code>B</code>, then <code>A</code>, and <code>object</code>.</li> <li>So, the MRO is: <code>[Second, C, B, A, object]</code>.</li> </ul> <p>Notice the difference: <code>First</code> has <code>B</code> before <code>C</code>, while <code>Second</code> has <code>C</code> before <code>B</code>.</p>"},{"location":"PyDeepOlympus/PyMROC3Algo/#3-why-does-combined-fail","title":"3. Why Does <code>Combined</code> Fail?","text":"<p>Now, <code>Combined</code> is defined as <code>class Combined(First, Second)</code>. Python needs to build its MRO by merging: - The MRO of <code>First</code>: <code>[First, B, C, A, object]</code> - The MRO of <code>Second</code>: <code>[Second, C, B, A, object]</code> - The list of parents: <code>[First, Second]</code></p> <p>The MRO starts with <code>Combined</code>, then merges these three lists using the C3 rule. Let\u2019s do it step by step:</p>"},{"location":"PyDeepOlympus/PyMROC3Algo/#starting-lists","title":"Starting Lists:","text":"Text Only<pre><code>L1 = [First, B, C, A, object]    # MRO of First\nL2 = [Second, C, B, A, object]   # MRO of Second\nL3 = [First, Second]             # Parents of Combined\n</code></pre>"},{"location":"PyDeepOlympus/PyMROC3Algo/#step-1-pick-the-first-item","title":"Step 1: Pick the First Item","text":"<ul> <li>Heads: <code>First</code> (L1), <code>Second</code> (L2), <code>First</code> (L3).</li> <li>Check <code>First</code>:</li> <li>Tail of L1: <code>[B, C, A, object]</code> \u2192 no <code>First</code>.</li> <li>Tail of L2: <code>[C, B, A, object]</code> \u2192 no <code>First</code>.</li> <li>Tail of L3: <code>[Second]</code> \u2192 no <code>First</code>.</li> <li><code>First</code> isn\u2019t in any tail, so pick it.</li> <li>New MRO: <code>[Combined, First]</code>.</li> <li>Update lists:   Text Only<pre><code>L1 = [B, C, A, object]\nL2 = [Second, C, B, A, object]\nL3 = [Second]\n</code></pre></li> </ul>"},{"location":"PyDeepOlympus/PyMROC3Algo/#step-2-pick-the-next-item","title":"Step 2: Pick the Next Item","text":"<ul> <li>Heads: <code>B</code> (L1), <code>Second</code> (L2), <code>Second</code> (L3).</li> <li>Check <code>B</code>:</li> <li>Tail of L1: <code>[C, A, object]</code> \u2192 no <code>B</code>.</li> <li>Tail of L2: <code>[C, B, A, object]</code> \u2192 has <code>B</code>.</li> <li>Tail of L3: <code>[]</code> \u2192 no <code>B</code>.</li> <li><code>B</code> is in L2\u2019s tail, so we can\u2019t pick it.</li> <li>Check <code>Second</code>:</li> <li>Tail of L1: <code>[C, A, object]</code> \u2192 no <code>Second</code>.</li> <li>Tail of L2: <code>[C, B, A, object]</code> \u2192 no <code>Second</code>.</li> <li>Tail of L3: <code>[]</code> \u2192 no <code>Second</code>.</li> <li><code>Second</code> isn\u2019t in any tail, so pick it.</li> <li>New MRO: <code>[Combined, First, Second]</code>.</li> <li>Update lists:   Text Only<pre><code>L1 = [B, C, A, object]\nL2 = [C, B, A, object]\nL3 = []  # Empty now\n</code></pre></li> </ul>"},{"location":"PyDeepOlympus/PyMROC3Algo/#step-3-pick-the-next-item","title":"Step 3: Pick the Next Item","text":"<ul> <li>Heads: <code>B</code> (L1), <code>C</code> (L2). (L3 is empty, so ignore it.)</li> <li>Check <code>B</code>:</li> <li>Tail of L1: <code>[C, A, object]</code> \u2192 no <code>B</code>.</li> <li>Tail of L2: <code>[B, A, object]</code> \u2192 has <code>B</code>.</li> <li><code>B</code> is in L2\u2019s tail, so we can\u2019t pick it.</li> <li>Check <code>C</code>:</li> <li>Tail of L1: <code>[C, A, object]</code> \u2192 has <code>C</code>.</li> <li>Tail of L2: <code>[B, A, object]</code> \u2192 no <code>C</code>.</li> <li><code>C</code> is in L1\u2019s tail, so we can\u2019t pick it.</li> </ul>"},{"location":"PyDeepOlympus/PyMROC3Algo/#problem","title":"Problem!","text":"<ul> <li><code>B</code> can\u2019t be picked because it\u2019s in L2\u2019s tail (<code>[B, A, object]</code>).</li> <li><code>C</code> can\u2019t be picked because it\u2019s in L1\u2019s tail (<code>[C, A, object]</code>).</li> <li>There\u2019s no head we can choose! The merge gets stuck.</li> </ul>"},{"location":"PyDeepOlympus/PyMROC3Algo/#4-why-the-typeerror","title":"4. Why the TypeError?","text":"<p>Python\u2019s C3 algorithm fails when it can\u2019t find a consistent order. Here: - <code>First</code> wants <code>B</code> before <code>C</code> (from <code>[First, B, C, A, object]</code>). - <code>Second</code> wants <code>C</code> before <code>B</code> (from <code>[Second, C, B, A, object]</code>). - <code>Combined</code> inherits from both, but <code>B</code> and <code>C</code> can\u2019t agree on who comes first, and neither can be skipped or ignored.</p> <p>Since Python can\u2019t resolve this conflict, it raises:</p> Text Only<pre><code>TypeError: Cannot create a consistent method resolution order (MRO) for bases First, Second\n</code></pre> <p>Bones: Java doesn't support multiple inheritance specifically to avoid the complexity and ambiguity caused by the Diamond Problem.</p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/","title":"Bitwise Operations","text":""},{"location":"PyDeepOlympus/PyUnderstandBitwise/#what-are-bitwise-operations","title":"What Are Bitwise Operations?","text":"<p>Bitwise operations work directly on the binary representations of numbers (or other data). Instead of treating numbers as numerical values, we manipulate their individual bits (0s and 1s). These operations are fast, low-level, and super useful in scenarios like:</p> <ul> <li>Optimizing code for performance.</li> <li>Working with hardware, networking, or cryptography.</li> <li>Manipulating flags, permissions, or pixel data in images.</li> </ul> <p>Python provides six main bitwise operators: AND, OR, XOR, NOT, Left Shift, and Right Shift. Let\u2019s learn each one with examples.</p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#step-1-understanding-binary-numbers","title":"Step 1: Understanding Binary Numbers","text":"<p>Before we jump into the operators, you need to know how numbers are represented in binary. For example:</p> <ul> <li>The number <code>5</code> in binary is <code>0101</code>.</li> <li>The number <code>3</code> in binary is <code>0011</code>.</li> </ul> <p>Each digit in a binary number is a bit. The rightmost bit is the least significant bit (LSB), and the leftmost is the most significant bit (MSB).</p> <p>You can convert integers to binary in Python using the <code>bin()</code> function:</p> Python<pre><code>print(bin(5))  # Output: 0b101 (0b indicates binary)\nprint(bin(3))  # Output: 0b11\n</code></pre>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#step-2-the-bitwise-operators","title":"Step 2: The Bitwise Operators","text":"<p>Let\u2019s explore each bitwise operator, what it does, and how to use it in Python.</p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#1-bitwise-and","title":"1. Bitwise AND (<code>&amp;</code>)","text":"<ul> <li>What it does: Compares each bit of two numbers. If both bits are <code>1</code>, the result is <code>1</code>; otherwise, it\u2019s <code>0</code>.</li> <li>Use case: Masking (selecting specific bits) or checking if certain bits are set.</li> </ul> <p>Example: Python<pre><code>a = 5  # 0101\nb = 3  # 0011\nresult = a &amp; b  # 0101 &amp; 0011 = 0001\nprint(result)  # Output: 1\n</code></pre></p> <p>How it works: Text Only<pre><code>  0101  (5)\n&amp; 0011  (3)\n-------\n  0001  (1)\n</code></pre></p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#2-bitwise-or","title":"2. Bitwise OR (<code>|</code>)","text":"<ul> <li>What it does: Compares each bit of two numbers. If at least one bit is <code>1</code>, the result is <code>1</code>; otherwise, it\u2019s <code>0</code>.</li> <li>Use case: Setting specific bits or combining flags.</li> </ul> <p>Example: Python<pre><code>a = 5  # 0101\nb = 3  # 0011\nresult = a | b  # 0101 | 0011 = 0111\nprint(result)  # Output: 7\n</code></pre></p> <p>How it works: Text Only<pre><code>  0101  (5)\n| 0011  (3)\n-------\n  0111  (7)\n</code></pre></p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#3-bitwise-xor","title":"3. Bitwise XOR (<code>^</code>)","text":"<ul> <li>What it does: Compares each bit of two numbers. If the bits are different (one is <code>1</code>, the other is <code>0</code>), the result is <code>1</code>; otherwise, it\u2019s <code>0</code>.</li> <li>Use case: Toggling bits, encryption, or finding unique elements.</li> </ul> <p>Example: Python<pre><code>a = 5  # 0101\nb = 3  # 0011\nresult = a ^ b  # 0101 ^ 0011 = 0110\nprint(result)  # Output: 6\n</code></pre></p> <p>How it works: Text Only<pre><code>  0101  (5)\n^ 0011  (3)\n-------\n  0110  (6)\n</code></pre></p> <p>Fun Fact: XORing a number with itself gives <code>0</code>, and XORing a number with <code>0</code> gives the number itself.</p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#4-bitwise-not","title":"4. Bitwise NOT (<code>~</code>)","text":"<ul> <li>What it does: Flips all the bits of a number (<code>0</code> becomes <code>1</code>, <code>1</code> becomes <code>0</code>). In Python, this is equivalent to <code>-(x + 1)</code> due to how negative numbers are represented (two\u2019s complement).</li> <li>Use case: Inverting bits or computing complements.</li> </ul> <p>Example: Python<pre><code>a = 5  # 0101\nresult = ~a  # ~0101 = ...11111010 (in two\u2019s complement, this is -6)\nprint(result)  # Output: -6\n</code></pre></p> <p>Explanation: - For a number <code>x</code>, <code>~x = -(x + 1)</code>. - So, <code>~5 = -(5 + 1) = -6</code>.</p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#5-left-shift","title":"5. Left Shift (<code>&lt;&lt;</code>)","text":"<ul> <li>What it does: Shifts all bits of a number to the left by a specified number of positions. Zeros are filled in from the right. This is equivalent to multiplying by <code>2^n</code> (where <code>n</code> is the shift amount).</li> <li>Use case: Fast multiplication or aligning bits.</li> </ul> <p>Example: Python<pre><code>a = 5  # 0101\nresult = a &lt;&lt; 2  # 0101 &lt;&lt; 2 = 010100 (shift left by 2)\nprint(result)  # Output: 20\n</code></pre></p> <p>How it works: Text Only<pre><code>0101 &lt;&lt; 2 = 010100\n5 * (2^2) = 5 * 4 = 20\n</code></pre></p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#6-right-shift","title":"6. Right Shift (<code>&gt;&gt;</code>)","text":"<ul> <li>What it does: Shifts all bits of a number to the right by a specified number of positions. For positive numbers, zeros are filled in from the left. This is equivalent to dividing by <code>2^n</code> (integer division).</li> <li>Use case: Fast division or extracting specific bits.</li> </ul> <p>Example: Python<pre><code>a = 20  # 10100\nresult = a &gt;&gt; 2  # 10100 &gt;&gt; 2 = 00101\nprint(result)  # Output: 5\n</code></pre></p> <p>How it works: Text Only<pre><code>10100 &gt;&gt; 2 = 00101\n20 // (2^2) = 20 // 4 = 5\n</code></pre></p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#step-3-practical-examples","title":"Step 3: Practical Examples","text":"<p>Let\u2019s apply bitwise operations to solve some real-world problems.</p>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#example-1-checking-if-a-number-is-even-or-odd","title":"Example 1: Checking if a Number is Even or Odd","text":"<ul> <li>A number is even if its least significant bit (LSB) is <code>0</code>, and odd if it\u2019s <code>1</code>.</li> <li>We can use <code>&amp;</code> with <code>1</code> to check the LSB.</li> </ul> Python<pre><code>def is_even(n):\n    return (n &amp; 1) == 0\n\nprint(is_even(4))  # True (4 is 100, LSB is 0)\nprint(is_even(7))  # False (7 is 111, LSB is 1)\n</code></pre>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#example-2-swapping-two-numbers-without-a-temporary-variable","title":"Example 2: Swapping Two Numbers Without a Temporary Variable","text":"<ul> <li>XOR can be used to swap values efficiently.</li> </ul> Python<pre><code>a = 5  # 0101\nb = 3  # 0011\na ^= b  # a = 0101 ^ 0011 = 0110\nb ^= a  # b = 0011 ^ 0110 = 0101\na ^= b  # a = 0110 ^ 0101 = 0011\nprint(a, b)  # Output: 3 5\n</code></pre>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#example-3-setting-and-clearing-bits","title":"Example 3: Setting and Clearing Bits","text":"<ul> <li>Use <code>|</code> to set a bit (turn it to <code>1</code>) and <code>&amp;</code> with <code>~</code> to clear a bit (turn it to <code>0</code>).</li> </ul> Python<pre><code># Set the 2nd bit (position 1, counting from 0)\nnumber = 8  # 1000\nmask = 1 &lt;&lt; 1  # 0010\nnumber |= mask  # 1000 | 0010 = 1010\nprint(number)  # Output: 10\n\n# Clear the 3rd bit (position 2)\nnumber = 12  # 1100\nmask = ~(1 &lt;&lt; 2)  # ~(0100) = ...1011\nnumber &amp;= mask  # 1100 &amp; 1011 = 1000\nprint(number)  # Output: 8\n</code></pre>"},{"location":"PyDeepOlympus/PyUnderstandBitwise/#example-4-counting-set-bits-hamming-weight","title":"Example 4: Counting Set Bits (Hamming Weight)","text":"<ul> <li>Count the number of <code>1</code> bits in a number\u2019s binary representation.</li> </ul> Python<pre><code>def count_set_bits(n):\n    count = 0\n    while n:\n        count += n &amp; 1  # Check LSB\n        n &gt;&gt;= 1  # Shift right\n    return count\n\nprint(count_set_bits(13))  # 13 = 1101, Output: 3\n</code></pre>"},{"location":"PyDeepOlympus/PythonRunningCode/","title":"How Python Runs Your Code","text":""},{"location":"PyDeepOlympus/PythonRunningCode/#your-code-eg-examplepy","title":"Your Code (e.g., <code>example.py</code>):","text":"<p>A Python file that contains your source code, like: Python<pre><code>print(\"Hello World\")\n</code></pre></p>"},{"location":"PyDeepOlympus/PythonRunningCode/#interpreter-part-of-cpython-implementation","title":"Interpreter / Part of CPython Implementation:","text":"<p>The interpreter is the program that runs your Python code. CPython is the default and most widely used implementation of Python, written in C. The interpreter, a key part of CPython, executes bytecode, whether it\u2019s from a <code>.pyc</code> file or generated in memory. When you install Python from python.org, you get the CPython implementation.</p>"},{"location":"PyDeepOlympus/PythonRunningCode/#lexing-parsing","title":"Lexing &amp; Parsing:","text":"<p>Before running the code, Python analyzes it in two steps to understand its structure: - Lexing: Breaks code into tokens (e.g., <code>print</code>, <code>\"Hello World\"</code>). - Parsing: Arranges these tokens into a tree-like structure called an AST (Abstract Syntax Tree), which represents the logic and flow of your code.</p>"},{"location":"PyDeepOlympus/PythonRunningCode/#bytecode-and-compilation","title":"Bytecode and Compilation","text":"<p>The AST is then compiled into bytecode, a low-level, platform-independent set of instructions. This bytecode is generated in memory and executed immediately by the Python Virtual Machine (PVM).</p> <p>For modules that are imported into the main script, the bytecode is saved to disk in a <code>.pyc</code> file inside the <code>__pycache__</code> directory. This caching speeds up future imports by avoiding the need to recompile the module every time it is loaded.</p> <p>\ud83d\udd39 Note: The <code>.pyc</code> file is only written in disk when a module (e.g, <code>.py</code> file) is imported, Running the main script directly (e.g, without any imported <code>.py</code> file) only uses in-memory bytecode.</p> <p>\ud83d\udd39 Bytecode is not machine code\u2014it\u2019s an intermediate representation that Python understands.</p>"},{"location":"PyDeepOlympus/PythonRunningCode/#python-virtual-machine-pvm","title":"Python Virtual Machine (PVM):","text":"<p>The PVM is the runtime engine that reads and executes the bytecode instruction by instruction. It\u2019s a core part of the Python interpreter. It interacts with the OS, manages memory, handles I/O, and displays output (like <code>\"Hello World\"</code> on the screen).</p>"},{"location":"PyDeepOlympus/PythonRunningCode/#compiler-pvm-are-both-parts-of-the-python-interpreter","title":"Compiler &amp; PVM Are Both Parts of the Python Interpreter","text":"<p>The Python interpreter consists of two essential components: - \ud83d\udd38 A compiler that translates <code>.py</code> files into bytecode. - \ud83d\udd38 The PVM that executes that bytecode.</p>"},{"location":"PyDeepOlympus/PythonRunningCode/#this-is-the-flow-of-the-whole-process","title":"This Is the Flow of the Whole Process","text":"<p>Your code \u2192 Lexer &amp; Parser \u2192 AST \u2192 Compiler \u2192 Bytecode \u2192 PVM \u2192 Output  </p>"},{"location":"PyDeepOlympus/PythonRunningCode/#what-about-jit","title":"What About JIT?","text":"<ul> <li>CPython doesn\u2019t use JIT (Just-In-Time compilation) by default; it interprets bytecode directly. Other implementations like PyPy use JIT for better performance.  </li> <li>However, CPython is experimenting with JIT! In CPython 3.13, a JIT compiler is being developed, but as of April 7, 2025, it\u2019s experimental and disabled by default. Learn more here: What docs say about JIT.</li> </ul>"},{"location":"PyDeepOlympus/PythonRunningCode/#cpython-vs-jit-how-they-work","title":"CPython vs. JIT: How They Work","text":"<p>CPython compiles Python source code into bytecode once, then interprets it step by step using the Python Virtual Machine (PVM). It\u2019s a slow process no machine code is made directly.</p> <p>JIT compiles code into machine code during runtime, and reusing the most used parts of the program. </p> <p>CPython keeps things simple; JIT focuses on speed.</p>"},{"location":"PyDeepOlympus/PythonRunningCode/#diagram","title":"Diagram","text":""}]}